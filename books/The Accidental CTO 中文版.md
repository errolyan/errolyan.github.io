![Book Cover ](https://github.com/user-attachments/assets/8f527042-fd7e-43bb-ac63-8c179fe19f28)



# 章节

- [第1章：凌晨3点的电话](#第1章凌晨3点的电话)

- [第2章：WhatsApp PDF问题（起源）](#第2章whatsapp-pdf问题起源)

- [第3章：伟大的分离：应用程序与数据库的分离](#第3章伟大的分离应用程序与数据库的分离)

- [第4章：交通警察：负载均衡简介](#第4章交通警察负载均衡简介)

- [第5章：数据库俱乐部的保镖：读副本](#第5章数据库俱乐部的保镖读副本)

- [第6章："不要在生产环境测试，兄弟！"：预发布环境](#第6章不要在生产环境测试兄弟预发布环境)

- [第7章：速度的需求：使用Redis缓存](#第7章速度的需求使用redis缓存)

- [第8章：打破单体架构：我们的第一个微服务](#第8章打破单体架构我们的第一个微服务)

- [第9章：不可违背的承诺：使用Kafka实现数据一致性](#第9章不可违背的承诺使用kafka实现数据一致性) 

- [第10章：集装箱革命：Docker简介](#第10章集装箱革命docker简介)  

- [第11章：聪明的店员：构建世界级搜索](#第11章聪明的店员构建世界级搜索)

- [第12章：快递员：CDN静态资源](#第12章快递员cdn静态资源)

- [第13章：指挥家：使用Kubernetes编排一切](#第13章指挥家使用kubernetes编排一切)

- [第14章：鲨鱼池效应：火的考验](#第14章鲨鱼池效应火的考验)

- [第15章：我们的全球大脑：设计Dukaan边缘网络](#第15章我们的全球大脑设计dukaan边缘网络)

- [第16章：聚光灯下：从意外的CTO到技术领导者](#第16章聚光灯下从意外的cto到技术领导者)

- [第17章：逃离黄金笼子：从AWS到裸金属](#第17章逃离黄金笼子从aws到裸金属)

- [第18章：盛大结局：实时故障转移](#第18章盛大结局实时故障转移) 

- [第19章：意外的CTO](#第19章意外的cto)

- [🖊️ 纠正错误](https://github.com/hitesh-c/The-Accidental-CTO/blob/main/The%20Accidental%20CTO.md)

<br/>

## 第1章：凌晨3点的电话

### 第1部分：崩溃

电话不只是响了；它在尖叫。

这是一种特定的振动，是手机制造商为引起最大恐慌而设计的。廉价木制床头柜上剧烈而愤怒的嗡嗡声。这种声音不只是把你吵醒，而是让你瞬间进入高度警觉状态。时钟上的时间显示着一个不祥的红色：**凌晨3:14**。

我的心开始狂跳，甚至还没睁开眼睛。在这个不合理的时刻，只有两个原因会让人打电话：家庭紧急情况，或者公司灾难。来电显示证实了后者。它显示了一个我非常熟悉的名字：**"Suumit."**

Suumit Shah，我的联合创始人，我的合作伙伴，我的商业大脑对应我的构建之手。他只在凌晨3点打电话，原因只有一个。公司着火了。

我滑动接听，声音干涩沙哑。"喂？"

> "Subhash! Uth! Sab bandh ho gaya hai!"
Suumit的声音通过电话听筒传来，像一连串肾上腺素和恐慌的霰弹枪。_醒醒！一切都瘫痪了！_

他不需要说更多。我已经从床上跳了起来，冰冷的地板让我瞬间清醒。我踉跄着走向笔记本电脑，熟悉的苹果标志白光在黑暗的房间里像灯塔一样。我的脑海里一片混乱，一个疯狂的数字灾难清单。

我们被黑客攻击了吗？DDoS攻击？某个外国的孩子决定为了好玩把我们搞垮？

是开发人员推送了错误的代码？一个放错位置的分号让整个系统瘫痪？

我们的云提供商出现故障了吗？这完全超出了我们的控制范围？

"网站无法加载。应用程序显示错误。什么都没有。完全瘫痪了，"Suumit继续说道，声音里透着焦虑。我能听到他在电话那头踱步的声音。

"好的，好的，我在处理。冷静点，"我说道，试图让自己听起来比实际更冷静。_保持冷静。_ 消防的第一条规则是不要成为火灾的一部分。

我的手指还因为睡眠而笨拙，在键盘上飞快地敲击。我打开了终端，那个显示绿色文字的黑色屏幕，那是我进入整个操作的窗口。这是我的指挥中心。

> ssh <root@dukaan.app>

我按下了回车键。

光标在闪烁。又在闪烁。又在闪烁。

通常，登录提示会立即出现。这种延迟……这是一个坏兆头。一个非常坏的兆头。这意味着服务器不只是生病了；它在垂死挣扎，连应答门都困难。感觉过了一个世纪，提示终于出现了。服务器还活着，但勉强维持着。

我的大脑在飞速运转。如果服务器这么慢，那就不是简单的代码错误。这是更深层的问题。这是根本性的问题。感觉就像机器本身在窒息。

我输入了第一个诊断命令，一个检查服务器生命体征的简单工具。

> htop

填满屏幕的结果让我的血液变冷。那是一片红色的海洋。

每个进程都在尖叫着寻求关注。CPU使用率条达到了100%。内存条，显示服务器"思考空间"的那个，完全满了。交换使用率，服务器的紧急溢出内存，也满了。

服务器不只是在窒息。它已经死了，我们只是在见证它神经系统的最后几次抽搐。

然后我看到了原因。简单得几乎令人尴尬的灾难性失败原因。系统监视器顶部显示的服务器总内存：**512MB**。

五百一十二兆字节。

我的现代智能手机有8吉字节的RAM，是这台运行我们整个公司的机器的十六倍。成千上万依赖我们的企业，数百万他们目录中的产品，整个Dukaan的希望和梦想，都在一台比口袋里的手机功率还小的机器上运行。

这不是复杂的黑客攻击或复杂的错误。我们只是用完了空间。我们试图在电话亭里举办摇滚音乐会，而电话亭终于倒塌了。

盯着屏幕，电话还贴在耳边，我有了一刻恐怖的清醒。我这个没有正规计算机科学学位、没有接受过系统扩展培训的人，怎么会最终负责这一切？

要理解这一点，你首先需要理解我们试图驯服的野兽。你需要理解当前正在着火的东西的解剖结构：我们的服务器。

### 第2部分：服务器解剖学，或者说，单厨师厨房

让我们从凌晨3点的恐慌中休息一下。在我们解决问题之前，我们需要理解它。什么是"服务器"？

忘记技术术语。忘记数据中心里闪烁的灯光。在本书的其余部分，我希望你把服务器想象成更简单的东西：**一个只有一个非常忙碌的厨师的餐厅厨房**。

这个类比是你将学到的关于基础设施的最重要的东西。其他一切都建立在这个基础上。

#### **CPU：厨师的速度**

**CPU（中央处理器）**就是你的厨师。它是操作的大脑。厨师拿着原材料（数据），按照食谱（代码）制作出完成的菜肴（网页、搜索结果、完成的订单）。

- **更快的CPU**（以千兆赫兹或GHz为单位）意味着你有一个更快的厨师。他可以更快地切菜、搅拌锅子和装盘。
- 拥有**更多"核心"**的CPU就像一个有多只手臂的厨师。一个4核CPU就像一个可以同时切菜、搅拌、煎炸和调味的厨师。他可以同时处理多个任务。

我们小小的512MB服务器有一个单核CPU。我们有一个独臂厨师，却让他为一万名客人烹饪盛宴。

#### **RAM：台面空间**

**RAM（随机存取存储器）**是厨房的台面。这是最需要理解的部分。RAM是厨师的工作空间。这是他存放当前正在烹饪的菜肴所需的所有食材、锅碗瓢盆和用具的地方。

从台面上拿东西是超快的。厨师不需要思考；他只需伸手就能拿到所需的东西。更多的RAM意味着更大的台面。拥有巨大台面的厨师可以同时处理许多不同的订单，因为他把所有食材都摆在面前。

如果台面满了，厨师就有严重问题了。他必须停下正在做的事情，去后面的慢 pantry，找到他需要的食材，然后带回来，把其他东西推下台面腾出空间。这会大大减慢一切速度。

这正是发生在我们服务器上的情况。我们的512MB RAM就像一个切菜板大小的台面。我们的应用程序、数据库和操作系统本身都在争夺这个小台面上的空间。当它满了时，服务器开始使用"交换空间"——专门指定为紧急台面空间的慢 pantry 的特殊部分。这是极其低效的。厨师花在来回跑 pantry 的时间比实际烹饪的时间还多。

#### **磁盘/存储：食品储藏室**

**磁盘（或存储）**，无论是硬盘驱动器（HDD）还是固态驱动器（SSD），都是厨房的食品储藏室和冰箱。这里长期存放着所有的食谱（你的代码）、食材（你的数据）和厨房设备（操作系统）。

它比台面（RAM）大得多，但访问速度也慢得多。你不希望你的厨师每次需要一点盐都要跑到 pantry。他应该已经把盐放在台面上了。

#### **资源争用：厨房里的混乱**

现在，想象一下我们的设置。在这个只有一个独臂厨师和切菜板大小台面的小厨房里，我们让他做所有事情：

- **运行应用程序：**他必须阅读食谱书（我们的Python代码）并烹饪菜肴。
- **管理数据库：**他还必须充当 pantry 管理员，不断整理、获取和存储食材（我们的用户数据）。
- **处理网络流量：**除此之外，他还必须当服务员，跑到餐厅前面一次接待成千上万的顾客订单。

这就是**资源争用**。每个人都在同时大声呼喊要厨师的注意力。应用程序需要CPU时间，数据库需要写入磁盘，传入的用户请求需要RAM。他们都在争夺相同的有限资源，结果就是完全的僵局。

要看到这种混乱，你需要厨房里的闭路电视摄像头。在服务器的世界里，我们的摄像头是一个简单的命令：htop。它是名为top的工具的改进版本，它让你实时看到你的厨师在做什么。

它看起来很复杂，但你只需要看几样东西：

- **顶部的CPU条：**如果它们都达到100%（并且是红色的），说明你的厨师过度劳累了。
- **内存（RAM）条：**如果这个满了，说明你的台面溢出了。
- **交换空间条：**如果这个开始填满，这是一个绝望的信号。你的厨师正在使用慢 pantry 作为工作空间。
- **进程列表：**这显示了厨师正在处理的每一个任务以及谁占用了最多的资源。

学会阅读这个屏幕是成为CTO的第一步，无论是意外的还是其他的。这就是你停止猜测开始诊断的方法。对我来说，那天晚上，屏幕在尖叫着一个单一的、不可否认的真相：我们的厨房从根本上说是致命的，对于我们的雄心壮志来说太小了。

### 第3部分：我们辉煌而危险的单体架构

服务器是厨房，但我们的厨师使用的食谱书呢？在软件中，我们称之为**架构**。我们的架构是一个经典的架构，几乎所有初创公司都以此开始。

我们有一个**单体架构**。

这个名字听起来很大很吓人，就像一些古老的石头建筑。实际上，这是一个非常简单的概念。单体应用程序是指所有代码都生活在一个单一的统一块中。我们的用户注册、产品目录、订单管理、卖家仪表板、支付等代码——所有东西都在一个单一的Django项目中。

把它想象成一个单一的、巨大的、一体化的食谱书。它包含了开胃菜、主菜、甜点和饮料的食谱，全部装订在一巨册中。

#### 为什么我们从单体架构开始（以及为什么这是正确的选择！）

我想对此非常明确：从单体架构开始并不是一个错误。对于初创公司来说，这通常是**最佳选择**。在早期，你的唯一目标是尽可能快地构建和发布。你需要找出是否有人甚至想要你正在制作的东西。

单体架构是为速度而构建的：

- **开发简单：**一切都在一个地方。你不必担心不同服务之间的复杂通信。你只需编写一个函数并调用它。
- **测试简单：**你可以在笔记本电脑上运行整个应用程序并轻松地一起测试所有功能。
- **部署简单：**你只需把单个代码块放到服务器上。完成。

我们的单体架构让Suumit和我两个人在仅仅48小时内就构建并发布了一个功能性的电子商务平台。如果我们试图从更复杂的"微服务"架构开始（我们稍后会讲到），我们可能还在争论设计。

单体架构是我们的超能力。它让我们以闪电般的速度前进。但就像所有超能力一样，它有一个隐藏的、危险的副作用。

<br/>
#### 单体架构的隐藏危险

随着我们的食谱书越来越大，问题开始出现。

- **它变得沉重而笨拙。** 找到特定的食谱需要更长时间。理解甜点部分的更改如何影响开胃菜部分变得几乎不可能。在软件中，我们称之为**紧密耦合**。
- **一个错误就可能毁掉整本书。** 一个食谱中的小拼写错误，理论上可能使整本书无法阅读。一个小功能中的错误可能导致整个网站瘫痪。
- **你不能雇佣专业厨师团队。** 如果你想雇佣专门的糕点师，他仍然需要理解整个巨大的食谱书。这让新开发人员很难快速上手。

最重要的是，这也是那晚杀死我们的原因，**你不能只扩展书的一部分而不扩展整个书**。

我们的店面获得了成千上万的访客（"主菜"非常受欢迎）。但我们的卖家仪表板（"开胃菜"）使用得少得多。因为它们都在同一个单体架构中，我们必须同时为所有事情使用服务器的资源。对主菜的巨大需求使厨房的其他部分资源匮乏，导致整个系统崩溃。

我们的单体架构运行在我们 tiny 的单个服务器上，形成了完美风暴。从软件角度看是单点故障，从硬件角度看也是单点故障。它是一个定时炸弹，而在周二凌晨3:14，它终于爆炸了。

## 第1章：要点总结

- **你的第一个服务器总是会失败。** 这不是**是否**的问题，而是**何时**的问题。目标不是防止失败，而是快速恢复并从中学习。
- **掌握基础知识。** 在你学习复杂架构之前，深入了解服务器是什么。从**CPU（厨师的速度）**、**RAM（台面空间）**和**磁盘（食品储藏室）**的角度思考。
- **学习基本诊断工具。** 你看不到的东西就无法修复。学会使用ssh和htop（或top）。它们是系统管理员的听诊器。
- **从单体架构开始是一个特性，而不是缺陷。** 在开始时优先考虑开发速度。不要过度设计你的初始产品。
- **认识到你的初始选择有保质期。** 能让你获得前10,000个用户的架构不会让你获得100,000个用户。准备好进化。

## 第2章：WhatsApp PDF问题（起源）

### 第1部分：想法和滑板

每个初创公司的故事都始于一个问题。我们的故事始于一个模糊的PDF和全国封锁的混乱。

那是2020年。世界按下了巨大的暂停键。在印度，曾经充满喇叭声、小贩叫卖声和人声鼎沸的街道陷入了寂静。熟悉的日常生活节奏被打破了。对于数百万小企业主——杂货店老板、社区蔬菜商贩、卖手工纱丽的女士来说，这是一场灾难。他们的店铺关门了，顾客被锁在家里，他们的生计正在蒸发。

唯一的救命稻草是互联网，特别是WhatsApp。它成为了新的市场、新的店面、新的讨价还价柜台。但这是一个混乱、低效且令人极度沮丧的平台。

这就是我的联合创始人Suumit登场的地方。他不是在试图建立公司；他只是在试图帮助附近的一家杂货店维持经营。他观看着，既着迷又恐惧，因为他们通过一系列混乱的WhatsApp聊天进行全部业务。

这个过程是低效的典范。

**第1步：目录。** 店主会给每个潜在客户发送一个多页PDF文档。这是一个格式糟糕、分辨率低的文件，可能是用Microsoft Word制作并保存错误的。产品名称错位，价格模糊，无法搜索。要找出他们是否有你喜欢的饼干品牌，你必须手动滚动浏览五页模糊的图片。

**第2步：订单。** 客户在大量眯眼和缩放后，必须在一条长而容易出错的文本消息中输入整个订单。"一包Maggi，两公斤面粉，半公斤糖，那个蓝色的Lays包装..."（_一包Maggi，两公斤面粉，半公斤糖，那个蓝色的Lays包装..._）

**第3步：确认。** 店主现在要处理几十个这样的聊天，必须手动确认每个项目。"抱歉女士，蓝色的Lays缺货了，我们有绿色的。"这会引发另一轮来回消息。

**第4步：付款。** 最后，在订单确认后，店主会发送他的UPI ID或二维码。客户使用GPay或PhonePe付款，然后——最关键的一步——发送成功交易的截图作为证明。店主的手机相册是成千上万付款截图的墓地，没有简单的方法来核对谁付了什么。

这是一场噩梦。一个出于绝望而诞生的数字jugaad（临时拼凑的解决方案）。

一天晚上，我的电话响了。是Suumit。我能听到能量通过听筒噼啪作响。他不只是在说话；他几乎因为一个想法而颤抖。

"Subhash，兄弟，这太疯狂了，"他开始说道，直接跳过问候。"我在看这个叔叔试图在WhatsApp上经营整个店铺，这是一场灾难。他在丢失订单，混淆付款...这完全坏了。我们必须做点什么。"

他描绘了我刚才描述的画面，PDF和截图的混乱舞蹈。

"我们需要为这些人建立一些简单的东西，"他说，声音变得更加激烈。"一个应用程序，让他们可以列出自己的商品，客户可以直接下单。简单。没有PDF，没有长聊天。只是一个链接。他们拥有自己的在线店铺。"（_一个应用程序，让他们可以列出自己的产品，客户可以直接下单。简单...他们自己的在线店铺。_）

这个词在空中回荡：**Dukaan**。店铺。

他是对的。问题不是缺乏技术；而是缺乏简单性。这些卖家不需要像亚马逊或Shopify这样的复杂平台。他们没有时间也没有技术技能。他们需要像WhatsApp本身一样易于使用的东西，但是为商业设计的。

那个电话是火花。但想法只是一个念头。要把它变成现实，你必须建造。在初创公司世界里，你必须快速建造。我们没有奢侈地花六个月时间创造完美产品。我们需要在几天内知道这是否是一个值得追求的想法。

这让我们了解了对任何有抱负的创始人或技术员来说最重要的概念之一：**MVP**。

#### 技术深度探讨：最小可行产品（MVP）

MVP或最小可行产品这个术语在科技界被广泛使用。大多数人认为这意味着建造一个更小、更易出错的最终产品版本。这是错误的。

MVP不是产品。**它是一个实验。**

它的主要目标不是赚钱或吸引数百万用户。它的主要目标是**学习**。它是一个科学工具，旨在用最少的努力测试你最关键的前提假设。我们的假设是：**"如果我们给小企业主一个极其简单的工具来创建在线店铺，他们会使用吗？"**

为了测试这一点，我们不需要完美产品。我们需要最简单可能的东西来回答这个问题。这就是MVP理念。

这样想：如果你的目标是解决"交通"问题，你不会从建造汽车开始。汽车很复杂——它有发动机、轮子、座椅、底盘、电子设备。建造一辆需要很长时间。当你完成时，你可能会发现你的客户实际上想要的是摩托车。

MVP方法是首先建造一个**滑板**。它很基础，很简单，但它解决了核心问题：它能让你从A点到B点。它让你能够测试你的核心假设。人们真的想要在轮子上移动吗？

一旦你确认了这一点，你就可以利用反馈来建造下一个版本：滑板车。然后是自行车。然后是摩托车。最后是汽车。在每个阶段，你都在学习和创造价值。

所以，对于Dukaan，我们必须定义我们的滑板。为了测试我们的想法，我们需要建造的绝对最低限度是什么？我们抛弃了能想到的每一个花哨功能。没有支付网关集成，没有配送跟踪，没有客户账户，没有主题，没有分析。我们将其简化到绝对本质。

我们将其定义为**Dukaan的核心循环**：

- **创建店铺：** 一个单页，用户输入手机号码，接收OTP（一次性密码）进行验证，并给店铺命名。就这样。他们的店铺就上线了。没有邮箱，没有密码，没有复杂表单。
- **添加产品：** 最简单的产品表单。一个产品名称字段，一个价格字段，以及一个从手机相册上传单张图片的按钮。没有分类，没有变体（如尺寸或颜色），没有库存计数。只是创建目录的基本要素。
- **分享链接：** 一旦他们添加了几个产品，应用程序会生成一个唯一的、可分享的链接，如mydukaan.io/mystore。他们可以复制这个链接并直接粘贴到他们的WhatsApp聊天中。

就是这样。这就是我们的滑板。

它不是一个"平台"。它不是一个"电子商务套件"。它是一个简单的工具，用干净、移动友好的网页替换丑陋的PDF。它是解决核心问题所需的绝对最低限度。

计划确定后，我们给自己设定了一个挑战。没有漫长的开发周期。没有冗长的规划会议。我们将在一个周末内建造并发布这个MVP。

**48小时黑客马拉松开始了。** 时钟在滴答作响。现在，我们必须做出第一个重大技术决定：使用什么工具来建造我们的滑板？

### 第2部分：选择我们的工具

48小时的时钟在滴答作响。我们有了"什么"——MVP，我们的数字滑板。现在我们需要"如何"。我们将使用什么工具来构建它？

在软件世界中，你的工具集被称为**"技术栈"**。把它想象成建造房屋。你必须决定主要材料。你会使用砖块、木材还是钢材？你会打什么基础？你会使用什么工具把它们组装在一起？这些选择决定了你能多快建造，你的房子会有多坚固，以及以后添加新房间有多容易。

对于周末黑客马拉松，这个选择受一个原则支配：**速度**。我们不需要最具可扩展性、最前沿或最"流行词合规"的技术栈。我们需要能让我们在最短时间内从零到工作产品的技术栈。

这意味着选择我们熟悉的、可靠的、能为我们完成大量繁重工作的工具。

#### 技术深度探讨：语言和框架

我们第一个也是最重要的选择是编程语言和框架。

为什么选择Python？

编程语言是你用来向计算机发送指令的词汇。我们选择了Python。为什么？因为Python以其简单、干净的语法而闻名。它读起来几乎像普通英语。当你在与时间赛跑时，你最不想做的就是与自己的工具作斗争，试图记住复杂的规则或遗漏的分号。Python不会妨碍你，让你专注于你正在解决的问题。它也拥有庞大的社区，几乎任何你能想象到的功能都有库支持。

为什么选择Django："自带电池"的框架

语言只是词汇。框架是整个使用手册。它提供结构、蓝图和一组预构建的组件，这样你就不必从头开始。

想象你正在建造一所房子。你可以去砍伐自己的树木，自己锯木材，自己锻造钉子。或者，你可以购买一个预制房屋套件，其中包含已经建造好的所有墙壁、窗户和门。你所要做的就是组装它们并添加你个人的风格。

这就是Django。它是Web应用程序的预制房屋套件。它的理念是著名的**"自带电池"**。这意味着它几乎包含了你开箱即用所需的一切。对于我们48小时的MVP，其中两个"电池"是绝对的游戏规则改变者：

- **Django管理面板：** 这是Django的杀手级功能。只需几行代码，Django就会自动生成一个完整、安全、专业外观的管理面板。这是一个私人网站，我们作为创始人可以登录并查看应用程序中的所有数据。当新用户创建商店时，我们可以在管理面板中看到它弹出。我们可以查看他们的产品，如果需要可以编辑它们，或者解决任何问题。从零开始构建这样的自定义仪表板至少需要一天时间。Django在大约15分钟内就免费提供了它。它是我们发布时的任务控制中心。
- **ORM（对象关系映射器）：** 这听起来很复杂，但想法很简单。要从数据库获取数据，你通常必须编写一种叫做SQL（结构化查询语言）的特殊语言。它看起来像这样：SELECT \* FROM products WHERE store_id = 123;。它功能强大，但很容易打错字，而且感觉与我们正在编写的Python代码不同。Django的ORM充当翻译器。它允许我们使用简单的Python代码与数据库交互。同样的命令在Django中看起来是这样的：Product.objects.filter(store_id=123)。这不仅更容易编写和阅读，还能保护你免受一类称为SQL注入的安全漏洞的侵害。它使我们的代码更干净，开发速度更快。

#### **技术深度探讨：我们考虑的替代方案**

当然，Django并不是唯一的选择。在科技领域，做任何事情总有十几种方法。关键是为手头的工作选择正确的工具。

- **Node.js与Express：** 这是一个流行的替代方案。Node.js允许你用JavaScript编写服务器端代码，这与在Web浏览器中运行的语言相同。这对许多团队来说是一个巨大的优势。框架Express非常简约和灵活。但这对我们来说是它的缺点。Express不太像预制房屋套件，更像是一个高质量的乐高积木盒。它给你基本的构建块，但你必须自己组装所有东西。对于48小时的构建，我们不想要那么多自由；我们想要Django"自带电池"方法的结构和预构建组件。
- **Ruby on Rails：** 这是一个更接近的竞争对手。Rails有着与Django非常相似的理念。它重视"约定优于配置"，这意味着它为你做出许多明智的决定以加快开发速度。说实话，Rails也会是一个很好的选择。最终决定归结为个人偏好和熟悉度。我在Python和Django上花了更多时间，在速度竞赛中，你总是押注于你最熟悉的工具。

#### 技术深度探讨：数据库

选择了框架后，我们需要决定在哪里永久存储所有信息——商店名称、产品详情、价格等。我们需要一个**数据库**。如果框架是房屋套件，数据库就是它建立的基础。它需要坚实、可靠且有条理。

为什么选择关系型数据库？

我们选择了关系型数据库。这个想法很简单：数据存储在表中，就像巨大、强大的Excel电子表格一样。你有一个包含所有商店信息的stores表。你有一个包含所有产品信息的products表。而且，至关重要的是，你可以在它们之间创建关系。你可以告诉数据库每个产品必须属于一个商店。

这种结构非常适合电子商务。你的数据有明确的关系和规则。你不希望产品没有商店就漂浮着，或者订单没有客户。关系型数据库强制执行这种结构，确保你的数据保持干净和一致。

为什么选择PostgreSQL？

在许多关系型数据库中（如MySQL、Microsoft SQL Server等），我们选择了PostgreSQL（通常简称为"Postgres"）。

为什么？对于MVP，Postgres和它的主要竞争对手MySQL都是优秀的选择。但我们倾向于Postgres有几个原因。它在开发者社区中以极其健壮、可靠和符合标准而闻名。它是一个真正的工作马。更重要的是，我知道它有一些强大的高级功能，这些功能稍后可能会有用。其中一个叫做LISTEN/NOTIFY的功能，将成为第8章我们实时缓存系统背后的秘密武器。我们不需要它用于MVP，但从一开始就选择一个强大的基础，即使你不立即使用它的所有功能，在未来可能会带来巨大回报。

所以，蓝图已经准备好了。我们的技术栈已经确定：

- **语言：** Python
- **框架：** Django
- **数据库：** PostgreSQL

我们有了建筑材料。现在是时候浇筑基础并搭建第一面墙了。是时候设置我们的服务器了。

### 第3部分：浇筑基础

确定了技术栈后，理论部分就结束了。是时候做出一些实际的东西了。我们需要一个地方让我们的代码在互联网上生存。我们需要一个服务器。

如果我们的代码是蓝图，我们的栈是建筑材料，那么服务器就是实际的地块。它是你浇筑基础并为世界建造建筑物的物理（或在我们的情况下，虚拟）空间。

在早期选择服务器提供商是为了找到成本、简单性和性能之间的正确平衡。我们不需要亚马逊网络服务（AWS）或谷歌云的无限力量。那将像购买整个工业园区来建造一所房子。对于我们的需求来说，它太复杂且太贵了。我们需要一个简单、经济实惠的地块。

为此，我们转向了**DigitalOcean**。

#### 技术深度探讨：设置你的第一台服务器

DigitalOcean受到开发者欢迎的一个原因是：它让获取服务器变得极其简单。他们称他们的服务器为"Droplets"，这个友好的名称消除了传统服务器托管的令人生畏的光环。

**DigitalOcean Droplets实用指南**

获取我们的第一台服务器，即后来会引起如此多戏剧性的臭名昭著的512MB机器，花了不到五分钟。它就是这么简单：

- **创建账户：** 标准的注册流程。
- **创建Droplet：** 这是神奇发生的地方。你会看到一个干净、简单的仪表板。
- **选择镜像：** "镜像"是服务器操作系统和软件的预打包模板。我们选择了标准的**Ubuntu**镜像。Ubuntu是Linux的一个版本，是Web服务器的主导操作系统。它是免费的、安全的，拥有庞大的社区。我们选择了最新的LTS（长期支持）版本，它保证了几年的安全更新。
- **选择计划：** 这是你决定服务器大小（你的地块）的地方。我们向下滚动到最便宜的可能选项：**512MB RAM，1个vCPU，20GB SSD**。每月只需5美元。对于一个零用户和零收入的项目，这感觉像是最负责任的选择。这是一个小小的地块，但它是我们的。
- **选择数据中心区域：** 这是你的服务器在世界上的物理位置。我们选择了**班加罗尔**地区。为什么？因为我们知道我们的第一批用户会在印度。将服务器放在尽可能靠近他们的地方会通过减少延迟（数据传输所需的时间）使应用程序感觉更快。
- **点击"创建Droplet"。**

就是这样。我们等待了大约60秒，DigitalOcean就为我们配置了服务器。我们现在是一个公共IP地址的骄傲拥有者——我们服务器在全球互联网上的唯一地址。我们的小块土地已经准备好了。

**SSH：你进入服务器房间的钥匙**

现在我们有了土地，我们需要一种方法进入并开始建造。你不只是使用Web浏览器来控制服务器；你使用一种叫做**SSH（安全外壳）**的特殊工具。

把你的服务器想象成一个位于偏远地区的安全、无窗户的建筑。SSH是你的神奇、加密的钥匙。你在终端中使用一个命令，如ssh root@123.45.67.89，来解锁门并进入。一旦你进入，你就有了一个命令行界面，一种基于文本的方式，可以直接向服务器发出指令：安装软件、创建文件、运行你的应用程序。

这是我们的工作空间。一个带有闪烁光标的黑屏，安全地连接到我们在班加罗尔的新服务器。是时候安装我们基础设施的最后部分了：实际上会向世界展示我们网站的软件。

**Nginx + Gunicorn：服务员和厨房工作人员**

你不能仅仅运行Django应用程序并期望它工作。Django是一个用于**构建**应用程序逻辑的框架——它是一组食谱和主厨。但它不是为处理来自互联网的数千个原始、未过滤的请求而设计的。这样做就像让成千上万饥饿的顾客直接冲进你的厨房，对着厨师喊他们的订单。那将是混乱的。

你需要一个系统来管理流量。对于Python Web应用程序，这通常是一个两部分系统：**Web服务器**和**应用服务器**。

- **Web服务器（Nginx）：服务员。** 我们的Web服务器是**Nginx**（发音为"Engine-X"）。Nginx是你餐厅前面友好、高效的服务员。它是每个访问者的第一个接触点。它非常擅长一次处理数千个连接并执行简单、快速的任务。它的主要工作是：
  - **提供静态文件：** 如果用户请求图像、CSS文件或JavaScript文件，Nginx直接从食品储藏室（磁盘）获取并提供给他们。它不需要为此打扰忙碌的厨师。这是一个巨大的性能提升。
  - **充当反向代理：** 对于任何需要实际"烹饪"的请求（如加载商店的产品页面），Nginx不会自己完成这项工作。相反，它整洁地接受订单，走到厨房门口，并将其传递给应用服务器。
- **应用服务器（Gunicorn）：厨房经理。** 我们的应用服务器是**Gunicorn**。Gunicorn是厨房经理。它从Nginx获取订单，并将其翻译成我们的主厨Django能够理解的格式。它管理多个"流水线厨师"（称为工作进程）来一次处理几个订单。它是外部世界（由Nginx管理）和我们的应用程序代码（用Django编写）之间的关键链接。

流程简单但强大：用户的请求来自互联网并命中Nginx。Nginx要么直接提供静态文件，要么将请求传递给Gunicorn。然后Gunicorn运行我们的Django代码来处理请求，生成HTML页面，并将响应返回给Nginx，Nginx最终将其传递给用户。

设置这涉及到在我们的服务器上安装Nginx和Gunicorn，并编写几个简单的配置文件来告诉它们如何相互通信。随着最后一块到位，我们的基础已经浇筑，结构已经搭建完成。我们将域名mydukaan.io指向我们服务器的IP地址。

我颤抖着手指，在浏览器中输入地址并点击Enter。

它工作了。一个简单的"Hello, World"页面从我们5美元的小服务器提供服务。

我们的滑板已经建成。48小时即将结束。是时候看看是否有人想骑它了。

## 第2章：要点总结

- **MVP是一个用来测试你核心假设的实验，而不是最终产品的小型版本。** 它的目标是最大化学习，而不是功能。在编写一行代码之前，先定义你的"滑板"。
- **选择初始技术栈时优先考虑速度和熟悉度。** 在开始时，"上市时间"是最重要的指标。选择像Django或Rails这样"自带电池"的框架，它们会为你承担繁重的工作。
- **坚实的数据库是一个很好的长期投资。** 从一开始就选择像PostgreSQL这样健壮、功能丰富的数据库可以为你省去很多麻烦，并在以后启用强大的功能，即使你不立即使用它们。
- **从简单的基础设施开始。** 每月5美元的服务器足以处理你的前几千个用户。在你有明确需求之前，不要过度复杂化或在云服务上过度支出。
- **理解Web服务器（Nginx）和应用服务器（Gunicorn）的角色。** 这种"服务员"和"厨房经理"的基本模式是现代Web应用程序如何提供服务的骨干。

## 第3章：伟大的分离：分离应用和数据库

### 第1部分：第二天早晨

发布是模糊的。48小时的黑客马拉松结束后，我们在几个面向小企业主的WhatsApp群组中分享了我们新构建的MVP链接。我们不知道会发生什么。也许是少数几个注册，一些礼貌的反馈，然后慢慢淡出人们的视线。

相反，发生的是爆炸式增长。

事实证明，我们的假设是正确的。"WhatsApp PDF"问题的痛苦是如此严重，以至于卖家们迫切需要一个解决方案。我们简单、没有花里胡哨的工具正是他们所需要的。链接从一个群组转发到另一个群组。在短短几天内，我们的用户从十几个增加到几百个，然后又增加到几千个。每个新店主都会添加他们的产品，然后与他们的客户分享自己的mydukaan.io链接，而这些客户自己往往也是小企业主。这种增长是病毒性的。

这是世界上最令人振奋的感觉。每次刷新我们的Django管理面板，都会看到来自全国各地的新商店弹出。我们正在实时观看我们的想法，我们的"滑板"，变成现实。但在兴奋之下，一种无声的恐惧开始增长。

应用程序变得越来越慢。

以前立即加载的页面现在需要几秒钟。管理面板有时会挂起。我们收到了来自用户的第一条消息："网站打不开"或"服务器宕机了吗？"我们像疯狂的消防员一样四处奔跑，每隔几个小时就重启一次服务器，这种粗略的修复方法变得越来越无效。我们的小厨房被我们自己的成功压得喘不过气来。

然后，它发生了。凌晨3点的电话。崩溃。这本书开头的故事。

那个夜晚是我们初始、天真的增长阶段的高潮。我们那台单一、过度工作的服务器，那个曾是我们整个宇宙的5美元DigitalOcean Droplet，终于放弃了。这是我们第一个架构不可避免、痛苦的死亡。

第二天早上，经过几个小时不安的睡眠，Suumit和我通了电话。即时的火灾已经扑灭——我们设法又重启了一次服务器——但我们都知道我们的时间不多了。它会再次崩溃，可能在几个小时内。

"我们不能只是继续重启它，Subhash，"Suumit说，他的声音很紧张。"我们需要一个真正的修复。实际问题是什么？"

我花了最后几个小时挖掘服务器日志，盯着htop输出，直到眼睛都灼痛了。答案变得清晰起来。

"数据库，"我说。"数据库正在把其他一切都窒息而死。"

#### **识别瓶颈：厨房里的战争**

让我们回到我们的"单一厨师厨房"类比。我们的服务器是一个小房间，厨师（CPU）、台面（RAM）和食品储藏室（磁盘）都住在一起。

崩溃后，我们的分析显示了一个关键细节。厨师大部分时间不是在做饭（执行我们的Python代码）。他大部分时间都在来回奔波到食品储藏室，疯狂地寻找原料并把它们放回去（读取和写入数据库）。

数据库操作的需求如此之大，以至于它们正在耗尽应用程序其余部分的资源。服务员（Nginx）会站在门口接受新订单，但厨师太忙于管理混乱的食品储藏室，甚至无法看它们一眼。这就是为什么网站感觉如此缓慢，最终完全停止响应。

要解决这个问题，我们需要理解系统设计中的一个基本概念：应用程序的不同部分做不同类型的工作。

#### **技术深度探讨：应用程序与数据库工作负载**

并非所有工作都是平等的。Web应用程序执行两种非常不同的主要任务：

- **应用程序工作（厨房）：** 这是"思考"工作。它由我们的Django代码处理，由CPU运行。这项工作是**CPU密集型**的。它的工作是执行逻辑：确定显示哪些产品，计算订单的总价，检查用户是否已登录。这就像厨师积极遵循食谱，切菜、混合和品尝。这种工作需要一个快速的厨师（一个好的CPU）和足够的台面空间（RAM）才能高效工作。
- **数据库工作（食品储藏室和图书馆）：** 这是"获取和存储"工作。数据库的主要工作是读取和写入数据到磁盘。这项工作是**I/O密集型**（输入/输出密集型）的。它更多是关于物理检索信息的任务，而不是思考。想象一下图书管理员跑到书架上寻找一本特定的书，或者食品储藏室管理员在货架上存放物品。这种工作取决于快速的食品储藏室（快速的SSD）和良好的组织系统。

我们的问题是我们迫使我们出色的厨师同时也是全职的图书管理员。他试图在一个繁忙、嘈杂的图书馆中间烹饪一顿美食。不断地来回奔跑于书架之间（磁盘I/O）阻止了他做他的实际工作：烹饪（执行代码）。两项工作都做得不好。

解决方案在概念上很简单，但在实践中很可怕。

"我们需要把它们分开，"我告诉Suumit。"我们需要给数据库自己的、专用的房间。一个合适的图书馆，有专门的图书管理员。我们需要给厨师一个单独的厨房。"

这意味着从一台服务器移动到两台。这是我们第一个重大的架构变更。

- **服务器1：应用服务器。** 这台服务器将针对CPU密集型任务进行优化。它将运行Nginx、Gunicorn和我们的Django代码。它唯一的工作就是"思考"。
- **服务器2：数据库服务器。** 这台服务器将针对I/O密集型任务进行优化。它将只运行一件事：我们的PostgreSQL数据库。它唯一的工作就是"记忆"。

这就是计划。伟大的分离。它听起来很合乎逻辑。它听起来是对的。但它也意味着我们必须对我们运行中的、活着的应用程序进行心脏手术。我们将不得不小心地将整个数据库——每个用户、每个产品、我们公司的每一条数据——从一台机器移动到另一台机器。

如果我们搞砸了，我们可能会损坏数据。我们可能会丢失订单。我们可能会破坏刚刚开始依赖我们的数千名卖家的信任。风险再高不过了。

### 第2部分：迁移手册

决定已经做出。我们将执行伟大的分离。这感觉就像站在悬崖边缘，知道你必须跳下去。唯一的问题是如何在下降的过程中构建一个降落伞。

我们花了几个小时规划每一个步骤。我们把它写下来，就像飞行前的检查表。在这样高风险的操作中，你不能即兴发挥。你必须遵循计划。一个错误可能是致命的。

#### **蓝图：之前和之后**

我们的目标是将我们的基础设施从一台超负荷的单一机器转变为两台专门的机器。

- **之前：** 一台服务器（例如，IP：104.248.62.77）运行所有东西：Nginx、Gunicorn、Django和PostgreSQL。
- **之后：**
  - **应用服务器**（IP：104.248.62.77）：运行Nginx、Gunicorn和Django。
  - **数据库服务器**（IP：142.93.218.155）：仅运行PostgreSQL。

应用服务器将不再与localhost（意为"在同一台机器上"）上的数据库通信。它将必须通过网络与新的、专用的数据库服务器通信。

以下是我们遵循的确切手册。如果你发现自己处于这种情况，这些是移动实时数据库的可怕但必要的步骤。

#### **步骤1：准备新家**

在新房子建好之前，你不能搬进去。我们的第一步是为我们的数据库创建新的、专用的服务器。

我们回到DigitalOcean仪表板并启动了一个新的Droplet。这次，我们选择了一个稍微不同的计划。我们没有选择通用服务器，而是选择了一个"存储优化"的服务器。它有更快类型的SSD存储（称为NVMe）和相对于其CPU功率更多的RAM。它是为数据库的I/O密集型工作量身定制的。它是我们新的、最先进的图书馆。

服务器启动后，我通过SSH登录。我安装的唯一软件是PostgreSQL。没有其他的。没有Nginx，没有Python，没有应用程序代码。它的目的是单一的：成为我们数据的最佳可能家园。我还配置了防火墙，只允许来自我们应用服务器特定IP地址的连接。互联网上的其他人甚至不应该尝试与我们的数据库对话。这就像给我们的应用服务器一把特殊的、私人的图书馆钥匙。

#### **步骤2：备份（pg_dump）**

这是最关键的步骤。你如何复制一个活着的、呼吸着的数据库？你不能只是复制文件，因为它们可能正在被更改的过程中，导致副本损坏。

你需要创建一个完美的快照。对于PostgreSQL，pg_dump就是实现这一目标的神奇工具。

pg_dump是一个命令行实用工具，它会读取你整个数据库——每个表、所有数据、每个关系——并输出一个带有.sql扩展名的单一大型文本文件。这个文件包含了从零开始完美重建数据库所需的所有SQL命令。

可以这样理解：pg_dump就像一位神奇的抄写员，走进你的图书馆，阅读每一本书，然后写出一本新的、单一的主书，标题为“重建整个图书馆的说明”。

我在原始的一体化服务器上运行了以下命令：

> pg_dump -U postgres dukaan_prod > dukaan_backup.sql

我看着服务器的CPU飙升。它正在努力创建这个快照。几分钟后，任务完成。我们现在有了一个名为dukaan_backup.sql的文件，它包含了我们公司的整个灵魂。

#### **步骤3：传输和恢复**

现在我们有了蓝图，但它在错误的服务器上。我们需要将这个备份文件从旧服务器安全地传输到新的、空的数据库服务器上。为此，我们使用了另一个名为scp（Secure Copy）的命令行工具。

> scp dukaan_backup.sql root@142.93.218.155:/root/

这个命令通过网络安全地复制了我们的备份文件。现在，新图书馆拥有了这本“说明”书。

备份文件到达新服务器后，是时候重建了。我通过SSH登录到新的数据库服务器，创建了一个名为dukaan_prod的空数据库外壳，然后运行命令从备份中恢复：

> psql -U postgres -d dukaan_prod < dukaan_backup.sql

这个命令执行的是pg_dump的反向操作。它读取庞大的说明文件并逐行执行每条命令。它创建表，插入数据，重建关系。我盯着屏幕，祈祷不会出现任何错误。几分钟后，它完成了。

我们现在有了数据库的完美克隆，运行在全新、强大且隔离的服务器上。旧数据库仍然活跃并为用户提供服务，但它的克隆已准备就绪，等待着。

#### **步骤4：切换**

这是关键时刻，也是整个操作中最危险的部分。我们必须将实时应用程序从使用旧数据库切换到新数据库。这将需要几分钟的停机时间。

- **激活维护模式**：我们的第一步是防止任何新数据被写入。我们迅速在网站上挂起了维护页面。任何访问mydukaan.io的人现在都会看到一条简单的消息："Dukaan正在进行快速升级。我们将在5分钟内回来！"
- **最终同步**：我们知道，在第一次备份和现在之间，一些新数据已经进入系统。一些新商店，一些新产品更新。所以，我们再次重复了步骤2和3，但这次，由于网站处于维护模式，速度要快得多。这确保了我们的克隆是100%最新的。
- **更新连接字符串**：这是真正的心脏手术。在Django应用程序的设置文件深处，有一行代码告诉它在哪里可以找到数据库。它看起来像这样：HOST: 'localhost'。我们将其更改为指向新服务器的IP地址：HOST: '142.93.218.155'。
- **重启并祈祷**：保存新设置后，我运行命令重启应用服务器：sudo systemctl restart gunicorn。有几秒钟，我的心提到了嗓子眼。应用程序现在正在启动，并将首次尝试通过网络与数据库通信。
- **疯狂测试**：服务器重新上线的那一刻，Suumit和我就在网站上，点击所有内容。我们能登录吗？是的。商店能加载吗？是的。我们能添加新产品吗？是的！一切正常。连接成功。
- **禁用维护模式**：带着如释重负的深呼吸，我们关闭了网站的维护模式。

整个停机时间约为三分钟。

伟大的分离完成了。我们的应用程序在自己的厨房中运行，我们的数据在自己的图书馆中安全存放。用户立即开始告诉我们网站感觉"更快了"。我们成功完成了第一次重大架构升级。厨房干净，图书馆整洁，两者现在都可以在不互相干扰的情况下发挥最佳作用。

### 第3部分：新的瓶颈

几周来，我们第一次可以松口气了。

伟大的分离取得了成功。网站稳定、快速，可以处理稳定的新用户流而不会每隔几个小时崩溃一次。厨房和图书馆都在各自专用的空间中，整个运作比以往任何时候都更顺畅。Suumit和我庆祝了。我们面对了第一次真正的扩展危机，并变得更加强大。我们为自己赢得了时间。

然而，在初创公司的世界里，时间是你唯一能买到的东西。你实施的每个解决方案，你清除的每个瓶颈，只会揭示下一个瓶颈在等着你。扩展科技公司有点像打地鼠游戏；一旦你解决了一个问题，另一个问题就会冒出来。

我们的新问题很微妙。它不是灾难性的崩溃或服务器起火。它是一种安静、渐进的缓慢。即使有了我们两台强大的专用服务器，一些页面仍然感觉有点...迟钝。我们解决了资源争用问题，但这样做的同时，我们为自己创造了一个全新的、更复杂的问题：**网络延迟**。

#### **技术深入：网络调用的成本**

当我们的应用和数据库位于同一台机器（localhost）上时，它们之间的通信几乎是瞬时的。这就像厨房中的厨师转身从身后的架子上拿取食材一样。"旅行时间"实际上为零。

现在，我们的厨房（应用服务器）和图书馆（数据库服务器）位于两个不同的建筑物中。它们位于班加罗尔的同一个数据中心，所以它们就像两个紧挨着的建筑物，通过超高速的专用光纤电缆连接。但无论这个连接有多快，厨师仍然必须：

- 停下正在做的事情。
- 走出厨房门。
- 穿过小小的"街道"到图书馆。
- 找到图书管理员并请求书籍（数据）。
- 等待图书管理员找到它。
- 穿过街道返回厨房。

整个旅程就是一次**网络调用**。所需的时间称为**延迟**。

对于单个请求，这种延迟可能很小——可能只有1或2毫秒（ms）。你甚至不会注意到它。但这里有一个问题：典型的网页不仅仅向数据库发出一个请求。为了构建单个商店页面，我们的代码可能需要：

- 获取商店详情。（1次去图书馆）
- 获取该商店的所有类别。（1次去图书馆）
- 获取第一类别的所有产品。（1次去图书馆）
- 获取第二类别的所有产品。（1次去图书馆）
- 依此类推。

单个页面加载很容易导致10次、20次甚至50次单独的数据库访问。在分离之前，这50次访问几乎是免费的。现在，它们有了真实世界的成本。

> 50次访问 * 每次访问2ms延迟 = 100ms

突然之间，仅网络延迟一项就增加了十分之一秒的加载时间，即使两台服务器都能立即执行各自的任务。这是我们的新瓶颈。我们不能只是投入更多硬件。我们必须变得更聪明。我们必须优化我们的代码，减少与数据库的"闲聊"。

#### **对抗延迟：进行更少、更智能的访问**

如果每次去图书馆都很昂贵，那么逻辑解决方案是减少访问次数。厨师不应该为每一件物品来回奔波，而应该带着详细的购物清单去一次。在Django世界中，这意味着我们必须积极优化我们的数据库查询。

- **N+1查询问题**：我们发现我们犯了Web开发中最常见的性能杀手："N+1查询"问题。假设你想获取10家商店的列表以及每家商店的第一个产品。一种简单的编码方式是：
  - 进行**1**次查询以获取10家商店。
  - 然后，遍历每家商店并进行N次（在这种情况下为10次）单独查询，以获取每家商店的第一个产品。
    这总共需要11次去图书馆的行程。效率低下！
- **解决方案（select_related和prefetch_related）**：Django有内置工具来解决这个问题。使用名为prefetch_related的功能，我们可以告诉Django："嘿，当你去获取这10家商店时，我知道我还需要它们的产品，所以在那里的时候也一并获取吧。"然后Django会巧妙地执行**2**次查询而不是11次。它会在一次行程中获取所有10家商店，然后在第二次行程中获取它们所有的产品，并在我们的应用程序代码中将数据拼接在一起。这就是我们的"购物清单"。在整个代码库中实施这些优化产生了巨大影响，显著减少了网络调用次数，使应用程序感觉更快。
- **连接池（PgBouncer）**：我们还意识到，为每个请求创建一个新的数据库连接很慢。这就像厨师必须找到他的钥匙，走到图书馆，打开门，获取书籍，锁上门，然后走回来。这有很多开销。为了解决这个问题，我们引入了一个名为**PgBouncer**的工具。它是一个连接池。可以把它想象成一个站在厨房和图书馆之间的保安，持有一组预先解锁的钥匙。当我们的应用程序需要与数据库通信时，它只需向PgBouncer请求一个可用的连接，这会立即获得批准。这为我们节省了为每个小请求建立新连接的开销，进一步减少了我们的有效延迟。

### 第4部分：岔路口 - 为什么我们坚持使用SQL

我们成功完成了伟大的分离。我们的PostgreSQL数据库现在生活在自己强大的服务器上，远离应用程序逻辑的混乱。这是关系型数据库的经典扩展举措。

但在现代科技世界中，一个问题自然浮现：**为什么坚持使用传统的SQL数据库？**为什么不使用我们经常听到的那些快速、水平可扩展的NoSQL数据库，如MongoDB或Cassandra？

这是我们做出的有意识的选择。要理解为什么，你需要理解数据库世界的两种基本哲学。

#### **技术深入：两个数据库星系**

将数据库世界想象成两个平行的星系：SQL和NoSQL。它们都很庞大且强大，但它们运行在不同的物理法则下。

**1. SQL星系（关系型数据库）**

这个星系是**PostgreSQL、MySQL和Microsoft SQL Server**等知名行星的家园。

- **类比**：SQL数据库就像一个完美组织的**Excel工作簿**，包含多个相互关联的电子表格（表）。
- **核心理念：结构和一致性**。数据存储在具有预定义列和严格数据类型的表中（价格列必须是数字，created_at列必须是时间戳）。表之间的关系被严格执行。你不能拥有一个属于不存在商店的产品。
- **超能力：ACID合规性**。这是一套保证（原子性、一致性、隔离性、持久性），确保你的事务非常可靠。用电子商务术语来说，这意味着如果客户订购了五件商品，交易要么保存所有五件商品并更新所有五件商品的库存，要么完全失败且什么都不保存。你的数据**永远不会**处于未完成的损坏状态。
- **最适合**：任何数据完整性和一致性至关重要的应用程序。这包括电子商务、银行、金融系统和任何类型的预订平台。

**2. NoSQL星系（非关系型数据库）**

这是一个更多样化的星系，有**MongoDB**（文档）、**Cassandra**（宽列）、**Redis**（键值）和**DynamoDB**等行星。

- **类比**：NoSQL数据库就像一个**装满灵活Word文档或JSON文件的文件夹**。每个文档可以有自己独特的结构。
- **核心理念：灵活性和可扩展性**。没有预定义的模式。一个产品文档可能有"color"字段，而另一个可能没有。这使得在不必迁移数据库结构的情况下更改应用程序变得容易。它们通常也从头开始设计为水平扩展（跨许多更便宜的服务器）。
- **超能力：BASE和水平可扩展性**。许多NoSQL数据库提供BASE（基本可用、软状态、**最终一致性**），而不是严格的ACID。这意味着系统优先考虑可用性而非即时一致性——这是我们已经通过只读副本探索过的概念。它们的真正力量在于处理海量数据和极高的写入吞吐量。
- **最适合**：大数据应用、社交媒体信息流、物联网传感器数据、实时分析以及数据结构不断演变的应用。

<br/>
#### **快速比较**

| 特性 | SQL（PostgreSQL） | NoSQL（例如MongoDB） |
| --- | --- | --- |
| **数据模型** | 结构化（表和行） | 灵活（文档、键值） |
| --- | --- | --- |
| **模式** | 预定义且严格 | 动态且灵活 |
| --- | --- | --- |
| **扩展** | 主要垂直（更大的服务器）和只读副本 | 主要水平（更多服务器） |
| --- | --- | --- |
| **一致性** | 强（ACID保证） | 可调整，通常是最终一致性（BASE） |
| --- | --- | --- |
| **最适合** | 电子商务、金融、记录系统 | 社交媒体、大数据、物联网、分析 |
| --- | --- | --- |

<br/>
#### **为什么我们选择了SQL的道路**

查看这个表格，我们为Dukaan做出的选择变得清晰明了。

- **我们的数据高度结构化**：订单有客户、一组产品和总金额。产品有名称、价格和库存数量。我们的业务逻辑建立在这些严格的关系之上。NoSQL的灵活性是我们不需要的特性；事实上，SQL的严格性是我们**想要**的特性。
- **数据完整性是一切**：对于电子商务平台，保证订单正确处理、库存准确更新、支付无误反映是用户信任的基础。PostgreSQL的ACID合规性对我们来说是一个不可协商的要求。
- **我们的瓶颈是读取，而不是写入**：正如我们将在第5章中很快发现的，我们最大的扩展挑战不是处理每秒添加的数百万新产品（一个写入密集型问题，NoSQL在这方面表现出色）。而是处理数百万客户**查看**现有产品（一个读取密集型问题）。PostgreSQL有一个对此的出色、成熟且广为人知的解决方案：只读副本。

我们没有"大数据"问题。我们有一个经典的电子商务交易问题。选择一个时尚的NoSQL数据库就像用大锤砸坚果。PostgreSQL是精确、可靠且强大的工具，非常适合这项工作。它是我们知道可以建立十亿美元公司的基础。

<br/>
## 第3章：关键要点

- **分离应用程序和数据库服务器是扩展的第一个关键步骤**。它允许每个组件在不争夺资源的情况下发挥其最佳功能。
- **每个解决方案都会创造新问题**。转向分布式系统引入了网络延迟作为您现在必须考虑的主要性能瓶颈。
- **网络调用成本高昂；尽量减少它们**。对抗延迟的最有效方法是编写更智能的代码，对数据库进行更少、更高效的查询。学习使用select_related和prefetch_related等工具。
- **优化您的连接**。使用PgBouncer等连接池来减少建立新数据库连接的开销，使您的应用程序在负载下更具弹性和性能。

.

## 第4章：交通警察：负载均衡简介

### 第1部分：厨房起火

在初创公司的生活中，解决第一个重大危机后会有一段危险时期。这是一段平静的时期。

在数据库的伟大分离之后，我们的系统运行良好。应用程序感觉很快，服务器很稳定，而且我们第一次感觉我们领先于问题。我们成功地对基础设施进行了心脏手术，患者活了下来。不只是活了下来，它还在蓬勃发展。

我们的日常工作从不断的救火转变为乐观的监控。我们会看着实时用户数攀升，检查服务器负载图表（现在变得美丽地低且稳定），然后互相拍背。我们已经构建了一些真实的、有效的、可扩展的东西。在那个短暂、光荣的时刻，我们感觉有点不可战胜。

就在那时，下一场火灾开始了。

这次不是缓慢的降级。这是一次爆发。一位来自苏拉特的受欢迎卖家，她销售漂亮的手工纺织品，在一个庞大的Facebook群组中分享了她的Dukaan商店链接。与此同时，一家科技博客写了一篇关于我们的小特写。这两个事件结合在一起创造了一场完美风暴——一股前所未有的流量浪潮。

我的手机开始发出警报声。不是来自Suumit，而是来自我们的监控系统。"应用服务器CPU使用率高"，一个警报说。一分钟后，另一个："严重：CPU达到100%已持续5分钟。"

我几乎没有时间打开笔记本电脑，Suumit的短信就来了。短信简短、熟悉，并且带有疲惫的沮丧。

**"Ab kya hua?"**（"现在发生了什么？"）

我通过SSH登录到我们的服务器。我的手指直接指向我信任的诊断工具htop。我先检查了数据库服务器。它完全正常。CPU很低，内存使用稳定。新图书馆安静、有序，轻松处理所有图书请求。

然后我检查了应用服务器。这是一场大屠杀。CPU固定在100%。进程列表是Gunicorn工作进程疯狂滚动的景象，它们试图处理涌入的请求但失败了。服务器完全不堪重负。它活着，但没有响应。对外部世界来说，Dukaan又宕机了。

#### **识别瓶颈：一位厨师，一千位顾客**

我们的厨房起火了。

继续我们的类比，我们成功地在厨房旁边建造了一个最先进的图书馆。我们的厨师不再需要担心管理储藏室。但现在，一千名饥饿的顾客同时涌入餐厅，他们都在大声喊出他们的订单。

我们的单一厨师（我们的一个应用服务器），即使现在更有效率，也根本无法足够快地烹饪。一个人一次可以准备的菜肴数量有物理限制。我们已经达到了那个限制。订单票据的队伍变得如此之长，以至于它溢出了门外，整个餐厅都陷入了停顿。

很明显，我们需要更多的烹饪能力。但是怎么做呢？这导致我们面临每个扩展公司都必须面对的基本选择。一个在两条截然不同的路径之间的选择：向上扩展或向外扩展。

#### **技术深入：垂直扩展与水平扩展**

当您的服务器无法处理负载时，您有两个选择。

**1. 垂直扩展（向上扩展）**

这是最直观的方法。如果你的厨房太慢，你就用一位世界著名的超级厨师来代替你现有的厨师，他能以两倍的速度烹饪。

在服务器术语中，这叫做**垂直扩展**。你在DigitalOcean上点击一个按钮关闭当前服务器。然后选择一个更大、更强大的计划——拥有8个CPU和16GB内存，而不是2个CPU和4GB内存。重新启动它。瞧，你的应用现在运行在一台强大的机器上。这就像把你的家用车换成一辆巨大的怪物卡车。

- **优点：** 极其简单。你不需要更改代码或架构。你只需投入金钱解决问题，问题就会变小。
- **缺点：** 这种策略有三个致命缺陷。
  - **成本快速增长。** 一台两倍强大的服务器价格远不止两倍。它可能花费四到八倍的价格。价格呈指数级增长。
  - **存在硬性限制。** 你不能无限扩展。最终，你会达到提供商能提供的最大、最贵的服务器。然后怎么办？没有更大的怪物卡车可买。
  - **单点故障风险。** 这是最关键的缺陷。你现在有一台非常强大、非常昂贵的单一服务器。如果这台服务器发生硬件故障，或者需要为安全补丁重启，你的整个业务就会离线。你的整个餐厅都依赖于这位超级厨师。如果他生病了，餐厅就会关门。

**2. 水平扩展（向外扩展）**

这是一种不太直观但更强大的方法。不是雇佣一位超级厨师，而是保留你现有的有才华的厨师，并再雇佣三位类似的厨师。你扩建厨房，让他们并行工作。

在服务器术语中，这叫做**水平扩展**。不是一台大服务器，而是创建一组较小的、相同的服务器。不是一辆怪物卡车，而是四辆普通汽车。

- **优点：**
  - **成本效益高。** 你使用的是便宜的商用硬件。向你的服务器群中添加另一台小型服务器是一项小的增量成本。
  - **几乎无限扩展。** 如果你需要更多的能力，只需向车队添加另一辆车。你可以从四台服务器扩展到四十台，再到四百台。系统就是为此设计的。
  - **容错性强。** 这是水平扩展的超级能力。如果你的四位厨师中有一位生病了回家（一台服务器崩溃），其他三位仍在那里烹饪。餐厅可能会暂时慢一点，但它**不会关门**。你消除了单点故障。
- **缺点：** 它引入了一层新的复杂性。如果你在一个大厨房里有四位相同的厨师，服务员带着新订单进来，谁来处理这个订单？你如何决定？

对我们来说，选择是显而易见的。垂直扩展只是临时解决方案，是一个创可贴。这不是一个真正的长期战略。我们正在建立一家希望服务数百万用户的公司，这意味着我们需要一个能与我们一起成长的架构。我们必须学会水平扩展。

我们做出了决定。我们要建立一个应用服务器群。但这意味着我们现在必须解决这个新架构带来的问题。我们需要一个系统来智能地在我们的新厨师团队中分配所有传入的订单。

我们需要一个交通警察。我们需要一个负载均衡器。

### 第二部分：交通警察

我们决定水平扩展是一个重大转折点。我们正在从单一服务器思维转向服务器群思维。但没有指挥官、没有指挥部队的系统，服务器群就毫无用处。我们的厨师团队已经准备好烹饪，但我们需要一个领班服务员来智能地分配传入的订单。

这位领班服务员，这位交通警察，这个关键的拼图碎片，被称为**负载均衡器**。

#### 技术深度解析：什么是负载均衡器？

负载均衡器正如其名所示。它是一个专门的服务器或服务，位于应用服务器前面，唯一的工作是平衡传入流量在这些服务器之间的负载。

对于互联网上的每个用户来说，负载均衡器**就是**你的网站。他们都访问负载均衡器的单一地址（例如，dukaan.app）。用户不知道在这个单一地址背后是两台、十台或一百台准备工作的相同服务器。负载均衡器充当单一前门，隐藏其背后厨房的复杂性。

负载均衡器的最佳类比是**繁忙超市收银台的经理**。

想象一长队顾客（网络流量）等待付款。如果只有一个收银员（单一应用服务器），队伍会很快变得很长。顾客会感到沮丧，收银员会不知所措。

现在，想象商店经理打开四个新的收银台（我们的应用服务器群）。但不是让顾客随机选择一个队伍，经理站在前面，积极引导下一位顾客到下一个可用的收银台。

- "先生，请前往3号柜台。"
- "女士，1号柜台为您开放。"

这位经理就是负载均衡器。他的工作是确保没有一个收银员被压垮，而其他人却闲着。他平滑了顾客流量的高峰和低谷，确保整个系统高效运行。负载均衡器还执行健康检查。如果一个收银员突然昏倒（服务器崩溃），经理立即停止将顾客引导到那个柜台，而是将他们引导到其他正常工作的收银员那里。系统继续运行。

#### 技术深度解析：负载均衡算法

超市经理需要一套规则——一种策略——来决定将下一位顾客送到哪里。在负载均衡的世界中，这些规则被称为**算法**。有许多复杂的算法，但对于我们的需求，我们只需要了解两种最常见的算法。

**1. 轮询：简单但笨拙的方法**

这是最基本的负载均衡算法。它就像它的名字所暗示的那样：它以简单的循环方式将请求分配给服务器。

- 第一个请求发送到服务器A。
- 第二个请求发送到服务器B。
- 第三个请求发送到服务器C。
- 第四个请求回到服务器A。
- ……依此类推。

这就像发牌给一群玩家。每个玩家依次得到一张牌。

- **优点：** 设置极其简单，负载均衡器几乎不需要思考。
- **缺点（"笨拙"部分）：** 它假设每个请求都是相同的，并且每个服务器的能力相等。但是如果发送到服务器B的第二个请求是一个巨大的、复杂的任务，需要10秒处理，而其他请求只需要1秒呢？轮询不在乎。它会盲目地将第四个请求发送到服务器A，第五个发送到服务器B，即使服务器B仍在处理之前的复杂任务，而服务器A完全空闲。这可能导致实际**工作负载**的不均衡分配。

**2. 最少连接：更智能的方法**

这是一种更智能、更动态的算法。负载均衡器积极跟踪当前打开到其服务器群中每个应用服务器的连接数量。当新请求进来时，负载均衡器将其发送到**活动连接最少**的服务器。

这就像聪明的超市经理，不只是按顺序将你送到下一个柜台，而是积极扫描所有队伍，将你送到**当前最短**的一个。

- **优点：** 这种方法自然地考虑到一些请求比其他请求慢的事实。一个忙于处理慢任务的服务器将有更多的开放连接，因此负载均衡器会给它一个"休息"，直到它赶上。这导致工作负载的更公平、更高效的分配。
- **缺点：** 负载均衡器需要稍微更多的开销，因为它必须积极计算连接，而不是仅仅遵循一个简单的列表。

对于Dukaan来说，选择是明确的。"最少连接"算法是更智能、更健壮的选项，可以更好地处理用户流量的不可预测性。

我们现在理解了理论。我们有了交通警察的策略。是时候将其付诸实践了。我们需要选择一个工具来完成这项工作，并配置它来管理我们新的、不断增长的服务器群。

### 第三部分：我们的第一个交通警察

理论是健全的。我们有了建立服务器群的计划和管理它们的策略。现在，是时候动手实际构建它了。

我们的第一个问题是为负载均衡器使用哪种软件。有许多选择，从昂贵的专用硬件设备到AWS的Elastic Load Balancer等云服务。但我们仍然是一家资源有限的初创公司。我们需要强大、可靠、最好是免费的东西。

答案已经在我们的服务器上了。

#### 技术深度解析：作为负载均衡器的Nginx

我们已经在使用**Nginx**作为我们的网络服务器——高效的"服务员"，负责提供静态文件并将请求传递给我们的应用程序。事实证明，Nginx也是世界级的负载均衡器。只需在其配置文件中添加几行额外的文本，我们就可以教会我们现有的服务员同时成为聪明的超市经理。

这是一个巨大的胜利。我们不需要学习或安装一个新的、复杂的技术。我们可以使用我们已经了解和信任的工具。

实现令人惊讶地简单。我在DigitalOcean上启动了第二个相同的应用服务器。现在我们有两位"厨师"准备烹饪。然后，我通过SSH登录到我们的第一台服务器，即我们的域名dukaan.app指向的IP地址的服务器。这台服务器现在将承担负载均衡器的额外角色。

我打开了Nginx配置文件（/etc/nginx/nginx.conf）并添加了两个小的文本块。

**我们的Nginx负载均衡器配置**

```nginx
#定义将处理应用程序工作的服务器组。
#我们将这个组称为"app_servers"。

upstream app_servers {
  #这是魔法规则。它告诉Nginx使用我们讨论过的"最少连接"算法。
  #将流量发送到连接数最少的服务器。
  least_conn;

  #列出我们服务器群中所有服务器的IP地址。
  #这些是私有网络IP，用于提高速度和安全性。
  server 10.132.2.31; # 我们的第一台应用服务器
  server 10.132.4.55; # 我们的第二台应用服务器

  #要扩展，我们只需在这里添加更多行！
}

server   {
  listen 80;
  server_name dukaan.app;

  location  {
    #这是完成所有工作的行。
    #它告诉Nginx将每个传入请求传递给我们在上面定义的"app_servers"组。
    proxy_pass http://app_servers;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
  }
}

```

就是这样。upstream块定义了我们的服务器群。least_conn;行设置了我们的智能路由策略。proxy_pass指令告诉Nginx开始引导流量。保存文件并重启Nginx后，我们的负载均衡器就上线了。

<br/>
#### **新的蓝图**

我们的架构再次进化。流量流程现在更加复杂和弹性。

- 用户访问dukaan.app。他们的请求到达我们的Nginx负载均衡器。
- 负载均衡器查看我们的两台应用服务器，并检查哪一台的活动连接更少。
- 它将请求转发到不太忙的服务器（例如，应用服务器2）。
- 应用服务器2通过运行我们的Django代码来处理请求。为此，它需要数据。
- 应用服务器2连接到我们单一的、共享的数据库服务器以获取必要的信息。
- 响应沿着相同的路径返回给用户。

如果应用服务器1崩溃，Nginx负载均衡器的健康检查会检测到它，并自动停止向那里发送任何流量。所有请求将被路由到应用服务器2。网站将保持在线。我们终于构建了一个容错系统。我们可以处理流量高峰，并且可以在服务器崩溃的情况下生存。我们再次感到无敌。

#### **新问题：图书馆变得拥挤**

有一段时间，这个新设置工作得非常好。当流量增长时，我们不惊慌。我们只需启动第三台应用服务器，将其IP地址添加到Nginx的upstream块中，然后重新加载配置。我们可以在几分钟内向厨房添加更多的"厨师"。

但是当你有十位厨师同时疯狂烹饪时会发生什么？

他们都需要食材。他们都跑向同一个图书馆，都向同一个、单一的图书管理员大声请求。

我们的瓶颈只是移动了位置。它不再是单一应用服务器的CPU能力。我们通过水平扩展解决了这个问题。新的瓶颈正成为上一章拯救我们的东西：我们单一的、整体式的数据库。

随着整个强大的应用服务器群都对数据库发起请求，我们的数据库服务器开始吃力。数据库服务器上的CPU使用率在攀升。查询开始变慢。

我们成功地扩展了我们的"厨房"，但我们的"图书馆"仍然是一个只有单一图书管理员的单一房间。它即将被淹没。

## 第4章：关键要点

- **水平扩展是实现高可用性和大规模扩展的唯一长期路径。** 它比垂直扩展更复杂，但具有成本效益、灵活性，并消除了单点故障。
- **负载均衡器是使水平扩展成为可能的必要交通警察。** 它在服务器群中分配请求并绕过故障。
- **你可以从简单开始。** 像Nginx这样的强大工具可以同时充当你的网络服务器和负载均衡器，减少初始设置的复杂性。
- **"最少连接"是一个智能的默认算法。** 与更简单的"轮询"方法相比，它提供了更均匀的工作负载分配。
- **瓶颈总是会移动。** 当你解决一个性能问题时，负载只是转移到链中的下一个最薄弱的环节。我们的应用服务器不再是问题；我们的数据库即将成为新的火灾。

## 第5章：数据库俱乐部的保镖：只读副本

当你的初创公司从几千用户发展到十万用户时，会发生根本性的转变。早期是关于生存，关于灭火。你的问题是响亮且明显的：服务器宕机，应用程序崩溃。解决方案通常是暴力的：重启它，添加更多内存，使用更大的机器。

但是当你跨越10万用户大关时，会出现一类新的问题。火灾被缓慢、蔓延的热量所取代。系统不会崩溃；它只是变得……沉重。迟缓。这些问题不再关乎生存，而是关于性能。解决方案需要更少的暴力和更多的手术精确性。你必须停止思考如何保持灯亮，而是开始思考建筑物本身的架构。

我们负载均衡、水平扩展的应用服务器群已经解决了"厨房起火"的问题。但是现在，图书馆变得如此拥挤，几乎无法移动。

### 第一部分：图书馆内的交通堵塞

有了负载均衡器的生活很好。我们的应用层是一件美丽的事物。我们可以实时观察流量高峰，看到应用服务器的CPU攀升，然后，只需点击几下，向服务器群添加新服务器，看着负载神奇地分布并稳定下来。我们有控制权。我们有可扩展性。

我们的用户群迅速超过50,000名卖家，然后是80,000名，正迅速接近100,000名的惊人里程碑。每个卖家都有客户，这意味着浏览Dukaan商店的人数达到了数百万。我们提供的流量超出了我们的想象。

但是，那种熟悉的恐惧感开始悄悄回来。我们开始收到投诉，不是关于网站宕机，而是关于它变慢了。

- "我的商店加载需要5-6秒。"
- "有时当我添加新产品时，它只是长时间旋转然后才保存。"

这种迟缓在印度的高峰营业时间（上午11点到下午5点）最为严重。Suumit和我会观看我们的监控图表。应用服务器状况良好，它们的CPU使用率均匀分布，很少超过50%。负载均衡器完美地完成了它的工作。

但是我们单一、强大的数据库服务器的图表讲述了一个不同的故事。CPU持续达到80-90%。衡量存储驱动器繁忙程度的磁盘I/O指标已达到最大值。我们的数据库，曾经是我们救星的强大、隔离的服务器，现在正在喘不过气来。图书馆里挤满了人，我们单一、英勇的图书管理员正在被淹没。

#### **识别瓶颈：太多人只是"看看"**

说"数据库很慢"就像医生说"病人病了"。这不是诊断；这只是观察。要找到治疗方法，我们必须了解具体的疾病。我们需要深入数据库内部，了解它正在处理的**类型**的工作。

这促使我们分析数据库查询，即应用程序发送到数据库的基本命令。

#### **技术深入：分析查询类型（读 vs 写）**

从核心来看，数据库做两种截然不同的工作，理解这种区别至关重要。

- **写查询：** 这些是**更改**数据的操作。主要命令是INSERT（添加新数据）、UPDATE（修改现有数据）和DELETE（删除数据）。
  - **类比：** 这就像作者或图书管理员实际改变图书馆藏书的工作。INSERT就像一本新书到达。UPDATE就像图书管理员纠正卡片目录中卡片上的一个拼写错误。DELETE就是从书架上移除一本旧的、损坏的书。
  - 这些操作至关重要。它们必须小心处理，以维护藏书的完整性。它们通常需要"锁"以确保两个人不会尝试同时更改同一件事。它们通常更慢且更耗资源。对于Dukaan，这就是卖家添加产品、更新价格或客户下订单的操作。
- **读查询：** 这是任何仅**获取**数据的操作。命令是SELECT。
  - **类比：** 这就像公众进入图书馆阅读一本书。他们不会改变任何东西。他们找到一本书，阅读它，然后把它放回原处。他们只是在消费信息。
  - 这些操作通常比写操作快得多且消耗资源少。对于Dukaan，这就是来自浏览商店和查看产品目录的大量客户流量。

我们安装了一个工具来分析数据库流量，我们发现的内容是整个问题的关键。这种模式在Web应用程序中非常常见，以至于它有一个名称。

**95/5规则（或读写分离）**

我们的分析揭示了一种惊人的不平衡。对于每100个命中我们数据库的查询：

- **95个是SELECT查询。**（读取）
- **5个是INSERT、UPDATE或DELETE查询。**（写入）

这完全有意义。一个卖家可能每天更新几次产品（少量写入），但他们的商店可能被成千上万的客户查看（成千上万的读取）。我们的系统绝大多数由读取流量主导。

**读取如何减慢写入速度**

这是我们问题的核心：我们单一的数据库服务器对两种类型的工作给予相同的优先级。它只有一个队列。

再次想象我们的图书馆。只有一个入口和一条排队与图书管理员交谈的队伍。在那条队伍中有95个人只想问："我在哪里可以找到这本书？"（一个快速的读查询）。但在那条队伍中也有5位作者需要注册一本新书，这个过程涉及填写表格和更新主目录（一个较慢的写查询）。

作者们被迫在同一长队中等候，排在大量的休闲读者后面。简单读取请求的庞大数量造成了交通拥堵，延迟了关键的、时间敏感的写入请求。这就是为什么卖家在尝试保存新产品时会遇到长时间延迟——他们重要的"写入"请求被卡在队列中，排在数百个来自匿名购物者的"读取"请求后面。

解决方案变得清晰。我们不能再强迫每个人通过同一个单一的门。我们需要为作者创建一个单独的、专属的入口，同时让阅读公众使用不同的、大得多的入口。我们需要将读取与写入分开。

### 第二部分：保镖和VIP入口

问题很明确。我们的图书馆有一个单一的、拥挤的入口，大量的休闲读者阻碍了重要的作者完成工作。因此，解决方案是建造一个新入口。我们需要一个只为作者准备的私人VIP门，以及一个为阅读公众准备的单独的、大开的大门。

在数据库架构中，这种策略称为**复制**。

#### **技术深入：解决方案——数据库复制**

复制是创建和维护同一数据库的多个副本的过程。我们不再让一个单一的数据库服务器尝试做所有事情，而是现在有一个数据库团队，每个都有专门的角色。最常见的复制形式，也是我们实施的形式，称为**主从复制**。

让我们暂时放弃图书馆类比，考虑一家受欢迎的夜总会。

**主数据库：VIP俱乐部**

**主数据库**是俱乐部的专属VIP区域。它是真相的唯一来源。

- **它处理所有写操作（INSERT、UPDATE、DELETE）。** 俱乐部状态的任何变化——新VIP客人到达、现有客人点饮料或客人离开——都必须在这里登记。一位严厉、严肃的保镖站在门口，确保每一个变化都是合法的并被正确记录。
- **这是我们卖家的入口。** 当卖家更新产品价格、添加新项目或删除旧项目时，他们的请求直接发送到主数据库。这些操作至关重要，在这个不那么拥挤、专属的环境中以高优先级处理。

**只读副本（从数据库）：主舞池**

**只读副本**是俱乐部的主舞池。它是VIP区域发生的一切的完美、实时的副本，但它向公众开放供查看。

- **它只处理读操作（SELECT）。** 成千上万的人（我们的客户）可以同时在舞池上，环顾四周，看看谁在那里，享受音乐。他们可以看向VIP区域并看到一切，但他们自己不能做出任何改变。
- **它的工作是吸收大量的读取流量。** 通过将所有"只是看看"的请求卸载到只读副本，我们释放了主数据库，让它专注于处理更改的重要工作。如果人群变得足够大，我们甚至可以有多个只读副本——几个舞池。

这种关注点分离是我们需要的架构飞跃。它允许我们独立扩展读取和写入。

#### **技术深入：实施**

理论很棒，但在实践中它是如何工作的？主舞池如何神奇地实时知道VIP区域发生了什么？

PostgreSQL中的流式复制如何工作

PostgreSQL为此有一个出色的内置功能，称为流式复制。

- **WAL（预写式日志）：** 主数据库，我们的VIP俱乐部，有一位勤奋的保安，他将发生的每一件事都写在一本特殊的日志簿中。新客人到达？他写下来。价格变化？他写下来。这本日志簿称为**预写式日志（WAL）**。它是对数据库所做的每一个更改的有序、实时记录。
- **流：** 我们设置了一台新服务器，我们的只读副本，并配置它连接到主数据库。副本的第一条指令是："订阅WAL。" 然后，主数据库开始通过安全的私有网络连接实时将日志簿中的每个新条目"流式传输"到副本。
- **应用：** 只读副本接收这股更改流，并以完全相同的顺序将它们应用到自己的数据副本上。

结果是，副本始终是主数据库近乎完美、实时的镜像。这就像从VIP区域的实时视频源被广播到主舞池上方的大屏幕上，供所有人观看。

更新我们的应用程序使其具有"复制感知"能力

设置服务器只是战斗的一半。我们的Django应用程序仍然"愚蠢"。它只知道如何与一个数据库交谈。我们必须教它变得聪明，成为决定谁去VIP入口、谁去主楼层的保镖。

这是对我们代码库的重大更改。

- **多个数据库配置：** 首先，在我们的Django设置中，我们配置了两个数据库连接而不是一个：一个默认连接指向主数据库的IP，一个read_replica连接指向新副本的IP。
- **创建数据库路由器：** 接下来，我们实现了一个自定义的"数据库路由器"。这是Django中的一个特殊代码，它在每个数据库查询发生之前拦截它，并决定应该将其发送到哪个数据库。逻辑很简单，但很关键：

```Python
#我们路由器逻辑的简化版本
class PrimaryReplicaRouter:

  def db_for_read(self, model, \*\*hints):
  #所有读取操作都去副本。
    return 'read_replica'

  def db_for_write(self, model, \*\*hints):
  #所有写入操作都去主数据库。
    return 'default'
有了这个路由器，我们的应用程序现在变得智能了。每次客户加载商店页面（触发几十个SELECT查询）时，路由器都会将所有这些流量发送到强大的只读副本。但是当卖家点击新产品的"保存"按钮（触发INSERT或UPDATE查询）时，路由器会将这个单一的、关键的请求发送到受保护的、不太繁忙的主数据库。

我们部署了这些更改。差异是立即且显著的。商店页面立即加载。卖家报告说保存更改再次变得迅速。我们主数据库上的高CPU和I/O负载几乎降至零。我们做到了。我们成功地扩展了我们的数据库。

<br/>
### **你无法逃避的三角形：CAP定理**

当我们第一次推出复制时，结果感觉很神奇。主数据库处理写入，副本处理读取，整个俱乐部突然流畅运行。卖家可以更新他们的目录，而不会被休闲购物者的浪潮压垮。顾客可以浏览而不用排队。看起来我们找到了完美的系统。

但是分布式系统从不会免费给你完美。计算机科学中有一个古老的原则，我过去曾略读但现在每天都在面对：CAP定理。

CAP代表**一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）**。它指出，在任何分布式数据系统中，你最多只能同时保证其中两个属性。你不能拥有全部三个。

- **一致性**意味着俱乐部中的每个房间在同一时间看到相同的东西。如果VIP区域更改了播放列表，舞池应该立即听到新歌曲。
- **可用性**意味着门总是开着的。无论如何，俱乐部永远不会拒绝客人——每个请求都会得到某种回答。
- **分区容错性**意味着即使房间之间的走廊被阻塞，俱乐部也能继续运行。也许VIP休息室和舞池之间的音响系统链接出现故障——聚会不能因为这个而停止。

这里的技巧是：在现实世界中，分区是不可避免的。网络故障，数据包丢失，电缆被切断。因此，每个实际系统在分区发生时都必须在一致性和可用性之间做出选择。

当我们引入副本时，我们正在做出那个选择——无论我们是否意识到这一点。我们选择了**可用性而非严格的一致性**。舞池（副本）总是开放并准备好为客户服务，即使它还没有跟上VIP区域（主数据库）的步伐。结果是，有时舞池显示的信息略微过时。

### **具体例子**

假设一个卖家在VIP区域将一件连衣裙的价格从1000卢比更新到800卢比。主数据库立即记录下来。

- 如果下一个请求直接进入主数据库，客人看到的是800卢比。  
- 如果它在更新流过来之前到达副本，客人仍然看到的是1000卢比。  

这两个答案都"有效"，取决于你站在哪个房间。不过，从卖家的角度来看，它看起来像是坏了。他们刚刚更改了价格——为什么店面仍然显示旧价格？

<br/>
### **为什么CAP很重要**

CAP不是锁在教科书里的理论；它是一个隐形的三角形，每当你添加副本、分发数据或跨区域同步时，你都在与它搏斗。我们拥抱复制的那一刻，我们就进入了一个某些读取可能落后于写入的世界。这不是一个错误。这就是CAP，提醒我们分布式系统总是让你有所取舍。

### **一致性的不同层次**

一旦你接受了CAP，下一个问题就变成了：如果我们不能拥有一切，我们真正想要什么样的一致性？在实践中，答案不止一个。分布式系统存在于一个一致性模型的谱系上，你使用过的每个产品都会根据最关心的事情做出不同的选择。

以下是你会遇到的三种主要模型：

- **强一致性**
    这是人们直觉上期望的世界。如果卖家将产品价格更新为800卢比，那么之后的**每一次读取**——无论它命中哪个服务器——都必须返回800卢比。  
    在夜总会类比中，VIP房间的DJ一旦更换曲目，舞池立即听到新歌曲，没有例外。  
    强一致性感觉很干净，但它通常以可用性为代价。如果VIP和舞池之间的走廊哪怕被阻塞片刻，俱乐部宁愿停滞也不愿冒险让任何人听到"错误"的歌曲。  

- **最终一致性** 
    这是副本真正的生存环境。VIP区域的更新尽快流到舞池，但不是立即。如果你运气不好，你可能会在更改通过前再听几拍旧歌。  
    从用户的角度来看，这可能会令人困惑：他们刚刚保存了新数据，但店面仍然显示旧值。给足够的时间，一切都会赶上，所有房间都会同步——但"足够的时间"可能是一秒或五秒，你无法准确预测何时。  

- **因果一致性** 
    这是一个中间立场，试图保留因果关系的顺序。如果Priya降低了她项链的价格，然后查看自己的商店，因果一致性保证**她**会看到她的更新，即使世界其他地方还没有看到。  
    在夜总会：如果DJ更换曲目，任何在VIP房间亲眼目睹这一过程的人将始终听到新曲目，即使舞池上的人仍在追赶。  
    它不保证完美的全局一致性，但它保护了"我更改了某些东西，因此我应该看到更改"的逻辑。  

### **选择适合的模型**

不同的系统选择不同的模型。银行软件要求强一致性——你不希望一个分行显示余额为10,000卢比，而另一个显示5,000卢比。社交网络倾向于最终一致性——如果你的点赞数落后几秒钟，没有人会惊慌。因果一致性在面向用户的应用程序中越来越受欢迎，因为它平衡了规模和用户对即时性的个人期望。

在Dukaan，我们在采用副本时基本上构建了一个最终一致性系统。这就是为什么Priya有时会看到"旧数据的幽灵"。她的经历不是一个错误，而是最终一致性的典型案例。

但就像每个解决方案一样，这种新架构引入了一个新的、微妙的、潜在危险的副作用。

<br/>

#### **新问题：复制延迟**

从主数据库到副本的实时数据流非常快，但不是**即时的**。总会有微小的延迟，以毫秒为单位。在高负载下，这种延迟有时可能会飙升到整整一两秒钟。这种延迟称为**复制延迟**。

这意味着主舞池对VIP区域的视图比现实落后一小部分时间。

这会产生一系列非常混乱的潜在问题。当卖家将产品价格从100卢比更新到90卢比（写入主数据库），立即在他们的商店页面上点击刷新（从副本读取），但由于更改尚未流过来，仍然看到价格为100卢比，这会发生什么？

这是**最终一致性**这个混乱且危险的世界。

### 第三部分：旧数据的幽灵

从所有技术指标来看，我们的新架构是一个巨大的成功。系统速度快、稳定，处理着10万用户。从系统工程的角度来看，我们赢了。但从用户的角度来看，我们刚刚引入了一个非常奇怪、几乎神奇且极其混乱的新问题。

想象一个卖家，我们叫她Priya。她经营着一家销售定制珠宝的小精品店。她登录到她的Dukaan仪表板，看到她最受欢迎的一条项链标价为1000卢比。她决定进行限时促销，将价格更改为800卢比。她点击"保存"。系统立即响应："产品更新成功！"

为了仔细检查她的工作，Priya立即点击"查看商店"按钮，以客户的视角查看。她看了看项链。价格仍然是1000卢比。

她的心一沉。没有保存吗？她回到管理面板。它显示价格为800卢比。她回到她的商店页面。它显示1000卢比。她现在困惑并开始恐慌。她的商店坏了吗？她的客户是否被多收费了？她一遍又一遍地刷新页面。1000卢比。1000卢比。然后，在疯狂刷新五秒钟后，它突然变成了800卢比。

Priya刚刚经历的就是旧数据的幽灵。她是复制延迟的受害者。她的"保存"操作是一个立即写入主数据库的写入。她的"查看商店"操作是一个发送到我们只读副本的读取，而该副本在那一刻落后于主数据库一小部分时间。

这不是代码中的错误。这是我们刚刚构建的高性能新系统的一个基本特性。我们以即时一致性换取了大规模可扩展性。我们进入了**最终一致性**的世界。

#### **技术深入：最终一致性**

要理解这个概念，你需要将它与每个人自然期望的进行比较。

- **强一致性：** 这是我们都习惯的世界。当你在银行转账时，你期望余额在任何地方都能立即更新。在写操作之后，**保证**每一个后续的读操作都能看到新数据。单个服务器与单个数据库默认提供强一致性。只有一个真相来源。
- **最终一致性：** 这是分布式系统的世界，是我们现在生活的世界。系统**保证**，如果你停止更改，所有数据副本**最终**会看起来相同。它不承诺这需要多长时间。它承诺一致性会发生，但不一定是立即的。

这就是权衡。我们牺牲了即时一致性的保证，以获得处理数百万次读取的能力。对于我们99.9%的用户（浏览网站的客户），看到价格更新延迟一秒钟是完全可以接受的；他们甚至不会注意到。但对于那些0.1%是更改**原因**的用户（像Priya这样的卖家），那一秒钟的延迟是一种刺耳且不可接受的用户体验。

我们无法消除复制延迟——这是一个物理限制。但我们必须找到一种方法来保护我们的卖家免受其影响。

#### **技术深入：处理数据过时的策略**

如何解决这样的问题？你不能让系统更快，所以你必须让应用程序更智能。

策略1：什么都不做（以及何时合适）

对于许多功能，少量的延迟是完全可以接受的。例如，如果我们有一个显示"商店总数"的管理仪表板，那么该数字是否过时30秒并不重要。你必须有意识地识别应用程序中哪些部分需要强一致性，哪些可以容忍最终一致性。

策略2："写后读"解决方案（VIP通行证）

这是我们实施的解决Priya问题的策略。逻辑很简单：对于特定用户，在他们执行写操作后立即，我们应该暂时打破自己的规则，也将他们的读查询发送到主数据库。

这是Priya的VIP通行证。

- Priya为她的项链保存一个新价格。这是一个写入操作，发送到**主数据库**。
- 我们的应用程序看到这个成功的写入，并在Priya的会话中设置一个临时标志（就像她浏览器中的cookie），上面写着："这个用户在接下来的60秒内处于VIP窗口中。"
- Priya立即刷新她的商店页面。这是一个读查询。
- 我们的数据库路由器看到来自Priya的请求。它检查她的会话并看到"VIP窗口"标志。
- 路由器没有像对待其他用户那样将她的读取请求发送到只读副本，而是直接发送到**主数据库**。
- 由于主数据库始终拥有绝对最新的数据，它返回正确的新价格₹800。Priya立即看到她的更改，她对平台的信心得以维持。
- 一分钟后，她会话中的VIP标志过期。她的下一个读取请求将像正常情况一样发送到副本，到那时数据早就已经被复制了。

这种方法给了我们两全其美的好处：为普通大众提供大规模可扩展性，同时为实际进行更改的用户提供强烈一致性的感觉。

## 第5章：要点总结

- **使用只读副本扩展数据库是一个巨大的性能提升，但它是有代价的。** 你正在用强一致性的简单性换取最终一致性的复杂性。
- **复制延迟是物理现实，而不是错误。** 在主数据库和副本之间总会有一个小延迟。你无法消除它，所以你必须设计你的应用程序来处理它。
- **最终一致性可能会创造一种刺耳且令人困惑的用户体验。** 用户在做出更改后立即看到旧数据可能会严重损害他们对产品的信任。
- **实施"读取自己的写入"策略。** 对于刚刚修改数据的用户，暂时将他们的读查询路由到主数据库。这在最关键的地方提供了即时一致性的错觉，同时不牺牲只读副本的可扩展性。

## 第6章："不要在生产环境测试，兄弟！"：预发环境

到目前为止的旅程是关于与外部力量斗争。我们与服务器限制、流量峰值和物理定律作斗争。它们是规模扩大的光荣战斗，是我们自己成功的结果。本章是关于另一种战斗。这是与我们最大的敌人：我们自己的战斗。

随着公司的成长，你从一个两人军队变成一个小型排。你雇佣了你的第一批工程师。开发速度增加了，但复杂性和风险也增加了。让你起步的非正式、"快速行动，打破常规"的文化可能很快成为烧毁公司的原因。

这是我们第一次自找的灾难的故事，以及我们学到的关于在学会飞行之前建立安全网的关键教训。

### 第一部分：破坏一切的错误

随着我们新的、可扩展的架构到位，我们开始招聘。我们的团队从只有Suumit和我发展到几个才华横溢、热情洋溢的工程师。能量令人难以置信。我们比以往任何时候都更快地推出新功能。新的支付选项、产品过滤器、更好的订单管理方式——速度令人振奋。

回想起来，我们部署代码的过程简单得可怕。工程师会在他们的笔记本电脑上编写一些代码，测试它是否对他们有效，然后将其推送到我们在GitHub上的中央代码仓库。一个简单的脚本然后会自动获取这个新代码并直接部署到我们的实时生产服务器——我们十万卖家和他们数百万客户正在使用的服务器。

从开发者的大脑到实时用户的屏幕不到五分钟。我们认为这是敏捷性的巅峰。实际上，这就像在没有安全网的情况下进行空中飞人表演。

一个星期二下午，一位新的初级开发人员——我们叫他Rohan——负责添加一个简单的功能：一个按价格排序产品的按钮。他是个聪明的孩子，几个小时内就构建了这个功能。在他的笔记本电脑上，使用只有大约15个产品的测试商店，它完美地工作。产品立即排序。对自己的工作充满信心，他推送了代码。

五分钟后，我的手机爆炸了。

这不是服务器警报。是Suumit。而且他并不冷静。

"Subhash！我们的顶级卖家宕机了！gavranmisal.com，Jain Shikanji——所有的大客户！他们的商店无法加载。他们在给我打电话，他们正在损失钱！发生了什么？"

我跳到我的监控仪表板。服务器没有宕机。CPU很好，内存也很好。这不是扩展问题。这是代码问题。一个错误。

我们匆忙行动。我们检查了最近的代码推送，看到了Rohan的更改。查看他的代码，逻辑似乎没问题，但我的眼睛捕捉到了一个特定的数据库查询。这是一个简单的ORDER BY price子句。但它的编写方式在有数千个产品的商店上会非常低效。在Rohan有15个项目的测试商店上，它是即时的。在gavranmisal.com有2000个项目的商店上，查询会超时，崩溃页面加载过程。他意外地发布了一个只影响我们最大、最重要客户的错误。

接下来的十分钟纯粹是充满肾上腺素的混乱。我们必须执行紧急"回滚"，将代码回滚到之前版本的过程，而我们的实时系统正在着火，我们的支持渠道充斥着卖家的愤怒消息。

我们最终修复了它。商店恢复了在线。但损害已经造成。我们让用户失望了。而这不是因为流量峰值或硬件故障。这是100%我们自己的错。

那天晚上，Suumit和我进行了一次紧张的通话。

"这绝不能再发生，"他说，声音尖锐。"我们看起来像业余爱好者。将代码从笔记本电脑直接推送到实时站点是疯狂的。这就像厨师第一次在总理身上尝试新食谱。我们需要一个安全网。一个在东西上线前进行适当测试的地方。"

他是对的。我们一直在没有降落伞的情况下飞行。是时候成熟了。

<br/>
#### **解决方案：建立安全网**

我们问题的根源在于我们的代码只存在于两个地方：在开发人员的笔记本电脑上，以及在实时客户面前。中间没有任何步骤。一个专业的软件团队需要一个适当的装配线，每个阶段都有质量检查。这就是**软件开发生命周期**。

#### **技术深入：环境**

想象一家高端餐厅。他们不仅仅是烹饪和服务。他们有一个严格的过程，涉及三个不同的环境。

1. 开发环境（测试厨房）

这是开发人员的笔记本电脑。

- **类比：** 这是厨师的个人测试厨房。这是一个创造性的、混乱的、隔离的空间。在这里，厨师可以尝试疯狂的新食谱，尝试奇怪的食材组合，犯错而不会产生后果。没有客户会品尝直接来自测试厨房的菜肴。
- **我们的错误：** Rohan在他的测试厨房测试了他的新食谱，那里只储存了一顿小餐（一个有15个产品的商店）的食材。它在那里工作得很好。他没有食材来测试如果他必须为宴会（一个有2000个产品的商店）烹饪会发生什么。

2. 生产环境（餐厅）

这是与真实用户交互的实时服务器。

- **类比：** 这是餐厅的主餐厅，挤满了付费顾客。每一道从这个厨房出来并放在桌子上的菜都必须完美。这里的错误是公开的、羞辱性的，并会损害餐厅的声誉。
- **我们的错误：** 我们直接从测试厨房拿菜并在主餐厅供应。我们向最重要的客户提供我们的实验。

3. 预发环境（彩排）

这是我们缺少的关键部分。预发环境是一个完整的、平行的宇宙，是生产环境的精确镜像。

- **类比：** 这是一个设备齐全、与主厨房完全相同的副本，位于后面，配备了相同的烤箱、相同的员工和相同的高质量食材。在新菜正式加入菜单之前，厨师必须首先在这个"预发厨房"中在完整的"彩排"期间完美地烹饪它。他们将其提供给餐厅经理和员工（内部测试人员），他们表现得像真实客户。他们测试整个过程——从接单到压力下烹饪到最终装盘。只有当一道菜通过这个严格的、真实世界的测试时，它才被批准进入主餐厅。

这就是我们的安全网。一个Rohan可以部署他的新代码的地方，我们可以在一个有2000个产品的商店副本上测试它。这个错误会立即变得明显。页面会在预发环境中崩溃，而不会有一个真实客户受到影响。

我们知道我们必须做什么。我们需要为彩排构建一个主舞台的完美复制品。

### 第二部分：构建镜像

构建预发环境的决定对我们来说是一个关键时刻。这是一个声明，表明我们正在从一个混乱的车库乐队转变为一个专业的管弦乐队。管弦乐队需要一个专用的排练空间，我们即将构建我们的排练空间。

但是构建一个_有用的_预发环境比听起来要困难得多。这不仅仅是启动另一台服务器并将代码部署到上面。一个糟糕的预发环境可能比没有更糟糕，给你一种虚假的安全感。为了成为有效的安全网，我们的彩排舞台必须是真实事物的完美、精确到毫米的镜像。

#### **技术深入：环境一致性的重要性**

这是预发环境的黄金法则：**你的预发环境必须尽可能与你的生产环境相同**。

为什么？因为微妙的差异是bug喜欢隐藏的地方。

- 如果你的预发服务器的RAM比生产环境多，你永远不会发现内存泄漏的bug。
- 如果它运行的Python版本比生产环境新，你的代码可能在预发环境中工作，但由于库不兼容而在实时站点上崩溃。
- 如果它的网络规则不同，一个功能可能在预发环境中工作，但在生产环境中失败，因为防火墙阻止了它。

你不能在一个有纸板道具的小型高中舞台上为大型百老汇演出进行彩排，并期望发现所有问题。你需要一个具有相同尺寸、相同灯光和相同音响效果的舞台。

对我们来说，这意味着一项重大的新投资。我们必须复制我们的整个生产架构：

- **相同的"硬件"**：我们为预发环境启动了新的DigitalOcean Droplets，它们具有与我们的生产服务器完全相同的CPU、RAM和SSD规格。
- **相同的软件**：我们使用配置脚本来确保我们的预发服务器具有完全相同版本的Ubuntu、Python、Django、PostgreSQL、Nginx、Gunicorn以及我们依赖的所有其他库。
- **相同的架构**：我们的生产设置现在有一个负载均衡器、两个应用服务器和一个只读副本数据库。我们的新预发环境必须具有相同的配置：一个预发负载均衡器、两个预发应用服务器和一个预发主/副本数据库设置。

这实际上使我们的服务器成本翻倍。对于一个自举的初创公司来说，这是一笔痛苦的支出。但我们在脑海中重新构建了它。这不是成本；这是一笔**保险费**。我们支付可预测的月费，以确保我们免受灾难性、损害声誉的生产中断的巨大、不可预测的成本。

#### **技术深入：数据填充和清洗的挑战**

我们已经构建了一个完美的、空的舞台。但是没有演员和道具的彩排是毫无用处的。如果预发环境没有填充真实的、大规模的数据，那么它也是毫无用处的。毫无疑问，这是维护有用预发环境最困难的部分。

我们关于Rohan功能的bug之所以发生，是因为他在一个有15个产品的商店上测试，而生产环境中的bug只在产品超过1000个的商店上出现。为了捕捉这类bug，我们的预发环境需要镜像生产环境_规模_和_复杂性_的数据。

显而易见但危险错误的解决方案是简单地克隆你的生产数据库并将其加载到预发环境中。**你绝不能这样做**。

你的生产数据库包含用户最敏感的信息：他们的姓名、电话号码、电子邮件地址、私人订单历史。将这些数据复制到多个开发人员可以访问的不太安全的预发环境中，是对安全性和隐私的严重侵犯。这不仅仅是不良实践；它可能是非法的。

所以，我们面临一个两难境地：我们需要生产数据的规模，但我们不能使用生产数据本身。

解决方案是构建一个**数据填充和清洗管道**。这是一个自动化脚本，每晚执行两个步骤的过程：

步骤1：数据填充

脚本首先使用pg_dump对我们的实时生产数据库进行完整备份。这为我们提供了当时数据的完整、结构完美的快照。

步骤2：数据清洗

这是关键步骤。在将此备份加载到预发数据库之前，脚本会通过一个"清洗器"运行它，以清除所有敏感信息：

- **匿名化用户数据**：它会遍历用户表，将真实姓名替换为假名字，如"Test User 1234"。它会将电子邮件地址打乱为<testuser1234@example.com>，并将真实电话号码替换为随机生成的假号码。
- **模糊化财务数据**：它会将真实产品价格和订单总额更改为真实但随机化的值。
- **保留规模和结构**：至关重要的是，脚本_不会删除_数据。如果一个生产商店有2000个产品，清洗后的预发副本也有2000个产品，但名称和价格是打乱的。如果一个用户有500个订单，预发环境中匿名化的测试用户也有500个订单。

这个过程构建起来很复杂，需要不断维护。但它给了我们预发环境的圣杯：一个完美镜像生产环境规模和复杂性的数据库，但没有零敏感用户数据。

现在，Rohan可以在预发服务器上针对gavranmisal.com的2000个产品商店的清洗副本测试他的新排序功能。这个bug会导致预发站点崩溃，他会修复它，没有真实客户会知道这一切。

我们有了测试厨房（开发人员的笔记本电脑）、主餐厅（生产环境），现在还有一个设备齐全的专业彩排舞台（预发环境）。拼图的最后一块是创建一个正式、安全、可重复的流程，在它们之间移动代码。我们需要一个装配线。我们需要一个部署管道。

### 第三部分：装配线

我们已经构建了我们的环境。我们有测试厨房（开发环境）、彩排舞台（预发环境）和主餐厅（生产环境）。这是一个巨大的进步。但拥有房间是不够的；你需要一种安全高效的方式在它们之间移动菜肴。

我们以前开发人员手动运行脚本将代码直接推送到生产环境的方法，就像厨师拿着一个燃烧的锅从测试厨房直接冲向餐厅一样。它快速、令人兴奋，并且最终一定会以灾难告终。

我们需要用一个冷静、有序和可预测的过程来取代这种混乱的冲刺。我们需要为我们的代码构建一个装配线。在技术世界中，这被称为**部署管道**。

#### **技术深入：部署管道**

部署管道是一个自动化过程，它将开发人员笔记本电脑上的代码在最终交付给用户之前，安全地通过一系列质量检查。

把它想象成现代汽车工厂中的装配线。开发人员的代码是生钢。管道是一系列传送带和机械臂，它们自动将钢从一个工作站移动到下一个工作站。在每个工作站，运行测试并执行质量检查。只有通过每个工作站的每一项检查的汽车才被允许推出到展厅。

管道的目标是让部署变得**无聊**。部署不应该是一个高风险的戏剧性时刻和祈祷。它应该是一个常规、可预测和可重复的事件。无聊是好的。无聊意味着网站没有着火。

我们设计了我们的第一个简单的部署管道，包含一系列刻意的手动和自动步骤。

步骤1：GitHub上的Pull Request

整个过程始于开发人员完成编写他们的代码。他们现在不再直接将代码推送到主代码库（master分支），而是在GitHub上打开一个Pull Request（PR）。

PR是一个正式的请求："我已经完成了这个功能的工作。这是代码。请审查并批准将其合并到主项目中。"这是我们装配线的入口点。这是到达工厂门口的生钢。

步骤2：人工质量检查（代码审查）

这对我们来说是一个巨大的文化转变。在任何一行新代码能够沿着装配线前进之前，它必须至少由团队中的另一位工程师审查和批准。

这种"第二双眼睛"是一个非常强大的质量检查。审查者寻找诸如：

- 明显的bug或逻辑错误。
- 低效的数据库查询（如导致我们上次中断的查询）。
- 难以阅读或理解的代码。
- 安全漏洞。

这个简单的、以人类为中心的步骤迫使协作和代码的共同所有权。这是在bug到达服务器之前捕获它们的强大方法。

步骤3：自动化测试和部署到预发环境

一旦有人批准了Pull Request，机器就接管了。我们设置了一个自动化系统（使用一个名为GitHub Actions的工具），它会自动触发：

- **运行自动化测试**：系统首先会运行我们的整个"单元测试"和"集成测试"套件。这些是小型的自动化检查，用于验证新代码按预期工作，并且没有意外破坏任何现有功能（这被称为"回归"问题）。
- **部署到预发环境**：如果所有自动化测试都通过，系统会自动将代码合并到我们的预发分支并部署到我们的预发环境。

该功能现在已在我们完美的、镜像的彩排舞台上上线，并填充了经过清洗的生产规模数据。

步骤4：最终彩排（预发环境上的手动QA）

这是最终也是最关键的检查点。人类测试人员（开发人员自己或专门的QA人员）现在必须在预发服务器上手动测试该功能。

他们会按照清单操作，表现得像真实用户一样。"按价格排序"按钮是否工作？它在有5个产品的商店上是否工作？它在我们有5000个产品的商店的清洗副本上是否工作？它在移动浏览器上是否工作？它是否破坏了页面上的其他内容？

只有在该功能通过这个严格的、真实世界的手动测试后，它才能被批准进入最后一步。这是本可以捕获Rohan的bug并使我们免受生产中断影响的步骤。

步骤5：推送到生产环境

部署到生产环境现在是最后、深思熟虑和冷静的一步。它不再是一个疯狂的、临时的事件。一旦功能在预发环境中被批准，我们就会将代码合并到我们的master分支。这个合并会触发管道的最后阶段，该阶段会将代码推送到我们的实时生产服务器。

这个过程改变了我们的团队。它用秩序代替了混乱，用信心代替了焦虑。我们不再只是一群编码员；我们正在成为一个专业的工程组织。

<br/>
## 第6章：要点总结

- **预发环境是防止自找的中断的不可协商的保险单。** 构建和维护它的成本与生产故障的成本相比微不足道。
- **你的预发环境必须是生产环境的镜像。** 相同的硬件、软件和架构对于捕捉真实世界的bug至关重要。
- **绝不在预发环境中使用原始生产数据。** 构建自动化管道，用经过清洗和匿名化的数据填充你的预发数据库，以保护用户隐私。
- **部署管道用可靠的自动化过程取代了混乱的手动步骤。** 它强制执行代码审查、自动化测试和手动QA等质量检查。
- **一个好的流程的目标是让部署变得无聊。** 无聊意味着可预测。可预测意味着可靠。对于一个不断增长的业务来说，可靠性就是一切。

## 第7章：速度需求：使用Redis进行缓存

我们已经在稳定性的战争中幸存下来。我们的架构现在具有弹性，我们的部署过程是专业的，我们的系统可以承受崩溃和流量峰值。我们已经从一个车库乐队成长为一个经过良好排练的管弦乐队。我们的用户基础已经超过了100万卖家大关，几个月前这还是一个梦想。

但是一个新的挑战正在出现，一个比服务器崩溃更安静但同样危险的挑战。我们的问题不再是关于_可用性_；而是关于_性能_。我们的商店仅仅在线是不够的；它们必须快速。在电子商务的世界里，速度不是一个功能；它是一个基本要求。页面加载时间延迟一秒可能导致转化率显著下降。

我们即将了解到，一个好产品和一个伟大产品之间的区别可以用毫秒来衡量。

<br/>
### 第一部分：gavranmisal.com的投诉

电话来自我们的一位明星卖家。gavranmisal.com的老板，一位来自浦那的非常受欢迎的餐厅老板，是我们最早的采用者之一。他们有一个庞大、复杂的菜单，有几十个类别和项目，他们为他们的Dukaan商店带来了大量流量。对我们来说，他们是一个完美的成功案例。

而且他们很不满意。

Suumit接听了电话。老板的投诉不是网站宕机了。在很多方面，情况更糟。"我的客户抱怨说，商店加载需要5或6秒，"他说，声音中充满了沮丧。"他们很有耐心，但不会那么有耐心！人们甚至在看到菜单之前就离开了。这正在耗费我的业务。"

这是一种新的火灾。一种缓慢燃烧的火灾。这不是技术中断；这是一个业务问题。我们为赋能卖家而构建的平台现在因为它的缓慢而积极地伤害他们。

我的第一反应是查看我们的监控仪表板。我把它们拉起来，期望看到一台服务器处于压力之下。但一切看起来都...很好。负载均衡器完美地分发流量。应用服务器的CPU几乎没有超过30%。只读副本数据库处理查询没有任何压力迹象。根据我们的图表，整个系统是健康的，并且在其限制范围内运行良好。

然而，用户的实际体验是6秒的页面加载时间。我们的系统_能够_处理什么和用户实际_体验_什么之间存在巨大脱节。我们必须更深入地挖掘。

#### **识别瓶颈：重复的数据库查询**

我们使用了一个名为Django Debug Toolbar的工具，它允许你检查在单个页面加载期间发生的一切。我们加载了gavranmisal.com商店页面，同时激活了工具栏，慢加载的原因像一块砖头一样击中了我们。

为了渲染那个单一页面，我们的应用程序向我们的只读副本数据库发出了**114个单独的SELECT查询**。

我们获取商店的详细信息，然后是其主题设置，然后是所有类别，然后是第一个类别的所有产品，然后是第二个类别的产品，依此类推。我们**每次**单个用户访问页面时都这样做。

尽管我们的只读副本很强大，这114个查询中的每一个单独都非常快（可能每个5-10毫秒），但累积效应是毁灭性的。

> 114个查询 * 每个查询10毫秒 = 1140毫秒

> 这仅数据库时间就超过了整整一秒，这被称为"千刀万剐之死"。加上每个调用的网络延迟和我们的服务器渲染页面的时间，5-6秒的加载时间开始变得完全合理。

核心见解是这样的：gavranmisal.com菜单不会每秒都变化。事实上，它可能每天只更新一次或两次。然而，我们的系统忠实地为每小时访问页面的数千名客户中的每一位，从零开始，一块一块地从数据库重建整个菜单。

我们一遍又一遍地执行相同的昂贵计算，结果总是相同的。这就是低效率的定义。

#### **技术深入：缓存原则**

重复工作问题的解决方案是一个对世界上每一个高性能系统都至关重要的概念，从你的计算机CPU到全球互联网。这个概念就是**缓存**。

为了理解缓存，让我们使用一个简单的类比。

想象一位数学教授问你："135乘以782是多少？"

第一次，你可能会拿出手机的计算器，仔细输入数字，得到答案：**105,570**。这是一个"昂贵"的操作。它花了你几秒钟的时间和脑力。这就是你的应用程序查询数据库。

现在，想象教授在五秒钟后问你完全相同的问题。你会怎么做？你不会再次拿出计算器。你只是记住了答案。你已经将它存储在你的大脑的短期记忆中。你可以立即回答。你已经**缓存**了结果。对于那个特定的问题，你的大脑现在比计算器无限快。

这就是缓存的原则：

- 识别一个**昂贵**的操作。
- 这个操作被**频繁请求**。
- 每次都产生**相同的结果**。
- **只执行一次**这个操作。
- 将结果存储在一个**更快、临时的位置**（缓存）。
- 对于所有后续请求，从缓存中提供结果，而不是再次执行昂贵的操作。

我们的商店页面是缓存的完美候选者。该操作（从数据库获取114个项目并构建页面）很昂贵。它被请求数千次。而且对于99.9%的这些请求，结果是相同的。

我们需要为我们的应用程序构建一个"短期记忆"。一个存储最终的、随时可用的商店页面的地方，这样我们就不必每次都从数据库重新构建它们。是时候引入我们整个技术栈中最重要的工具之一：**Redis**。

### 第二部分：白板

我们知道我们需要一个缓存。我们需要为我们的应用程序构建一个"短期记忆"，以存储它被问到的重复问题的答案。下一步是为这项工作选择正确的工具。我们需要一些非常快速、易于使用且可靠的东西。选择几乎是立即的，因为在内存缓存的世界中有一个无可争议的王者：**Redis**。

#### **技术深入：Redis是什么？**

Redis（代表**RE**mote **DI**ctionary **S**erver）是一个开源的内存数据存储。为了理解这意味着什么，让我们来分解它。

**内存存储 vs. 磁盘存储**

这是最关键的概念。

- 像**PostgreSQL**这样的传统数据库主要是**基于磁盘**的。它将数据存储在固态硬盘（SSD）或硬盘驱动器（HDD）上。可以将其视为一个**图书馆**。它庞大、永久且组织良好。但要获取一条信息，图书管理员（数据库引擎）必须实际走到一个货架，找到一本书。这个过程虽然在人类看来很快，但对于计算机来说需要可测量的时间。
- **Redis**是一个**内存**数据库。它直接将所有数据存储在服务器的RAM中。将Redis视为一个巨大的**白板**放在你的桌子旁边。要获取一条信息，你只需要看一眼白板。检索数据的行为几乎是即时的。

从RAM读取比从最快的SSD读取快数千倍。这就是Redis速度如此之快的原因。当然，权衡是RAM是易失性的（如果服务器重启，白板上的数据就会被清除），而且比磁盘空间更昂贵。但对于缓存来说，数据是临时的，可以始终从"图书馆"（PostgreSQL）重新计算，这种权衡是完美的。

**键值存储解释**

另一个使Redis如此快速的原因是它的简单性。它是一个**键值存储**，这是可以想象的最简单的数据模型。

它的工作方式就像一本字典。你有一个唯一的**键**（你要查找的单词）和一个**值**（定义）。

- **键**：一个唯一的字符串，如store_catalog:gavranmisal.com
- **值**：与该键关联的数据。这可能是一个简单的字符串、一个数字，或者在我们的情况下，是一个包含所有产品信息的大型文本块。

要获取数据，你只需向Redis请求与特定键关联的值。没有复杂的查询语言如SQL。你只需说GET key。这种简单性使它变得异常快速，并且易于我们的应用程序与之通信。

#### **技术深入：我们的读取穿透缓存策略**

选择了正确的工具后，我们设计了缓存策略。我们决定预组装整个商店页面数据，并将其作为单个块存储在Redis中。

- **键**将是一个简单、可预测的字符串，如store_catalog:&lt;store_name&gt;。
- **值**将是一个单个的大型**JSON对象**。JSON对象是一种基于文本的格式，用于表示结构化数据。我们将从数据库中收集所有114个数据块，并将它们打包成一个随时可用的JSON文件。

我们的应用程序现在将遵循"读取穿透缓存"逻辑。获取商店目录的代码现在看起来像这样：

**简化的Python代码片段**

```Python

import redis
import json

# 连接到我们的Redis服务器
redis_client = redis.Redis(host='localhost', port=6379, db=0)

def get_store_catalog(store_slug):
  # 1. 定义我们将用于这个商店的键。
  cache_key = f"store_catalog:{store_slug}"
  # 2. 首先，尝试从缓存中获取数据（白板）。
  cached_data = redis_client.get(cache_key)
  if cached_data:
    # 3a. 缓存命中！数据在白板上。
    print("CACHE HIT!")
    # 将JSON字符串转换回Python字典并返回它。
    return json.loads(cached_data)
  else:
    # 3b. 缓存未命中！数据不在白板上。
    print("CACHE MISS!")
    # 4. 执行昂贵的操作：查询数据库（图书馆）。
    # （这是我们114个数据库查询的占位符）
    store_data_from_db = build_catalog_from_database(store_slug)
    # 5. 将新获取的数据转换为JSON字符串。
    json_data = json.dumps(store_data_from_db)
    # 6. 保存到缓存中以备下次使用！
    # 设置1小时（3600秒）的过期时间（ex）。
    redis_client.set(cache_key, json_data, ex=3600)
    # 7. 将数据返回给用户。
    return store_data_from_db
```

这个逻辑是一个游戏规则的改变者。在我们部署这个之后，第一个访问gavranmisal.com商店的人将触发"缓存未命中"。他们的请求会很慢，因为我们的服务器做了艰苦的工作：查询数据库并构建JSON对象。但在此过程中，它会将最终的JSON对象保存到Redis中。

在接下来的一个小时内，每一个后续访问者都会触发"缓存命中"。他们的请求甚至不会触及我们的PostgreSQL数据库。Redis将直接从内存中提供预构建的JSON，只需几毫秒。6秒的页面加载时间下降到不到200毫秒。这是一个惊人的成功。

#### **新的蓝图**

我们的架构再次演变，Redis现在作为应用程序和数据库之间的高速缓冲区。

新的流程是：用户请求 -> 应用程序 -> **首先检查Redis** -> （如果未命中） -> PostgreSQL数据库。

#### **新问题：过期缓存**

我们已经解决了速度问题。但在这样做的过程中，我们创造了一个新的、潜在危险的问题。

当gavranmisal.com的所有者将产品价格从₹150更新到₹120时会发生什么？更改被正确地写入我们的主PostgreSQL数据库（图书馆）。但我们的Redis缓存（白板）仍然保存着旧版本的目录，价格为₹150。

我们的set命令包含了一小时的过期时间。这意味着在接下来的一个小时内，访问商店的每一位客户都会看到快速但现在不正确的缓存过期数据。

我们创建了一个闪电般快速但可能向用户说谎的系统。当图书馆中的主副本被更改时，我们如何告诉白板自行擦除？

这就是**缓存失效**的问题，它是计算机科学中两个最难解决的问题之一。

### 第三部分：忘记的艺术

我们构建了一个具有闪电般快速记忆的系统。但我们忽略了一个关键细节：良好的记忆需要同样良好的遗忘能力。我们的缓存坚持保留旧的、过时的信息，将我们出色的性能解决方案变成了真相问题的源头。Priya的"幽灵价格"是一个明确的迹象，表明我们的架构仍然过于简单化。

#### **有缺陷的第一个想法：缩短约束**

我们的第一个、最明显的想法是简单地减少缓存的过期时间，也称为其**TTL（生存时间）**。我们随意地将其设置为一小时（ex=3600）。

"如果我们将其设置为一分钟呢？"一位队友建议。

这是一个诱人的快速修复方案。一分钟的TTL意味着过期数据最多只能可见60秒。这比一小时好，但这是一个糟糕的权衡。

缓存的有效性通过其**"命中率"**来衡量 - 它能够从内存中提供请求的时间百分比，而不是必须去数据库。使用一小时的TTL，一个受欢迎的商店将有一个缓慢的请求（"未命中"），然后是数千个快速请求（"命中"）。命中率将超过99%。

使用一分钟的TTL，我们将迫使每个受欢迎的商店每分钟都发生缓存未命中。这意味着我们的应用程序必须返回数据库，运行100多个查询，并每小时重建JSON对象60次，而不是一次。这将大大增加只读副本的负载，否定我们从缓存中获得的大部分好处。

这就像试图通过每分钟打开和关闭主水源来修复漏水的水龙头。它笨拙且效率低下。我们需要一把手术刀，而不是一把大锤。我们不应该等待缓存过期；我们需要能够按命令杀死它。

#### **真正的解决方案：事件驱动的失效**

正确的解决方案是使我们的系统具有主动性。当数据在我们的"真实来源"（主数据库）中发生变化的那一刻，我们需要向缓存发送一个信号，说："你持有的信息现在是错误的。立即忘记它。"

这就是**事件驱动的缓存失效**。要做到这一点，我们需要两件事：

- 一种检测数据已更改的"事件"的方法。
- 一种将关于该事件的消息广播给监听器的方法。

幸运的是，我们强大的数据库PostgreSQL内置了完美的工具来完成这项工作。

#### **技术深入：Postgres触发器和LISTEN/NOTIFY**

我们创建了一个系统，本质上是给我们的数据库一个声音，允许它在每次更改时进行宣布。

1. 数据库触发器（安全传感器）

触发器是数据库中的一个特殊函数，你可以设置它在特定表上发生特定操作时自动运行。

我们在products表上创建了一个触发器。这就像在图书馆保险库的门上安装了一个运动传感器。我们将其配置为在products表中的任何行被更改（INSERT、UPDATE或DELETE）时触发。

2. NOTIFY（广播）

现在，当传感器检测到运动时会做什么？它需要发出警报。我们编程我们的触发器执行NOTIFY命令。这是PostgreSQL的一个功能，它在特定的公共通道上发送消息，就像无线电广播一样。触发器会在我们命名为product_changes的通道上广播消息。关键的是，它还会在消息中包含一小部分信息 - 刚刚更改的产品的store_id。

3. LISTEN（无线电接收器）

最后一步是构建一个小型、独立的服务。让我们称之为"缓存失效器"。它在整个世界中的唯一工作是连接到数据库并LISTEN（监听）product_changes通道。它是一个专门的无线电操作员，不断监听单个广播。

完整的流程是一个美丽的事情：

- Priya更新了她商店（Store ID：456）中一条项链的价格。这是对**主**PostgreSQL数据库的UPDATE写入操作。
- products表上的UPDATE立即触发我们的触发器。
- 触发器在product_changes通道上广播一个NOTIFY消息，有效载荷为"456"。
- 我们的缓存失效器服务，一直在监听，接收消息。
- 服务立即知道该做什么。它说："商店456的数据现在已经过期。我必须销毁缓存。"
- 服务连接到Redis并发出一个单一的、闪电般快速的命令：DEL store_catalog:store-456。白板上的旧JSON对象立即被擦除。

现在，当Priya刷新她的商店页面时，我们的应用程序代码检查Redis。它什么也没找到（"缓存未命中"）。然后它查询数据库，获取正确的新价格₹800，重建JSON，并将这个正确的新版本保存回缓存。旧数据的幽灵被征服了。

<br/>
## 第7章：要点总结

- **速度是一个功能。** 一个缓慢的网站是一个坏掉的网站。缓存是提高应用程序性能的最强大工具。
- **Redis是一个用于缓存的出色工具。** 它的内存键值特性使其比传统的基于磁盘的数据库快几个数量级，适合临时数据存储。
- **缓存引入了数据一致性问题。** 简单的基于时间的过期（TTL）是处理过期数据的一种粗糙且通常效率低下的方式。
- **事件驱动的缓存失效是更好的方法。** 在底层数据变化的那一刻主动删除过期的缓存条目，这种方式更加高效和可靠。
- **利用数据库的高级功能。** 像PostgreSQL的触发器和LISTEN/NOTIFY这样的工具提供了一个强大的、内置的机制，用于创建驱动智能缓存失效系统所需的实时事件。

## 第8章：打破单体：我们的第一个微服务

我们已经征服了基础设施扩展的主要障碍。我们的服务器可以处理流量，我们的数据库可以处理读取，我们的部署过程是安全的。我们有一个专业的工程团队、一个弹性架构和一个数百万人使用的产品。第一次，我们感觉我们控制了局面。

但规模的本质是瓶颈总是在移动。我们优化了我们的机器，但我们忘记了优化我们的组织。随着我们团队的成长，我们单一的、单体的代码库，在一开始是我们速度的最大资产，现在正成为我们最大的负担。

下一场火灾不是由流量高峰引发的。它是由我们自己的团队引发的，是由太多人试图在同一个小厨房工作的混乱中产生的自伤。

### 第一部分：内战

我们的工程团队已经增长到大约十人。我们将他们分成两个小"小组"，专注于业务的不同部分。"增长"小组专注于卖家体验 - 入职、仪表板和产品管理。"运营"小组专注于核心功能 - 支付、物流和订单履行。

这似乎是一个合理的分工。问题是，两个小组仍然在同一个单一的单体Django应用程序上工作。他们的代码完全交织在一起。

一天下午，运营小组正在开发与物流合作伙伴的新集成。为此，他们需要在数据库中的Order模型中添加几个新字段，如tracking_id和shipping_provider。同时，增长小组正在开发一个允许卖家提供折扣的功能，这也需要修改Order模型以包含discount_code字段。

两个团队并行工作。两个团队都在预发服务器上测试了他们的功能。一切看起来都很好。物流功能首先被批准并部署到生产环境。一个小时后，折扣功能也被批准并部署。

然后，混乱开始了。

我们的支付处理webhook，确认客户支付成功的关键端点，开始为每一个订单失败。客户被收费，但订单在我们的系统中没有被标记为"已支付"。从卖家的角度来看，订单在支付后就消失了。

Suumit立即接到了电话。这不是关于网站速度慢的问题；这是关于丢失用户资金的问题。这是一场五级警报火灾。

我们深入研究了日志。错误是神秘的。它在支付确认逻辑的深处失败。经过疯狂的半小时调试，我们找到了原因。在添加discount_code字段的过程中，增长小组的代码略微改变了Order对象的保存方式。物流小组之前的更改对旧的保存方式做了假设。这两个功能独立测试时完全正常，但它们完全不兼容。当它们一起部署到生产环境时，它们创造了一个没有人预料到的关键bug。

这是我们自己代码库内的内战。两个友好的团队，朝着同一个目标努力，意外地破坏了彼此的工作。

#### **识别瓶颈：代码库**

在我们回滚更改并平息风暴后，我们举行了一次严肃的事后分析会议。很明显，我们的问题不是服务器、数据库或部署管道。管道工作得完美无缺；它完全按照我们告诉它的去部署。

瓶颈是**单体本身**。

我们的应用程序已经成为一个巨大的、纠结的依赖网络。支付的代码与订单的代码紧密耦合，订单的代码又与物流、卖家和产品紧密耦合。开发者不可能在一个领域进行更改而不需要了解其对整个系统的潜在影响。

我们的开发速度 - 我们能够安全地发布新功能的速度 - 已经大幅下降。每个新功能都需要更多的会议、更多的协调，以及更多对破坏不相关功能的恐惧。我们花在管理单体复杂性上的时间比为客户构建功能的时间还要多。厨房变得太拥挤了。

#### **技术深入：单体与微服务对决**

这次危机迫使我们面对一个重大的架构决策。是时候考虑将单体分解为**微服务**了。

**单体：一个巨大的餐厅** 正如我们所讨论的，单体是一个做所有事情的单一应用程序。

- **类比**：它是一个单一的、巨大的餐厅。它有一个巨大的厨房，制作所有东西：意大利面、墨西哥玉米卷和中国面条。有一个主厨负责监督一切。
- **优点（早期）**：开始很简单。每个人都在同一个厨房，沟通容易，你可以快速为第一批客户提供服务。
- **缺点（我们的当前现实）**：随着你的受欢迎程度增加，厨房变得混乱。意大利面厨师必须等待玉米卷厨师完成使用炉灶。更改面条配方可能会意外用完意大利面酱所需的所有盐。雇佣新的专业厨师很困难，因为他们需要学习整个庞大的菜单。这正是我们正在经历的。

**微服务：专业摊位的美食广场** 微服务架构将一个大型应用程序分解为一组较小的、独立的服务。

- **类比**：它是一个美食广场。不是一个巨大的餐厅，而是每种菜系都有一个独立的、自主的摊位。有一个披萨摊位、一个玉米卷摊位和一个面条摊位。每个摊位都有自己的小厨房、自己的专业厨师和自己的食材。
- **优点**：
  - **团队自主权**：披萨团队可以更改他们的菜单，尝试新的烤箱，并且完全独立于玉米卷团队运营。他们可以一天部署多次更新，而不会有破坏面条摊位的风险。
  - **专业技术**：面条摊位可以使用一种特殊的高性能锅（不同的编程语言或数据库），这对他们的需求来说是完美的，而披萨摊位使用传统的砖炉。
  - **故障隔离**：如果玉米卷摊位的厨房着火，它会关闭。但披萨和面条摊位仍然完美运行。整个美食广场不会宕机。
- **缺点**：设置和管理要复杂得多。摊位如何协调一个想要同时购买玉米卷和披萨的客户的订单？你现在必须担心网络通信、服务发现和分布式数据 - 这些问题在单体中根本不存在。

选择很明确。单体的痛苦已经超过了转向微服务的痛苦。我们必须开始分解它。我们必须计划我们的第一个切口。

### 第二部分：规划第一个切口

我们做了一个可怕但必要的决定，开始分解我们的单体。这感觉就像站在一台我们亲手建造的巨大、复杂的机器前，知道我们现在必须在它还在运行的情况下将其拆开。

你甚至从哪里开始？

外科医生不会随便开始切割。他们花数小时研究X光片并规划他们的第一个切口。第一次切割是最关键的；它为其余操作奠定了基础。我们需要一个计划。我们需要选择我们的第一个微服务，第一个我们将从应用程序主体中小心切下的部分。

选择你的第一个微服务是你在这个旅程中做出的最重要的决定之一。如果你选择了一个太复杂或太纠结的部分，整个操作可能会失败，留下一个比你开始时的单体更糟糕的混乱。

#### **技术深入：如何选择你的第一个微服务**

经过多次辩论和研究，我们为我们的第一个候选者制定了一套标准。这是任何人都可以使用的剧本。

**标准1：低业务关键性** 你构建、部署和管理微服务的第一次尝试将是一次学习经验。你会犯错误。最好在你的应用程序的一个部分犯这些错误，如果它宕机一小时不会破坏你的业务。

- **类比**：你不会通过为国宴准备主菜来学习烹饪。你从做沙拉开始。如果你把沙拉搞砸了，这不好，但主餐没有被毁。
- **对于Dukaan**：这立即排除了关键、复杂的领域，如支付或订单。支付服务中的一个bug可能意味着损失用户的钱，这是一个生存威胁。那不是实验的地方。

**标准2：少量且明确的依赖项** "依赖项"是服务与系统其他部分的任何连接或通信。你想要选择应用程序中尽可能独立的一部分。

- **类比**：将单体视为一团巨大的、纠结的毛线。你不想从尝试从非常中心的地方拉出一根线开始；你只会使结更紧。你想找到挂在边缘的一根松线，然后从那里开始解开。
- **对于Dukaan**：我们分析了我们的代码。订单服务是一个纠结的噩梦；它依赖于用户、产品、支付和物流。但其他部分更加孤立。

**标准3：明确且有界的领域** 你选择的服务应该有一个单一的、明确定义的目的。你应该能够用一个简单的句子描述它的作用。这就是软件架构师所说的"有界上下文"。

- **类比**：在我们的美食广场，"披萨摊位"是一个完美的有界上下文。你确切知道它做什么。"杂食品摊位"是一个糟糕的选择。

#### **我们的决定：店面服务**

在根据这些标准评估我们的整个单体后，一个候选者作为我们第一个切口的完美选择出现了：**店面（Storefront）**。

店面是卖家商店面向公众的部分 - 客户实际访问以浏览产品的页面。让我们看看它如何符合我们的规则：

- **较低的业务关键性**：这听起来可能违反直觉，但这是真的。如果店面服务宕机，客户无法_查看_商店。这显然不好，但不是灾难性的。卖家仍然可以登录他们的仪表板，管理产品，并查看他们现有的订单。至关重要的是，不会丢失任何数据或资金。我们仍然可以处理已经发起的订单的付款。它是沙拉，不是主菜。
- **很少的依赖项**：店面几乎完全是一个"只读"服务。它的主要工作是从我们的数据库（特别是我们在第5章中构建的只读副本）获取产品和商店信息，并将其美观地显示出来。它对单体其余部分的复杂"写入"逻辑几乎没有依赖，如订单处理或支付确认。它是挂在毛线球边缘的一根线。
- **明确的领域**：它的目的非常明确："显示卖家产品的公开、只读目录。"就是这样。一个完美的有界上下文。

还有一个巨大的优势。店面与应用程序的其余部分有完全不同的**扩展配置文件**。卖家仪表板每天可能获得数千次访问，但一个受欢迎的商店页面可能获得_数百万_次访问。通过将其切出，我们可以独立扩展它。我们可以为店面服务配备20台强大的服务器，同时为卖家仪表板保留一个更高效的3台服务器的小型集群，为我们节省大量资金。

计划已定。我们将进行手术。我们将仔细提取与公共店面相关的所有代码、模板和逻辑，并将其重建为一个全新的、完全独立的应用程序：storefront-service。

#### **新问题：服务间通信**

我们做出这个决定的那一刻，一个我们从未面对过的新问题出现了。

只要我们有单体，通信就很容易。如果订单逻辑需要获取产品的价格，它只会调用getProductPrice()函数。这是同一应用程序内的直接、内部对话。

但现在，我们将有两个独立的应用程序。旧的单体（我们现在称为core-api）和新的storefront-service。它们是两个独立的建筑。当卖家登录core-api并更新产品价格时会发生什么？完全独立的storefront-service如何知道这一变化？

他们不能再调用函数了。他们需要一种通过网络相互交谈的方式。我们需要定义一个契约，一个正式的API（应用程序编程接口），允许我们的服务进行通信。

我们的美食广场需要一种共同语言。披萨摊位需要一种正式的方式来告诉中央仓库它需要什么配料，而仓库需要一种方式来告诉摊位价格变化。我们刚刚进入了分布式系统的复杂世界。

### 第三部分：绞杀者树模式

我们有一个计划。我们知道我们想从单体中切出哪一部分 - 店面。但你如何在不杀死病人的情况下对活体病人进行手术？"大爆炸"重写，即关闭一切数月来重建，是不可能的。一个不发货的初创公司是一个死的初创公司。

我们需要一个策略，在应用程序的其余部分继续运行的同时，逐步、安全地一块一块地替换旧系统。为此，我们转向了一种美丽而强大的架构模式，以自然力量命名：**绞杀者树模式（Strangler Fig Pattern）**。

#### **技术深入：绞杀者树模式**

在雨林中，绞杀者树是一种藤蔓，它在一棵古老、成熟的树上开始它的生命。它开始很小，将根沿着宿主树的树干向下延伸。多年来，这些根变得越来越粗、越来越强，形成一个全新的、强大的格子结构，完全包围了旧树。最终，内部的宿主树可能会死亡并腐烂，留下绞杀者树的强壮、健康和完美形成的结构。

这是替换遗留系统的完美比喻。

- **不要一次性重写单体。**
- 相反，识别一个功能片段（旧树的一个分支）。
- 为它构建一个新的、独立的微服务（种植新藤蔓）。
- 在旧单体前面放一个"路由器"或"代理"。这个路由器是关键。
- 最初，路由器只是将所有流量传递给旧单体。
- 然后，你配置路由器拦截对特定功能的调用，并将该流量重定向到你的新微服务。你可以逐渐地这样做 - 首先是内部用户，然后是1%的真实用户，然后是10%，依此类推。
- 一旦该功能的100%流量由新服务提供，单体内部的旧代码实际上就死了。它不再被使用。
- 你现在可以安全地删除旧代码，使单体变得更小一点。
- 对下一个功能片段重复这个过程。

几个月或几年后，新的微服务（藤蔓）变得更强大，并接管了越来越多的单体职责，直到旧单体要么缩小到可管理的大小，要么完全消失。

#### **我们的实现：Nginx作为绞杀者**

再一次，我们可靠的工具Nginx非常适合这项工作。我们已经将其用作负载均衡器。我们可以增强其配置，使其也充当这个智能路由器。

我们构建了我们新的、独立的storefront-service。它是一个精简、快速的应用程序，其唯一工作是渲染商店页面。然后，我们用一些新逻辑更新了我们负载均衡器上的Nginx配置：

```nginx

#简化版的"绞杀者"配置
#定义我们的新店面微服务

upstream storefront_service {
  server 10.132.8.12; # 新服务的IP
}

#定义我们的旧单体
upstream monolith_service {
  server 10.132.2.31;
  server 10.132.4.55;
}

server {
  listen 80;
  server_name dukaan.app;

  location {
    #这是绞杀者逻辑。默认情况下，将所有流量
    #发送到旧单体。
    set $target_service $monolith_service;

    #然而，如果请求是针对商店页面（例如，dukaan.app/store/gavranmisal）
    #并且我们设置了一个特殊的测试cookie...

    if ($uri ~* "^/store/" and $cookie_use_new_storefront = "true") {
      #...然后将这个特定请求发送到我们的新微服务！
      set $target_service $storefront_service;
    }

    proxy_pass http://$target_service;
    #... 其他代理设置
  }
}

```

这个配置给了我们精确的控制权。我们现在可以给内部团队设置use_new_storefront=true的cookie。我们可以在实时生产流量上浏览网站并测试新服务，而没有任何真实用户看到它。一旦我们有信心，我们可以修改逻辑，将10%的匿名流量路由到新服务，然后是50%，最后是100%。

第一次手术取得了成功。storefront-service已经上线，它速度很快，我们有一个经过验证的、安全的模式，可以缓慢地绞杀我们的单体。

#### **新问题：通信的潘多拉盒子**

我们成功地切出了一个简单的只读服务。但在这样做的过程中，我们释放了一系列新的、复杂的问题，这些问题在我们简单的单体世界中是被屏蔽的。

店面的通信很简单。当单体中的卖家更新产品时，我们可以让单体的代码也更新店面的数据库或向其缓存发送消息。这主要是单向的。

但是，当我们提取更复杂的服务（如订单）时会发生什么？

- 新的order-service如何与user-service通信以验证客户的详细信息？
- 它如何与payment-service通信以确认交易？
- 如何处理客户的支付在payment-service中成功，但order-service在创建订单之前崩溃的情况？这是一个**分布式事务**，它是软件工程中最难的问题之一。如何在多个独立服务之间保持数据一致性？

我们简单的点对点通信方法行不通。我们刚刚打开了潘多拉盒子。我们不再仅仅是应用程序开发人员；我们现在被迫成为分布式系统工程师。

## 第8章：要点总结

- **永远不要从头重写复杂系统。** 风险太高。渐进式、一块一块地替换是更安全、更有效的路径。
- **绞杀者树模式是增量替换单体的黄金标准。** 它允许你用真实流量构建和验证新服务，在每一步最小化风险。
- **现有的反向代理（如Nginx）是实现绞杀者模式的强大工具。** 你可以用它来智能地在旧系统和新系统之间路由流量。
- **从简单、低风险、自包含的服务开始。** 像店面或搜索功能这样的读重服务是理想的第一个提取候选者。
- **转向微服务解决了团队扩展问题，但也引入了复杂的技术挑战。** 准备好应对诸如服务发现、网络延迟和分布式数据一致性等困难问题。

## 第9章：不可破的承诺：使用Kafka确保数据一致性

我们做到了。我们对单体进行了第一次成功的手术。我们新的、独立的storefront-service已经上线，速度快，扩展性好。绞杀者树模式给了我们一个安全、可重复的流程，为未来做好了准备。短暂的一刻，我们感觉自己已经解开了管理日益增长的复杂性的秘密。

但是在分布式系统的世界里，解决一个问题往往会揭示潜伏在下面的更深层次、更基本的问题。我们的服务现在是独立的实体，生活在不同的建筑中。我们在它们之间建立的简单电话线——我们聪明但脆弱的缓存失效系统——即将在第一个真正的压力迹象下断裂，向我们展示通信不仅仅是一个功能，而是微服务架构的基础。

### 第一部分：不稳定的监听器

与我们第一个微服务的生活很好。店面团队可以部署更改而不用担心破坏支付系统，核心API团队很高兴看到大量的店面流量由一个单独的服务器集群处理。分离正在发挥作用。

保持我们数据一致性的关键是我们在第7章中构建的小型独立Python脚本："缓存失效器"。这个单一脚本连接到我们的主数据库，不断LISTEN（监听）NOTIFY消息。当产品价格变化时，它会听到消息并忠实地从Redis中删除正确的条目。

几周来，它完美地工作。但随后，故障开始出现。

一天下午，一位卖家打电话给支持人员，很沮丧。"我过去一小时一直在尝试运行闪售！我不断更改我的主要产品的价格，但我的客户都看到的是旧价格。我正在失去所有的势头！"

我们查看了服务器日志。我们检查了缓存失效器脚本。它已经崩溃了。脚本和数据库之间的临时网络故障导致连接断开，脚本没有自动恢复。它已经默默死亡了一个多小时。在此期间，卖家所做的每一次产品更新都未能使缓存失效。我们的商店充满了过时的数据。

我们重启了脚本，问题解决了，但损害已经造成。我们有一个**单点故障**。

#### **识别问题：建立在祈祷之上的系统**

这一事件暴露了我们事件驱动系统的根本弱点。LISTEN/NOTIFY机制是Postgres的一个巧妙功能，但它不是一个健壮的、生产级的消息传递系统。

- **它很脆弱：** 正如我们发现的，如果我们的监听器服务崩溃或断开连接，数据库在停机期间发送的任何消息都将永远丢失。没有持久性。数据库只是对着虚空大喊消息；如果没有人在听，消息就会消失。
- **它无法扩展：** 该系统设计用于一对一通信。但是，如果我们需要更多服务对产品更新做出反应呢？如果一个新的search-service也需要知道产品详情何时变化，以便更新其搜索索引呢？我们将不得不构建另一个独立的监听器脚本，增加更多复杂性和另一个故障点。
- **它缺乏洞察力：** 我们无法知道消息是否已成功处理。监听器收到了吗？它成功删除了Redis键吗？我们盲目飞行。

我们的系统建立在一个简单、脆弱的脚本永远保持在线的希望之上。这不是工程；这是祈祷。

我们意识到我们需要用一个成熟的、工业级的邮政服务来取代我们简单的电话线。我们需要一个系统，可以保证消息传递，允许多个收件人，并记录发送的每条消息。我们需要一个真正的**消息总线**。

### 第二部分：邮政服务与报纸

我们知道我们需要一个消息总线，一种从一个服务向另一个服务发送信息的可靠方式。但当我们开始研究时，我们意识到构建这样一个系统有两种根本不同的哲学：传统的**消息队列**和更现代的**分布式日志**。

在它们之间进行选择是一个关键决定，将塑造我们整个架构的未来。

#### **技术深入：消息队列 vs. 分布式日志**

这是现代系统设计中的一个关键概念，让我们来详细分析。

**1. 消息队列（例如，RabbitMQ）：邮政服务**

传统的消息队列就像一个邮局或共享的待办事项列表。

- **类比**：生产者（如我们的主应用程序）写一封信（"使商店456的缓存失效"）并将其放入特定的邮箱（队列）。一个单一的、专门的工作者（消费者，如我们的缓存失效器）被分配到该邮箱。工作者拿起信，执行任务，然后**扔掉信**。消息被处理，然后永远消失。

这种模型非常适合分发任务。它保证一个特定的工作将由一个工作者完成。但其关键特性是消息是**短暂的**。一旦被消费，它就消失了。

**2. 分布式日志（例如，Apache Kafka）：报纸**

像Apache Kafka这样的分布式日志基于完全不同的原则工作。它不是邮局；它是一个报纸出版商。

- **类比**：生产者（我们的主应用程序）是一名记者。当产品价格变化时，记者写一篇简短的文章："商店456的价格已更改。"他们不会将其发送给特定的人；他们将其发布在报纸的特定部分，比如**product_updates**部分（这在Kafka中是**Topic**）。
- 现在，多个独立的订阅者可以阅读这份报纸。缓存失效器服务订阅了product_updates部分。一个新的搜索索引器服务也可以订阅同一部分来更新其搜索结果。分析服务也可以订阅以跟踪价格趋势。
- 至关重要的是，当缓存失效器阅读文章时，文章**不会被删除**。它仍然在那里，供其他订阅者阅读。报纸本身是一份持久的、永久的记录，记录了所发生的一切。

这对我们来说是一个灯泡时刻。我们的问题不仅仅是告诉一个服务使缓存失效。我们开始看到一个未来，我们系统的许多不同部分将需要对相同的事件做出反应。当订单被下单时，我们需要：

- 告诉配送服务准备履行。
- 告诉通知服务向客户发送电子邮件。
- 告诉分析服务更新每日销售数字。

报纸（Kafka）模型非常适合这种一对多通信模式。它的**持久性**（报纸在阅读后不会被销毁）给了我们渴望的可靠性。如果我们的缓存失效器崩溃，product_updates文章会安全地堆积在报纸中。当服务重新上线时，它可以拿起报纸并从离开的地方开始阅读，不会错过任何更新。

我们找到了我们的解决方案。我们将在Apache Kafka上构建Dukaan的中枢神经系统。

#### **技术深入：作为Dukaan中枢神经系统的Kafka**

我们移除了脆弱的LISTEN/NOTIFY系统，并开始集成Kafka。为此，我们必须了解它的核心组件。

- **生产者**：生产者是向Kafka主题写入数据的任何应用程序。对我们来说，主要生产者是我们的主单体。当卖家更新产品时，单体现在也会向Kafka生成一个小型结构化消息。
- **主题**：主题是一个命名的类别或消息发布的源。我们从一个开始，product_updates，但很快意识到我们可以为系统中的每个重要事件创建主题：orders_placed、new_users_registered、shipments_updated等。这些主题成为我们业务的官方、记录的历史。
- **消费者**：消费者是订阅一个或多个主题并处理消息的任何应用程序。我们新的、改进的缓存失效器服务是我们的第一个消费者。它订阅了product_updates主题。Kafka的一个关键特性是**消费者组**，它允许您有多个服务实例（如两个缓存失效器脚本）并行从主题读取，以实现可扩展性和容错性。
- **代理**：代理是单个Kafka服务器。Kafka集群由多个共同工作的代理组成。这就是使其具有弹性的原因。如果一个代理服务器失败，其他代理会继续发布报纸，而不会中断。

我们选择了我们的新基础。下一步是实施：正式退休我们不稳定的监听器，并使我们的单体成为全职记者，将每个重要事件发布到我们新的、坚不可摧的中央报纸上。

### 第三部分：实施与新现实

我们有了新的蓝图。我们将用一个由Apache Kafka提供支持的健壮的中枢神经系统替换我们脆弱的电话线。下一步是实际的手术：移除旧的、脆弱的监听器，并将我们的整个应用程序连接到这个新的、强大的消息总线上。

#### **技术深入：实施过程**

步骤1：设置Kafka集群

Kafka是一个复杂的分布式系统。从长远来看，我们将学习自己托管和管理它。但在开始时，我们的目标是速度和可靠性，而不是成为Kafka专家。因此，我们做出了一个战略决定：我们使用了托管的Kafka服务。

像Confluent Cloud或Aiven这样的服务允许您通过几次点击就能启动一个生产就绪、容错的Kafka集群。它们处理所有困难的部分——备份、监控和故障恢复——每月收费。这是一个关键的教训：**在采用复杂技术的早期，依靠托管服务。**它允许您的团队专注于使用工具，而不仅仅是保持它的运行。我们的Kafka"报纸印刷机"在不到一小时内就启动并运行了。

步骤2：使用Debezium进行变更数据捕获（CDC）的魔力

现在，我们如何让我们的单体向我们的新报纸发布文章？

天真的方法是遍历我们的整个Django代码库，并在各处添加kafka_producer.send()命令。每次product.save()之后，我们都会添加一行来发送Kafka消息。这将是一场噩梦。它会使我们的代码混乱，开发人员可能很容易在新功能中忘记添加它，导致数据不一致。

相反，我们使用了一个神奇的工具，叫做**Debezium**。

Debezium是一个开源的**变更数据捕获（CDC）**平台。要理解它的作用，让我们回到我们的图书馆类比。

- **问题**：你如何知道何时对保险库中的主手稿进行了更改？你可以要求作者每次写作时都大喊"我做了更改！"，但他们可能会忘记。
- **Debezium解决方案**：Debezium就像一个直接放置在主手稿上方的高科技扫描仪。它不观察作者；它观察手稿本身。它在墨水干燥时阅读它。当数据库事务日志（WAL）中的一个单词被添加、更改或删除的那一刻，Debezium会看到它，捕获确切的更改，将其格式化为完美的消息，并将其生成到正确的Kafka主题。

这对我们来说是革命性的。我们的单体应用程序**甚至不需要知道Kafka的存在**。我们对应用程序的写入逻辑进行了零更改。我们只是继续像往常一样将数据保存到我们的PostgreSQL数据库。Debezium在后台工作，完全独立，监视数据库的WAL，并忠实地将每一个更改发布到我们的Kafka主题。这完全解耦了我们的应用程序和我们的消息系统。它更干净、更可靠，并且开发人员不可能忘记。

步骤3：重写消费者

最后一步是升级我们的缓存失效器服务。这很简单。我们删除了所有用于直接连接到Postgres并监听通知的旧的复杂代码。我们用一个标准的、经过实战检验的Kafka消费者库替换了它。

新服务简单地订阅了product_updates主题（现在由Debezium可靠地填充）。当消息进来时，它会从消息有效载荷中读取store_id并发出redis_client.del()命令。由于Kafka的消费者组，我们现在可以同时运行两到三个这个服务的实例。如果一个崩溃，其他的会无缝接管。我们的单点故障已经消失了。

#### **新蓝图**

我们的数据一致性架构现在是健壮的、可扩展的和优雅的。流程完全不同了。

**单体应用 -> 主数据库（WAL） -> Debezium -> Kafka主题 -> 消费者服务 -> Redis**

应用程序写入数据库，系统的其余部分对该事件做出反应，Kafka和Debezium充当坚不可摧的传输层。

#### **新问题：运营复杂性**

我们已经解决了数据一致性问题。我们的系统现在比以往任何时候都更可靠。但我们的架构图中有一个新的、大的、复杂的框。我们在操作的核心引入了一个有状态的分布式系统。

Kafka是非常强大的，但它不是魔法。它是一个复杂的基础设施，需要管理和监控。我们现在必须担心新的事情：

- Kafka代理是否健康？
- 消息是否被正确生产和消费？
- 我们的消费者服务是否有延迟？
- 我们的主题是否填满了代理上的磁盘空间？

我们用新系统的**运营复杂性**换取了旧系统的脆弱性。我们简单的房子现在变成了一个拥有自己发电厂的复杂庄园。发电厂要可靠得多，但它需要熟练的工程师来维护它。这是一个必要的权衡，是构建真正的微服务架构的入场费。

## 第9章：要点总结

- **简单的消息系统是单点故障。** 为了在服务之间进行可靠的通信，您需要一个持久的、持久的事件日志，如Apache Kafka。
- **变更数据捕获（CDC）是一个强大的模式。** 使用像Debezium这样的工具直接从数据库日志中流式传输变更是更可靠的，并且可以将应用程序代码与消息系统解耦。
- **在开始阶段对复杂的基础设施使用托管服务。** 专注于使用新技术解决业务问题，而不是保持其在线的低级细节。
- **添加到堆栈中的每一个强大工具都会增加其运营复杂性。** 准备好投资所需的监控和专业知识来管理您引入的新系统。
- **有了中枢神经系统，您现在有了基础。** 这种可靠的事件流是解锁安全地将其余的单体分解成真正的、可扩展的微服务架构能力的关键。

## 第10章：集装箱革命：Docker简介

我们的架构变得越来越强大。我们有了一支自动扩展的网络服务器队伍，一个容错的数据库，以及一个中央神经系统，Kafka确保数据在我们的服务之间可靠地流动。我们有了一个现代、可扩展的科技公司的蓝图。

但在地面上，日常工作仍然很混乱。一种新的摩擦正在减缓我们的速度，这种混乱不是来自系统崩溃，而是来自微小、令人恼火的不一致性。同时，我们每月从AWS收到的账单正在攀升到一个让Suumit感到紧张的水平。我们已经建造了一个强大的引擎，但它效率低下，容易出现人为错误。

本章将介绍我们如何通过采用一项不仅彻底改变了我们公司，而且彻底改变了整个科技行业的技术来解决这些问题：**容器**。

### 第一部分：双头龙

我们正在与一头双头龙作战。一个头是折磨我们开发人员的不一致性野兽。另一个头是悄悄吞噬我们金钱的低效野兽。

**第一个头："但它在我的机器上工作！"**

我们的开发团队正在成长。我们有后端开发人员、前端开发人员和数据工程师，都在开发不同的功能。我们最聪明的新工程师之一，让我们叫她Anjali，负责为卖家构建一个生成PDF发票的功能。

她找到了一个很棒的新Python库reportlab-ng，使这个过程变得简单。她在她的Macbook上安装了它，编写了代码，并进行了彻底的测试。生成的发票非常漂亮，像素完美。"它工作得很完美！"她宣布，在成功的代码审查后，她的功能被部署到了我们的预发布环境中。

一小时后，一份错误报告传来。预发布环境中的PDF一团糟。文本错位，格式完全损坏。

Anjali感到困惑。她从预发布环境中拉取了完全相同的代码，并在她的笔记本电脑上运行。PDF是完美的。她把它推回预发布环境。它们又坏了。这导致了开发人员词汇中最令人沮丧的一句话：

**"我不明白...它在我的机器上工作！"**

经过数小时痛苦的调试，我们找到了原因。我们预发布环境中的Ubuntu服务器安装了与Anjali的Macbook略有不同的系统级字体库。PDF库依赖于这些底层系统字体，微小的差异导致格式崩溃。

这种"环境漂移"一直都在发生。一个开发人员可能使用的是Python 3.9.6，而服务器上使用的是3.9.2。另一个开发人员可能有特定库的较新版本。每一个微小的差异都是等待爆炸的地雷，浪费了无数小时的调试时间，并在"开发"和"运维"之间造成了巨大的摩擦。

**第二个头：不断攀升的AWS账单**

与此同时，Suumit为我们的每周预算审查打电话给我。他听起来很担心。

"Subhash，我在看AWS账单。它很大。上面说我们运行着二十台应用服务器。但当我查看我们的流量图时，大多数时候，只有一半的服务器很忙。另外十台在做什么？"

他是对的。我们的水平扩展正在工作，但效率非常低。每台服务器都是一个完整的虚拟机（EC2实例），有自己的操作系统，仅为了存在就占用了一部分RAM和CPU。我们在其上运行我们的应用程序进程。为了安全起见，我们只会在每台机器上运行一两个服务，以避免它们相互干扰。

结果是一支长期未充分利用的服务器队伍。服务器A可能使用20%的CPU，服务器B可能达到30%，但我们无法轻松地组合它们的工作负载。我们支付了整个汽车的费用，即使我们只使用了一个座位。这种低效率每月花费我们数千美元。

#### **识别问题：不一致性和低效率**

龙的两个头都被同一个核心问题所驱动：我们打包和运行软件的方式。

- **不一致性：** 我们的代码在开发人员的笔记本电脑上与在服务器上的行为不同，因为底层环境不同。
- **低效率：** 我们的服务器就像大型、半空的房子。我们支付了整个房子的费用，但只住在一个房间里，因为我们担心不同房间的居住者可能会发生冲突。

我们需要一种方法将我们的代码及其所有依赖项打包到一个单一、一致和隔离的盒子中。我们需要一种方法在单个大型服务器上运行许多这些小型、隔离的盒子，而不会相互干扰。我们需要软件等效的标准化运输集装箱。我们需要**Docker**。

### 第二部分：魔法盒

我们面临着两个不同的问题——不一致的环境和低效的服务器——但我们有一种预感，它们都可以被一个强大的新武器所征服。我们开始阅读博客文章并观看会议演讲，一个名字不断出现：**Docker**。

起初我们不理解。它听起来抽象而复杂。但后来我们遇到了一个类比，让一切都豁然开朗，这个类比是理解容器革命的关键。

#### **技术深入：运输集装箱类比**

在20世纪60年代之前，海外运输货物是一场混乱的噩梦。一艘船的货舱将是一堆混乱的不同大小的桶、麻袋、板条箱和盒子。装卸是一个缓慢、手动且极其昂贵的过程。货物经常会丢失、损坏或被盗。

然后出现了标准化运输集装箱的发明。这个简单的金属盒子改变了世界。无论你是运输香蕉、电子产品还是汽车，所有东西都放进了同样大小的盒子里。这些相同的盒子可以被起重机、火车和卡车高效地堆叠、移动和运输。起重机操作员不需要知道或关心盒子里装的是什么；他们只需要知道如何移动盒子。

**Docker是软件的标准化运输集装箱。**

在Docker之前，我们的应用程序就像那个混乱的货舱。要在服务器上运行它，我们必须手动安装正确版本的Python、正确的系统库、正确的应用程序代码以及其他十几个依赖项。每台服务器都略有不同，就像每个桶和板条箱都是不同的形状一样。

Docker允许我们将整个应用程序——代码、特定版本的Python、所有必需的库，甚至操作系统的必要部分——打包到一个单一、整洁、标准化的盒子中，称为**容器**。

现在，我们可以拿起那个容器并在任何安装了Docker的服务器上运行它——Anjali的Macbook、我们的预发布服务器、我们的生产服务器——它每次都会以完全相同的方式工作。"它在我的机器上工作"的问题永远解决了。容器*就是*机器。

#### **Docker组件：蓝图、盒子和构建**

要使用Docker，您只需要理解三个核心概念：

- **Dockerfile（蓝图）：** Dockerfile是一个简单的纯文本文件，包含构建容器镜像的逐步指令。它就像宜家的说明书。
  - 步骤1：以基本操作系统开始（例如，Ubuntu 20.04）。
  - 步骤2：安装Python 3.9.6版本。
  - 步骤3：将我们的应用程序代码复制到容器中。
  - 步骤4：运行命令'pip install -r requirements.txt'来安装我们所有的依赖项。
  - 步骤5：定义容器启动时应运行的命令。
- **镜像（平板包装的盒子）：** 当您使用Dockerfile运行docker build命令时，Docker执行这些指令并创建一个**镜像**。镜像是最终的、打包好的、准备发货但当前不活跃的盒子。它是平板包装的宜家书架盒子，包含所有零件、螺丝和说明，准备组装。
- **容器（组装好的书架）：** 当您在镜像上运行docker run命令时，您创建了一个**容器**。容器是镜像的实时运行实例。它是完全组装好的书架，现在在您的客厅里，存放您的书籍。神奇之处在于，您可以从同一个单一镜像创建数百个相同、隔离的容器。

#### **技术深入：虚拟机与容器**

"等等，"您可能会想，"这听起来很像虚拟机。"这是一个常见的混淆点，但它们之间的区别是理解效率提升的关键。

**虚拟机（VM）就像房子。** VM模拟完整的物理计算机，包括硬件。每个VM需要运行自己完整的客户操作系统副本。

- **类比：** 如果您的物理服务器是一块土地，VM是一个完整、独立的**房子**。它有自己沉重的地基、自己的墙壁、自己的管道和自己的电气系统（客户操作系统）。然后，您将应用程序（您的家具）放在那所房子里。它非常隔离，但只为了布置一个房间而建造整个房子是非常沉重和浪费的。

**容器就像公寓。** 容器不模拟硬件。它虚拟化操作系统。主机上的所有容器共享主机的OS内核。

- **类比：** 如果您的服务器是一块土地，Docker是一座**公寓楼**。所有公寓共享相同的主要地基和管道（主机OS内核），这要高效得多。但每个公寓都有自己的锁门、自己的墙壁和自己的家具和装饰（应用程序及其依赖项）。公寓是轻量级的，彼此完全隔离。

由于容器比VM轻得多，它们在几秒而不是几分钟内启动。而且，对于我们不断攀升的AWS账单最重要的是，您可以在一座建筑中容纳比在同一块土地上容纳整个房屋多得多的公寓。这称为**密度**。

我们现在可以采用一个大型服务器，而不是在其上只运行一两个应用程序进程，我们可以运行10个、20个甚至50个隔离的容器。这将使我们能够充分利用服务器的CPU和RAM潜力，大大减少我们需要的服务器数量并大幅降低成本。

我们理解了理论。运输集装箱是未来。现在是时候开始打包我们的第一个盒子了。

### 第三部分：打包我们的第一个盒子

Docker背后的理论是一个启示。它承诺解决我们两个困扰的问题：困扰开发人员的不一致性和消耗我们银行账户的低效率。是时候从理论转向实践了。是时候编写我们的第一个Dockerfile并将我们的第一个应用程序打包到标准化容器中了。

#### **技术深入：编写我们的第一个Dockerfile**

我们决定首先将我们最关键的应用程序容器化：Python/Django单体。Dockerfile只是一个与您的代码一起存放的名为Dockerfile的纯文本文件。它是构建镜像的配方。我们的Dockerfile看起来像这样：

```DockerFile
  #Step 1: 从官方、受信任的基础镜像开始。
  #我们使用的是特定版本的Python，运行在精简版的Debian（一种Linux操作系统）上。
  FROM python:3.9-slim

  #设置环境变量，使Python以优化模式运行。
  ENV PYTHONUNBUFFERED 1

  #设置容器内的工作目录。所有后续命令
  #将从这里运行。
  WORKDIR /app

  #将列出所有Python依赖项的文件复制到容器中。
  COPY requirements.txt .

  #Step 2: 安装所有依赖项。
  #这一步会被Docker「缓存」。如果requirements.txt没有更改，

  #Docker不会重新运行这一步，使未来的构建更快。
  RUN pip install --no-cache-dir -r requirements.txt

  #Step 3: 将我们的实际应用程序代码复制到容器中。
  COPY . .

  #Step 4: 定义容器启动时要运行的命令。
  #这个命令启动我们的Gunicorn应用服务器。

  CMD ["gunicorn", "--bind", "0.0.0.0:8000", "dukaan.wsgi:application"]
```

这个简单的文件是我们的蓝图。它是我们应用程序环境的完美、可重复和版本控制的定义。不再有歧义。Python的确切版本和依赖项的确切列表现在被编码了。

#### **新工作流程：运输镜像，而不是代码**

Docker彻底改变了我们的开发和部署流程。

- **旧方式：** 开发人员会在他们的笔记本电脑上编写代码，这有其自己独特的环境。他们会将代码推送到GitHub。然后我们的服务器会拉取该代码并尝试在*他们*自己独特的环境中运行它。这些环境之间的差距就是bug所在的地方。
- **新方式：** 工作流程现在以镜像为中心。
  - 开发人员编写代码和Dockerfile。
  - 在他们自己的笔记本电脑上，他们通过运行docker build .来构建镜像。这创建了一个打包的、自包含的盒子。
  - 他们通过在笔记本电脑上以容器的形式运行*完全相同的镜像*来测试该功能：docker run <image_name>。如果它在这里工作，我们就有很高的信心它会在任何地方工作。
  - 他们现在不再只是推送代码，而是将构建好的**镜像**推送到中央**容器注册表**（如Docker Hub或Amazon ECR）。注册表是您的容器镜像的仓库。
  - 我们的预发布和生产服务器不再从GitHub拉取代码。它们从注册表中拉取预构建、预测试的镜像并运行它。

根本转变是：**我们停止运输代码并开始运输镜像。** 环境现在与应用程序捆绑在一起。"它在我的机器上工作"的问题已经解决。如果它在开发人员笔记本电脑上的容器中工作，它将在生产服务器上完全相同的容器中工作。

#### **影响：大幅降低成本**

第二个同样显著的影响是对我们的服务器账单的影响。我们立即开始将我们的服务器视为通用的"容器主机"。我们现在可以采用一个较大的EC2实例，并使用它来运行多个完全隔离的服务的容器。在单个机器上，我们可以有：

- 五个运行我们主要Django单体的容器。
- 十个用于高流量店面服务的容器。
- 两个用于我们的缓存失效器服务的容器。
- 三个用于新的图像调整器服务的容器。

所有这些容器都在同一主机上运行，共享OS内核但彼此完全隔离。我们的服务器利用率飙升。我们不再为二十个半空的房子付费；我们为五个完全占用的公寓楼付费。我们能够将应用服务器数量减少60%以上，成本节约立即且巨大。

#### **新问题：没有指挥的管弦乐队**

我们已经解决了不一致性和低效率的问题。但与往常一样，解决一个问题会产生一个新的、更有趣的问题。

Docker是一个构建、运输和运行单个容器的绝佳工具。但我们的生产环境现在是一个复杂的系统，由数百个容器在服务器集群上运行。这引入了一系列全新的问题：

- 如果服务器死亡，我们如何将其50个容器移动到健康的服务器上？
- 如果单个容器崩溃，谁负责自动重启它？
- 当我们想要部署新版本的代码时，我们如何执行滚动更新，用100个新容器替换100个旧容器，而不会有任何停机时间？
- 店面服务的容器如何找到并与核心API的容器通信，当它们可能在不同的主机上运行时？

我们现在拥有一个庞大的容器管弦乐队。但我们试图手动管理它们，四处奔波告诉每个音乐家何时演奏。这是一种新的混乱。

我们需要一位指挥。我们需要一个自动化系统来大规模管理容器的整个生命周期。我们需要一个**容器编排器**。我们准备好迈出基础设施旅程的下一个巨大飞跃：是时候学习**Kubernetes**了。

## 第10章：要点总结

- **Docker解决了不一致性（"它在我的机器上工作"）和低效率（服务器利用率不足）的双头龙问题。**
- **Dockerfile是应用程序环境的蓝图。** 它将您的依赖项编码，确保您的应用程序在任何地方都以相同的方式运行。
- **将您的思维从运输代码转变为运输镜像。** 容器镜像是您的应用程序及其整个环境的自包含、不可变的包。
- **容器允许高密度服务器利用。** 通过在单个主机上运行多个隔离的容器，您可以大大降低基础设施成本。
- **管理几个容器很容易。管理数百个是一场噩梦。** 一旦您采用容器，您将不可避免地需要一个容器编排器来大规模管理它们。

## 第11章：聪明的店员：构建世界级搜索

我们的平台日益强大。我们已经解决了稳定性问题，征服了规模，并驯服了复杂性。我们的系统是现代工程的证明。但如果方向盘坏了，强大的引擎就毫无用处。对于我们的用户来说，搜索栏——电子商务最关键的工具之一——从根本上是坏的。

这不是服务器崩溃的故事，而是客户挫折和销售损失的故事。这是我们如何用智能发现引擎替换"愚蠢"搜索栏的故事，在这个过程中，我们了解到一个伟大的平台不仅仅是关于你构建什么，而是关于你如何帮助用户找到它。

### 第一部分：故障搜索的挫折

这个问题不是以仪表板上的警报形式出现的。它是以我们增长最快的卖家之一的愤怒电子邮件形式出现的。他在Dukaan上拥有一家大型在线鞋店，正在为排灯节促销活动开展一场大型营销活动。他的广告流量很高，但销售额却神秘地很低。

"我的客户在抱怨，"他写道。"他们搜索'跑鞋'却得到零结果。我有200多种不同的跑鞋！他们搜索'运动鞋'却找不到任何东西。我的畅销产品是'Nike Air Zoom Pegasus 38男士跑鞋'，但如果有人只输入'Nike Pegasus跑鞋'，它不会显示出来。这是一场灾难。你正在让我损失金钱。"

Suumit立即看到了业务影响。我们深入研究了分析数据。数据令人震惊。使用我们搜索栏的用户的转化率只是手动浏览用户转化率的一小部分。搜索栏不仅没有帮助用户找到产品；它实际上在阻止他们，并将他们引向死胡同。

#### **识别问题："愚蠢"的店员**

根本原因是我们在急于构建MVP时实现搜索的简单、天真的方式。当用户输入查询时，我们对PostgreSQL数据库运行基本的SQL命令，大致如下所示：

```sql
SELECT * FROM products WHERE name ILIKE '%running shoes%';
```

Postgres中的ILIKE命令执行不区分大小写的子字符串搜索。它向数据库询问："字母'r-u-n-n-i-n-g- -s-h-o-e-s'是否以完全相同的顺序出现在产品名称的某个位置？"

对于一个旨在成为可靠交易记录的系统来说，这是一个完全合理的查询。但对于试图查找产品的客户来说，这是一种极其"愚蠢"的查找方式。

- **类比：** 我们的PostgreSQL搜索就像一个非常字面的、略显愚蠢的图书管理员。如果你要求一本关于"American Cars"的书，图书管理员会沿着书架走，查看每一本书的标题，只拿出那些包含确切短语"American Cars"的书。他们会完全忽略一本名为"The Automobiles of the USA"或"A History of Ford and General Motors"的书。他们缺乏任何上下文或智能。

#### **技术深入：为什么简单的数据库搜索在电子商务中失败**

我们的"愚蠢店员"在四个关键方面让我们的用户失望：

- **它要求完美：** 它只匹配完全相同顺序的精确子字符串。一个标题为**"Nike Running Shoe for Men"**的产品不会被搜索**"Nike shoes for running"**找到。单词都在那里，但顺序不对。搜索复数形式的**"shoes"**不会找到单数形式的**"Shoe"**。
- **它缺乏所有上下文：** 它没有人类语言的概念。它不知道"sneakers"和"trainers"是"running shoes"的同义词。它不理解"run"、"runs"和"running"都是相关的（这一概念称为**词干提取**）。
- **它对拼写错误毫不宽容：** 如果用户在手机上不小心输入**"runing shoes"**，ILIKE查询将找不到匹配项。对人为错误零容忍，这在任何面向用户的系统中都是致命缺陷。
- **它无法按相关性排序：** 即使找到了匹配项，它也会按照数据库使用的默认顺序返回它们（通常是按添加日期）。一件恰好在其长描述中有"inspired by vintage running shoes"的T恤会被视为与实际畅销跑鞋本身同等重要。最相关的产品被埋没了。

此外，这些LIKE '%...%'查询对数据库来说众所周知地缓慢且低效。它们无法有效地使用标准索引，并且通常会强制数据库执行"全表扫描"——字面意思是读取每个产品的名称以找到匹配项。随着卖家的目录增长到数千个，我们的搜索变得越来越慢，给我们的读取副本带来了不必要的负担。

我们得出了明确的结论。我们的搜索功能不是可以通过几处调整来优化或改进的东西。整个方法对于电子商务平台来说根本上是错误的。我们无法训练我们的愚蠢店员变得更聪明。我们必须聘请一位天才。

### 第二部分：天才店员

我们旧的搜索系统是一个负担。我们无法通过一些聪明的SQL技巧来修复它；基础本身就是错误的。我们不需要重新培训我们的愚蠢店员；我们需要解雇他们，聘请一位世界知名的天才研究助手。经过深入研究搜索技术领域，选择是显而易见的。我们需要**Elasticsearch**。

Elasticsearch是一个开源、分布式的搜索和分析引擎。它不是我们PostgreSQL数据库的替代品；它是一个高度专业化的工具，就像一级方程式赛车，设计和制造只有一个目的：在搜索大量文本时速度极快且极其智能。

#### **技术深入：Elasticsearch的超能力**

要理解为什么Elasticsearch在电子商务搜索方面如此出色，您需要了解它的核心超能力。

- **类比：** 如果PostgreSQL是我们一丝不苟、有条理的图书管理员，是每本书主记录的守护者，那么**Elasticsearch**就是天才的多语言研究助手，他读过每一本书，并为每个概念、关键词和字符创建了一个庞大的、交叉引用的索引。您向图书管理员询问一本书的确切书名。您向研究助手询问"关于未来悲伤机器人的书籍"，他们立即给您一份完美排名的列表。

这位天才助手是这样施展魔法的：

**超能力#1：倒排索引（速度的秘密）** 当您搜索一个词时，传统数据库必须通读每一个产品描述才能找到匹配项。Elasticsearch则相反。它使用**倒排索引**。

在您进行任何搜索之前，Elasticsearch会读取所有产品数据并构建一个类似于教科书索引的映射。它将每个单词映射到包含该单词的所有产品的列表。

```
"nike" -> [Product 1, Product 5, Product 88]
"running" -> [Product 1, Product 7, Product 23, Product 88]
"shoes" -> [Product 1, Product 23, Product 55, Product 88]
```

当您搜索"nike running shoes"时，Elasticsearch不会读取产品。它只是查看其索引，找到"nike"、"running"和"shoes"的列表，并立即找到在所有三个列表中都存在的产品（在这种情况下，Product 1和Product 88）。这就是为什么它能在几毫秒内找到数百万文档中的匹配项。

**超能力#2：高级文本分析（智能）** Elasticsearch不仅仅存储文本；它还会**分析**文本以理解其含义。当我们索引一个名为"Nike's Best Running Shoes"的产品时，它执行一系列步骤：

- **分词：** 它将句子分解为单个单词，或"标记"：Nike's, Best, Running, Shoes。
- **标准化：** 它将所有内容转换为小写：nike's, best, running, shoes。
- **词干提取/词形还原：** 它将单词减少到其词根形式。Running变成run，Shoes变成shoe。这意味着搜索"run shoe"现在将匹配"Running Shoes"。这个单一功能解决了我们问题的很大一部分。
- **同义词支持：** 我们可以向Elasticsearch提供自定义词典，教它"sneakers"、"trainers"和"kicks"都是"shoes"的同义词。现在，搜索"Nike sneakers"将正确找到我们的"Nike Running Shoes"。

**超能力#3：相关性评分（排名）** 这是区分良好搜索和卓越搜索的关键。Elasticsearch不仅找到结果；它还**按相关性对它们进行排名**。它使用复杂的算法（如BM25），就像一个聪明的销售人员。它知道product_title字段中的匹配比product_description中的匹配重要得多。它给更罕见的关键词更多权重。结果是，用户最有可能购买的产品总是出现在顶部。

**超能力#4：模糊匹配（拼写错误宽容）** 最后，Elasticsearch是为人类而构建的。它可以配置为处理拼写错误和拼写错误。如果用户搜索**"runing sheos"**，Elasticsearch可以被调整为识别这可能是"running shoes"的拼写错误（它测量单词之间的"编辑距离"）并返回正确的结果。对于在手机上快速打字的用户来说，这是一个游戏规则的改变。

决定已经做出。Elasticsearch是我们需要聘请的天才店员。我们将构建一个新的、专门用于搜索的微服务。

唯一剩下的问题是一个熟悉的问题：我们如何使这个新的专业引擎与我们的主数据库完美同步？答案，再一次，在于我们强大的中枢神经系统：Kafka。

### 第三部分：实现（由Kafka驱动）

我们找到了Elasticsearch这位天才店员。拼图的最后一块是为它建造办公室，并确保它收到主图书馆发生的每一次更新的实时信息流。我们需要保持搜索索引与我们的主数据库完美同步。

几个章节前，这将是一个复杂的工程挑战，需要自定义构建的脆弱管道。但现在，由于我们投资建立了以Kafka为中心的中枢神经系统，解决方案变得极其简单和优雅。

#### **架构：新的监听器**

我们的Kafka和Debezium设置已经是平台上发生的所有变化的"真相来源"。我们不需要构建新的数据管道；我们只需要让我们的新搜索服务订阅现有的管道。

- 我们构建了一个新的独立微服务，称为search-service。它唯一的工作是管理Elasticsearch集群并向我们的店面暴露搜索API。
- 我们让这个新服务成为我们现有Kafka主题的另一个**消费者**。

这就是整个架构设计。我们正在将一个新设备插入到我们已经构建的电网中。

#### **数据流：一个事件，多个动作**

产品更新的新的、完整的数据流完美地说明了解耦、事件驱动架构的力量。

- 卖家创建或更新产品。更改保存到我们在孟买的主**PostgreSQL**数据库中。
- **Debezium**，我们的变更数据捕获代理，不断监视数据库的事务日志。它立即看到了变化。
- Debezium生成一个包含新产品数据的详细、结构化消息，并将其发送到**Kafka**中的product_updates主题。
- 现在，两个完全独立的服务，都订阅了同一个主题，同时开始行动：
  - **消费者#1（缓存失效器）**：我们现有的服务看到消息，读取store_id，并向**Redis**发送命令以删除该商店的旧缓存。
  - **消费者#2（新搜索服务）**：我们的新服务也收到完全相同的消息。它解析完整的产品数据，将其转换为JSON文档，并将其发送到**Elasticsearch**集群进行索引。

这个系统非常漂亮。处理卖家初始操作的单体应用程序**完全不知道**搜索引擎或缓存的存在。它唯一的工作是将数据保存到数据库。下游系统——缓存、搜索，以及未来的分析或推荐引擎——都可以独立订阅事件流并做出相应反应。我们可以在不接触核心应用程序代码的情况下，添加由这些事件驱动的新功能。

结果是Dukaan平台的变革性升级。新的搜索具有拼写错误容忍性、上下文感知能力，并在毫秒内返回高度相关的结果。我们的卖家欣喜若狂；他们的产品现在很容易被发现，我们看到来自搜索栏的销售直接显著增加。我们将一个关键弱点转变为强大的、一流的功能。

## 第11章：要点总结

- 简单的数据库查询（如SQL LIKE）不能替代真正的搜索引擎。对于良好的电子商务体验，你需要一个专门的工具。
- **Elasticsearch**是一个强大的解决方案，提供关键功能——相关性排名、拼写错误容忍和语言分析——这些功能能推动转化并改善用户体验。
- 使用像Kafka这样的工具的**事件驱动架构**是一种极其强大和优雅的方式，可以保持不同系统（如主数据库、缓存和搜索索引）的完美同步。
- "一对多"或"扇出"模式，其中来自生产者的单个事件触发多个独立消费者的行动，是构建可扩展和解耦微服务架构的基石。

<br/>
## 第12章：送货男孩：静态资产的CDN

随着我们平台的成长，我们的野心也在增长。我们不再只是一家印度公司；卖家从世界各地有机地发现了我们。我们在东南亚、欧洲甚至南美洲都有店铺出现。我们正在解决一个全球性的问题。

然而，这种全球影响力暴露了我们架构中的一个新弱点，这个弱点与CPU、内存或我们的代码无关。它与光速这一简单、古老、不可避免的物理现象有关。我们的服务器在孟买，但我们的用户无处不在。对于这些用户来说，我们的网站慢得令人痛苦。本章讲述我们如何通过建立自己的国际配送网络来解决全球性能问题。

### 第一部分：长途跋涉

第一个麻烦的迹象来自巴西的一位卖家。她经营着一家销售手工艺品的小店，爱上了Dukaan的简单性。但她有一个问题。"你们的平台太棒了，"她写信给我们的支持团队，"但我在圣保罗的客户告诉我，产品图片加载时间太长了。他们认为我的网站坏了。"

与此同时，Suumit向我转发了我们的月度AWS账单，并在一行用红色突出显示：**从EC2传出的数据传输**。成本正在螺旋上升，增长速度远快于我们的服务器成本。

我们面临着另一个双头龙。我们的国际用户体验很糟糕，而AWS账单上的一个神秘项目正在吞噬我们的利润。事实证明，这两个问题有着完全相同的原因。

#### **识别问题：孟买图书馆**

我们所有的基础设施——我们的应用服务器、负载均衡器、数据库——都位于一个单一的地方：亚马逊网络服务（AWS）在孟买的数据中心区域。这对我们的印度用户来说很棒，但对其他人来说很糟糕。

让我们思考一下单个产品图片的旅程。

- 我们在圣保罗的卖家上传了一张她手工艺品的漂亮、高分辨率照片。那个2MB的图像文件必须通过互联网的海底电缆跨越14,000多公里到达我们在孟买的服务器进行存储。
- 同样在圣保罗的一位客户访问她的商店。他们的网络浏览器请求那个图像。
- 请求从圣保罗到我们的孟买服务器旅行14,000公里。
- 我们的服务器找到2MB图像，并将其全部传回14,000公里外圣保罗客户的浏览器。

这个由光速决定的往返行程，给每个图像的加载增加了数秒的延迟。这就是为什么我们巴西卖家的商店对她的本地客户来说如此缓慢。

这也解释了我们庞大的AWS账单。那个项目，"数据传输出站"，本质上是AWS向您收取的将数据从他们的数据中心发送到公共互联网的邮费。每次我们向用户提供图像时，我们都支付一小笔费用。向全球受众提供数百万个大型图像文件意味着我们在数字邮费上花费了一大笔钱。

我们的服务器就像孟买的一个单一中央图书馆，保存着世界上所有的相册。如果巴西的某人想看一张照片，我们实际上是在邮寄给他们一份副本。这很慢，而且很贵。

#### **技术深入：什么是内容分发网络（CDN）？**

这个问题的解决方案是**内容分发网络**，或**CDN**。

CDN是一个专门设计用来尽可能快地向用户提供静态内容（如图像、视频、CSS和JavaScript文件）的全球服务器网络。

- **类比：** 想象一个全球图书馆特许经营店，而不是孟买的一个中央图书馆。在孟买有一个主分支，但在圣保罗、伦敦、新加坡和东京也有本地分支。当一本新相册在孟买主图书馆（我们的"源"服务器）发布时，副本会自动发送到世界各地的每个特许经营图书馆。

现在，当我们在圣保罗的客户想看照片时：

- 他们的请求不会去到孟买。它会自动路由到最近的"图书馆分支"——CDN在圣保罗的**边缘位置**。
- 圣保罗边缘位置已经有了照片的副本。它直接将其提供给客户，几乎是即时的。

这个过程被称为**边缘缓存**。当一个地区的用户首次请求一个文件时，边缘位置会从我们在孟买的源服务器获取它并保存副本。对于该地区的每个后续用户，它都会提供缓存的副本。

CDN同时解决了我们的两个问题：

- **速度：** 通过从物理上靠近用户的服务器提供图像，我们可以将加载时间从秒缩短到毫秒。我们巴西用户的体验将与我们印度用户的体验一样快。
- **成本：** 数据现在从CDN的网络提供，而不是我们的网络。CDN提供商批量购买带宽，并且可以比我们从应用服务器更便宜地提供数据。这将大大减少我们的"数据传输出站"账单。

我们理解了这个概念。是时候注册一个全球图书馆特许经营店，并教我们的应用程序如何使用它了。

### 第二部分：全球配送网络

我们完全理解了CDN的理论。这是一个简单、优雅的解决方案，同时解决了我们的全球速度问题和螺旋上升的数据传输成本。现在，是时候实施它了。

由于我们的整个基础设施已经在亚马逊网络服务（AWS）上运行，自然的选择是使用他们的原生CDN解决方案**Amazon CloudFront**。这意味着我们可以从与服务器和数据库相同的控制台管理我们的CDN，保持简单。

#### **技术深入：设置AWS CloudFront**

设置CDN的过程涉及三个主要步骤：为原始文件创建中央存储位置，配置CDN以从中拉取，最后，告诉应用程序使用新的CDN链接。

**步骤1：源 - Amazon S3**

首先，我们需要一个所有静态资产的"主图书馆"。直接从应用服务器提供图像效率低下。相反，我们使用了**Amazon S3（简单存储服务）**。

S3本质上是云中一个巨大的、极其耐用且非常便宜的硬盘。它专门设计用于存储文件。我们改变了应用程序逻辑，使得每当卖家上传产品图像时，我们的Django应用程序不再将其保存到服务器的本地磁盘，而是直接将其上传到名为dukaan-product-images的专用S3存储桶。这个位于孟买地区的存储桶将成为我们所有用户生成图像的单一真实来源。

**步骤2：创建CloudFront "分发"**

随着我们的源图书馆就位，我们进入AWS CloudFront控制台并创建了一个新的"分发"，这是他们对CDN配置的称呼。设置是一个简单的向导：

- **源域：** 我们告诉CloudFront在哪里找到原始文件。我们直接指向我们刚刚创建的S3存储桶。
- **缓存行为：** 我们配置了全球图书馆特许经营店的规则。我们设置了**TTL（生存时间）**为24小时。这意味着一旦边缘位置缓存了图像，它将提供该副本24小时，然后才会与我们在孟买的S3存储桶检查是否有更新的版本。
- **部署：** 我们点击"创建分发"。AWS随后花费了大约15分钟将我们的新配置部署到其整个全球网络的数百个边缘位置。

完成后，AWS为我们的CDN提供了一个唯一的域名，类似于d123xyzabcdef.cloudfront.net。这现在是我们全球配送网络的公共地址。

**步骤3：更新我们的应用程序**

这是最后一步。我们网站的代码仍然生成旧的图像链接。图像URL看起来像这样：<https://dukaan.app/media/seller123/product.jpg>

我们必须更新我们的代码，生成指向CDN的新链接。我们还设置了一个自定义的、更专业的域名cdn.dukaan.app，指向那个难看的CloudFront地址。新的图像URL现在看起来像这样：<https://cdn.dukaan.app/seller123/product.jpg>

我们部署了代码更改。手术完成了。

#### **结果：更快的网站和更便宜的账单**

影响是立竿见影且戏剧性的。

我们联系了巴西的卖家，请她再次检查她的商店。她第一次加载时，速度仍然有点慢。这是"缓存未命中"——CloudFront在圣保罗的边缘位置第一次看到她的图像，并从我们在孟买的S3存储桶拉取它们。

但下次她或南美洲的任何其他客户加载页面时，体验完全不同。图像立即从圣保罗边缘位置提供。她的页面加载时间从6秒多降至2秒以下。她非常高兴。

月底，Suumit和我查看了AWS账单。"从EC2传出的数据传输"行项目被大幅减少。它被一个新的、小得多的"CloudFront"行项目所取代。我们将带宽成本削减了70%以上。

双头龙被杀死了。通过一个相对简单的架构变更，我们大大改善了全球客户的用户体验，并在此过程中节省了大量资金。

<br/>
## 第12章：要点总结

- **对于任何拥有地理多样化用户群的应用程序，CDN是一个必不可少的工具。** 它是您可以做出的改善全球性能的最简单且影响最大的变更之一。
- **CDN同时解决两个问题：速度和成本。** 它们通过从附近的服务器提供内容来减少用户的延迟，并通过减少从源服务器进行昂贵的数据传输来降低您的成本。
- **将静态资产与应用程序服务器分开。** 使用像Amazon S3这样的专用对象存储服务作为CDN的源。这是一种更可扩展且更具成本效益的架构。
- **实施很简单：** 为原始文件创建存储桶，将CDN分发指向该存储桶，并更新应用程序的HTML以使用新的CDN URL来访问图像、CSS和JavaScript文件。

## 第13章：指挥家：使用Kubernetes编排一切

Docker改变了我们的世界。"在我机器上能运行"的问题消失了。我们的部署是一致的，我们的服务器比以往任何时候都更高效地打包。我们感觉自己发现了一种超能力。我们可以将应用程序的任何部分打包成整洁、便携的盒子，并在任何地方运行它。

但我们很快发现了这种新能力的黑暗面。我们用几十台混乱的虚拟机换来了数百个整洁、有序的容器组成的军队。军队需要一位将军。舰队需要一位海军上将。我们新的容器化世界是一种新的混乱，而我们试图手动管理这一切。

本章讲述我们进入复杂但强大的容器编排世界的旅程，以及我们如何找到管弦乐队的指挥：**Kubernetes**。

### 第一部分：没有指挥的管弦乐队

我们采用Docker后的最初几个月感觉像是生产力爆发。但随着我们将更多服务从单体中拆分出来，我们的容器数量从十几个增长到一百多个，运营现实开始变成一场噩梦。我成为了公司工资最高、压力最大的手动码头工人。

崩溃点出现在一个周六的凌晨2点。

一个警报把我惊醒："主机不可达：app-server-07"。我们的一个EC2实例发生了硬件故障并离线。在过去，这在我们的负载均衡器下是一个简单的问题。但这台服务器运行了30个不同的容器，用于各种服务——API的一部分、缓存失效器、图像调整服务等等。

我的"修复"是疯狂的手动忙乱。

- 启动一个新的空EC2实例。
- SSH进入它并安装Docker。
- 尝试记住死亡服务器上运行的30个容器。
- 手动输入30个不同的docker run ...命令，试图在凌晨2点不在长图像名称或端口映射中犯打字错误。

我花了将近一个小时的压力大、容易出错的工作来恢复服务。系统工作了，但这个过程很脆弱，依赖于我的记忆和打字技能。这不是一个真正的解决方案。

部署同样可怕。发布storefront-service的新版本现在意味着更新我们服务器群中的50个运行中的容器。我们必须进行复杂的手动操作：停止旧容器，启动新容器，等待它健康，然后移动到下一个，希望我们不会给用户造成任何停机时间。

Suumit能看出我的压力。"你看起来一周没睡了，"他在一次通话中说。"我们发货更快了，但你变成了瓶颈。我们不能让整个公司的稳定性依赖于你手动在终端中输入命令。"

他是对的。我晚上在与运营火灾作斗争，而不是白天构建更好的产品。我开始研究其他公司如何大规模管理容器。一个名字不断出现，这个名字似乎极其复杂和令人生畏：**Kubernetes**。

学习曲线看起来像一堵垂直的墙。文档中充满了奇怪的新词汇：Pods、Deployments、Services、Ingress、ReplicaSets、YAML文件。这感觉像是学习一种新的外星语言。但这个承诺太大了，不容忽视。自动化、自我修复系统的承诺。

我告诉Suumit："我需要消失一个星期。我要把自己锁在房间里，学习这个Kubernetes。我认为这是我们能够在成长的下一阶段生存下来的唯一方式。"

#### **识别问题：我们需要一个容器管理器**

我为期一周的Kubernetes探索之旅证实了我的怀疑。我们不仅缺少一个工具；我们缺少一整个类别的软件。我们的手动流程在四个关键方面失败：

- **没有自我修复：** 当容器崩溃时，它一直保持死亡状态，直到人类重新启动它。当服务器死亡时，其容器永远消失，直到人类在其他地方重建它们。
- **没有智能调度：** 我们手动决定哪些容器应该在哪些服务器上运行，导致效率低下。
- **没有自动滚动更新：** 部署新代码是一个危险的手动过程，需要停止和启动容器。
- **没有服务发现：** 当容器可以随时在服务器之间启动、停止和移动时，我们的order-service容器如何可靠地找到并与50个user-service容器中的一个通信？

我们不仅仅需要**运行**容器。我们需要一个系统来**自动管理**它们的整个生命周期。我们需要一个容器编排器。

<br/>
#### **技术深入：什么是容器编排？**

容器编排的最佳类比是交响乐团。

- 我们的单个容器是**音乐家**。每个都是有才华的专家。storefront-service容器是小提琴手，database-connector是大提琴手，image-resizer是打击乐手。
- 如果你只是把这100位有才华的音乐家放在一个房间里，告诉他们"演奏"，你会得到一种可怕的、混乱的噪音。这就是我们目前的情况。

要创造美妙的音乐，你需要一位**指挥家**。指挥家不演奏乐器。他们的工作是领导和管理整个管弦乐队。

- 指挥家有乐谱（期望状态），并确保管弦乐队正确演奏。
- 他们告诉小提琴手何时开始和停止演奏。
- 他们控制节奏和音量。
- 如果小提琴手的琴弦断了（容器崩溃），指挥家无缝地示意另一位小提琴手接管该部分。

**Kubernetes是我们容器管弦乐队的指挥。**

它不会替代Docker。Docker仍然是每个音乐家演奏的乐器。Kubernetes是主脑，告诉我们服务器群中的所有Docker实例做什么。它确保最终的表演——我们运行的应用程序——完全符合乐谱（我们的配置文件）所说的应该是什么样子。

我们找到了我们的指挥家。现在，是时候学习如何阅读它的音乐——了解Pods、Services和Deployments的基本语言。

### 第二部分：学习乐谱

我为期一周的Kubernetes深入研究是一次令人谦卑的经历。很明显，这是我们问题的答案，但概念很抽象，术语很陌生。要指挥我们新的容器管弦乐队，我首先必须学习指挥家的语言。

我发现Kubernetes建立在几个简单、强大的理念之上。一旦你理解了这些核心构建块，整个系统就开始变得有意义。

#### **技术深入：Kubernetes核心组件**

让我们分解Kubernetes的基本词汇。

**1. Nodes：音乐家的椅子**

一个**Node**是一台服务器——物理机或虚拟机——它是我们服务器集群的一部分。它是一台工作机器，已经配置为Kubernetes"集群"的一部分。它的工作是准备好等待指挥家的指令。

- **类比：** **Nodes**是管弦乐队舞台上的椅子。它们是音乐家将坐下来演奏的物理位置。我们将EC2实例集群安装了必要的Kubernetes软件，使它们成为我们新集群中的Nodes——一组空的、随时可用的椅子。

**2. Pods：音乐家**

这是最基本的概念。在Docker世界中，最小的单位是容器。在Kubernetes世界中，最小的可部署单位是**Pod**。

一个Pod是一个或多个容器的包装器。在99%的情况下，最简单的思考方式是将**Pod视为单个运行中的容器**。

- **类比：** 一个**Pod**是一位**坐在椅子上的音乐家**。Pod为容器提供直接环境——网络、存储、存在的空间。我们永远不会告诉Kubernetes"运行一个容器"。我们总是告诉它"创建一个Pod"，而Pod的配置指定在其中运行哪个容器镜像。这个额外的抽象层正是Kubernetes强大的原因。

**3. Services：乐组指示牌**

Pods是有生命周期的。它们可能崩溃并被替换。当Kubernetes替换一个崩溃的Pod时，新Pod将有一个全新的、不同的内部IP地址。这会造成一个巨大的问题：如果我们的storefront-service Pods需要与api-service Pods通信，如果它们的地址不断变化，它们如何找到它们？

答案是**Service**。Service为一组Pods提供单个、稳定的网络端点（一致的IP地址和DNS名称）。

- **类比：** 管弦乐队有20名小提琴手（Pods）。你不希望指挥家必须跟踪每位小提琴手的个人姓名。相反，在他们乐组前面有一个大指示牌，上面简单地写着"**小提琴**"（Service）。Service知道哪些小提琴手当前是健康的并且在场。当storefront-service想与API通信时，它不会请求特定的Pod；它只是向"api-service"的稳定地址发送请求。Service充当内部负载均衡器，接收请求并将其转发到其背后的一个健康的API Pod。如果一个API Pod死亡并被替换，Service会自动更新其健康端点列表。前端永远不知道也不关心。

**4. Deployments：乐谱**

这就是一切汇集在一起的地方。**Deployment**是我们人类描述我们期望状态的地方。它是告诉指挥家我们希望管弦乐队如何演奏的蓝图。

- **类比：** **Deployment**是给指挥家的**乐谱**。这份乐谱包含所有关键指令：
  - **"我需要50个storefront-service Pod的副本。"**（这个乐组应该有多少音乐家）。
  - **"它们必须使用Docker镜像dukaan/storefront:v2.1。"**（他们应该演奏哪种版本的乐器）。
  - **"当我给你新版本的音乐时，执行滚动更新：一个接一个地替换音乐家，这样音乐永远不会停止。"**（在不停机的情况下部署新代码的策略）。

Kubernetes的真正魔力在于它如何处理这份乐谱。指挥家（Kubernetes控制平面）处于一个持续的循环中，将Deployment（期望状态）与舞台上实际发生的情况（当前状态）进行比较。

如果乐谱说应该有50个storefront Pod，但指挥家只数到49个，因为其中一个刚刚崩溃，它不会惊慌失措。它只是说，"当前状态与期望状态不匹配"，并立即在健康的Node上创建一个新的storefront Pod，使计数恢复到50个。这正是我们梦寐以求的自我修复能力。

我们理解了这些概念。我们有词汇来描述我们的需求：我们想要一个**Deployment**来管理在我们的**Nodes**上运行的一组**Pods**，以及一个**Service**来让它们相互通信。

现在，是时候学习如何编写乐谱本身了。

### 第三部分：编写乐谱（YAML）

我们已经学习了这些概念。我们知道Kubernetes是指挥家，它将使用我们期望状态的"乐谱"来管理我们的容器管弦乐队。现在是时候学习如何编写那首音乐了。这意味着接受一种思考基础设施的新方式，并学习Kubernetes的语言：**YAML**。

#### **技术深入：声明式 vs 命令式**

这是转向Kubernetes时最重要的哲学转变。

- **旧方式（命令式）：** 这是我们一直以来的工作方式。我们给系统一系列逐步命令。"SSH到服务器A。运行这个docker run命令。现在SSH到服务器B。运行另一个docker run命令。"
  - **类比：** 你是一个微观管理者，告诉学徒_如何_建造椅子。"首先，找四条腿。接下来，找座位。现在，用三个螺丝固定左前腿..."这很繁琐，容易出错，而且如果出现问题也无法适应。
- **Kubernetes方式（声明式）：** 这完全不同。你不告诉Kubernetes_如何_做某事。你只是描述你想要达到的**最终状态**，然后让Kubernetes找出达到那里的最佳方式。
  - **类比：** 你是一个客户，给一个专业木匠一张详细的椅子蓝图。你说，"我想要一把看起来完全像这样的椅子。它必须有四条腿，漆成蓝色，高20英寸。"你不告诉他们先安装哪条腿。专业木匠（Kubernetes）拿着你的蓝图（你的配置文件）并使它成为现实。如果他们建好后发现一条腿摇晃，他们会修复它。如果他们发现颜色不对，他们会重新喷漆。他们的整个工作就是不断努力使现实与蓝图匹配。

这就是魔力。你声明你想要的状态，Kubernetes在后台作为"协调引擎"不知疲倦地工作来执行它。这种声明式模型的蓝图是用一种称为**YAML**的格式编写的。YAML是一种人类可读的数据格式，使用简单的缩进而键值对来描述配置。

#### **技术深入：我们的第一个Deployment YAML**

是时候为我们的storefront-service编写乐谱了。我们创建了一个名为storefront-deployment.yaml的文件。它一开始看起来令人生畏，但它只是我们想要的精确描述。

让我们一行一行地分解我们的第一个Deployment蓝图。

```yaml
#1. API版本和Kind告诉Kubernetes这是什么类型的对象。
#我们正在创建一个"Deployment"。
apiVersion: apps/v1
kind: Deployment

#2. Metadata是关于对象本身的数据，如其名称。
metadata:
  name: storefront-deployment

#3. Spec（规范）是最重要的部分。
#这是我们期望的状态——椅子的蓝图。
spec:
  #4. 我们希望运行50个相同的应用程序副本。
  replicas: 50

  #5. 这告诉Deployment如何找到它应该管理的Pods。
  #它查找任何具有标签"app: storefront"的Pods。
  selector:
    matchLabels:
      app: storefront

  #6. 这是Pod本身的模板或蓝图。
  #Deployment将基于此模板创建50个Pods。
  template:
    metadata:
      #7. 我们给Pods一个标签，以便Deployment可以找到它们。
      labels:
        app: storefront

    spec:
      #8. 此部分定义了要在Pod内运行的容器。
      containers:
        - name: storefront-container
          #9. 这是最关键的行：要运行的特定Docker镜像。
          image: dukaan/storefront:v2.1

          #10. 告诉Kubernetes我们的应用程序在容器内监听哪个端口。
          ports:
            - containerPort: 8000
```

#### **kubectl apply的魔力**

这个YAML文件只是一个文本文件。它自己什么都不做。我们需要一种方式将这份乐谱交给指挥家。为此的命令是kubectl（发音为"koob-control"），这是与Kubernetes集群交互的主要工具。

我们运行了一个简单的命令：

> kubectl apply -f storefront-deployment.yaml

然后，魔法开始了。

Kubernetes读取了我们的文件。它看到期望的状态是"50个运行dukaan/storefront:v2.1镜像的Pods"。它查看了集群的当前状态，发现"0个匹配的Pods"。协调循环启动了。Kubernetes的调度器智能地在我们的各个Nodes上找到空间，并在接下来的几秒钟内，精确地启动了50个storefront Pod的副本。

当我们需要部署更新时，真正的力量变得清晰。我们有了新版本的代码v2.2。我们只需编辑YAML文件并更改一行：image: dukaan/storefront:v2.2。

然后，我们运行**完全相同的命令**：kubectl apply -f storefront-deployment.yaml。

我们不必告诉Kubernetes如何执行更新。它查看了新的蓝图，将其与运行中的系统进行比较，并理解了差异。它自动启动了"滚动更新"，终止一个旧的v2.1 Pod并启动一个新的v2.2 Pod，等待它健康后再继续处理下一个。它在所有50个副本上执行此操作，没有任何停机时间。这正是我们一直梦想的安全、自动化且稳定的部署过程。

我们已经将乐谱交给了指挥家，管弦乐队开始演奏。但到目前为止，他们是在隔音室里演奏。我们仍然需要一种方式来打开门，让观众——我们的用户——听到音乐。

### 第四部分：打开音乐厅的门

我们成功地将乐谱交给了指挥家。我们的Kubernetes集群是活跃的，我们的storefront-deployment尽职地维护着恰好50个运行中的Pods，根据需要进行修复和替换。管弦乐队演奏得非常完美。

只有一个问题：他们在一个完全隔音的房间里演奏。

我们的Pods在集群内部的私有内部网络上运行。来自外部互联网的任何人都无法访问它们。我们有一个没有观众的管弦乐队。最后一步是创建一种安全可靠的方式来打开门，让我们的用户进来听音乐。这是Kubernetes **Service**的工作。

我们已经讨论过Service如何为Pods提供稳定的内部地址，以便它们相互通信。但Service也可以配置为将这些Pods暴露给外部世界。这时候你需要选择一个**Service类型**。

#### **技术深入：Kubernetes Service类型详解**

Kubernetes提供了几种开门的方式，每种都设计用于不同的用例。选择正确的方式对于构建安全且经济高效的系统至关重要。

**1. ClusterIP：私人内部对讲机**

这是默认的Service类型。它提供一个稳定的IP地址，**只能从集群内部访问**。

- **类比：** 这是音乐厅内的私人**内部对讲系统**。指挥家可以用它与"小提琴"乐组交谈，"小提琴"乐组可以用它与"打击乐"乐组交谈。但站在音乐厅外街道上的人无法访问这个对讲系统。
- **用例：** 这是所有内部、服务间通信的主力军。当我们的order-service需要从user-service获取数据时，它会与user-service的ClusterIP通信。这是最常见和最安全的服务类型。

**2. NodePort：消防通道**

NodePort服务接受内部ClusterIP并将其暴露在集群中**每个**Node上的特定高编号端口上。

- **类比：** 这就像在每个音乐家的椅子（Nodes）旁边打开一个**消防通道门**。如果你知道建筑物的主地址和特定的消防通道门编号（例如，Node_IP:30080），你可以直接访问该服务。这有点笨拙，你必须知道要去哪个建筑物，但这是一种快速进入的方式。
- **用例：** 这主要用于**调试或临时访问**。开发人员可能会使用NodePort来快速测试新服务，而无需设置适当的负载均衡器。由于它不安全并且要求客户端知道特定Node的IP，通常不用于实际的生产流量。

**3. LoadBalancer：主入口**

这是在AWS或Google Cloud等云提供商上将服务暴露给互联网的标准、最常见的方式。当你创建类型为LoadBalancer的Service时，你会得到NodePort提供的所有内容，但Kubernetes还会自动为你配置一个真正的外部云负载均衡器。

- **类比：** 这是音乐厅的宏伟**主入口和售票处**。云提供商构建了一个适当的、面向公众的入口（具有公共IP地址的负载均衡器）。这个入口接收所有入场的观众（互联网流量），并自动将他们引导到一个可用的消防通道门（NodePorts），然后引导他们找到正确的音乐家（Pods）。
- **用例：** 这是将单个服务暴露给互联网的完美方式。如果你有一个主要API，你为它创建一个类型为LoadBalancer的Service，你的云提供商给你一个单一、稳定的IP地址，你可以将你的域名指向这个地址。

**4. Ingress：智能礼宾**

LoadBalancer服务很好，但它有一个缺点：每次创建一个，你都会配置一个新的云负载均衡器，这可能会很昂贵。如果你有10个不同的服务想要暴露给公众怎么办？你不想为10个单独的负载均衡器付费。

这就是**Ingress**的用武之地。Ingress本身不是一种Service类型，而是一个更智能、更强大的层，位于多个服务前面。

- **类比：** Ingress就像一个智能的**前台礼宾**，位于一个容纳多个音乐厅的大型建筑的前台。观众来到一个单一的主入口（一个负载均衡器）。他们向礼宾出示门票。礼宾查看门票并说："啊，你是来看摇滚音乐会的（/rock），请去A厅。你是来看古典音乐会的（/classical），请去B厅。"
- **用例：** 这是管理微服务架构外部访问的最具成本效益和最强大的方式。你使用一个LoadBalancer服务将流量引导到你的Ingress控制器。然后，Ingress使用一组规则，根据URL路径（/api, /storefront）或主机名（api.dukaan.app, store.dukaan.app）将流量路由到不同的内部ClusterIP服务。

#### **我们的实现**

对于我们的storefront-service，我们使用了Ingress。这是完美的契合。我们为整个集群有一个主负载均衡器，我们的Ingress控制器充当智能礼宾。我们创建了一个规则，说："任何来自dukaan.app/store/*的流量都应该路由到storefront-service的内部ClusterIP。"

我们为Ingress规则编写了另一个YAML文件，运行了kubectl apply，就这样，音乐厅的门打开了。现在，公共互联网可以访问我们的storefront-service，它作为50个自我修复、自动管理的Pods在我们新的Kubernetes集群中运行。

我们做到了。我们驯服了数百个容器的混乱。我们为管弦乐队找到了指挥家。

## 第13章：要点总结

- **Kubernetes是容器管弦乐队的指挥家。** 它用自动化、声明式管理取代了手动、容易出错的任务。
- **接受声明式配置。** 与其告诉Kubernetes_如何_做某事，不如编写一个YAML文件，描述你想要的最终_状态_，然后让Kubernetes努力实现它。
- **理解核心组件：** **Nodes**是服务器，**Pods**是运行中的容器，**Deployments**是管理Pods的蓝图，**Services**是允许它们通信的组件。
- **Kubernetes提供强大的内置自我修复功能。** 如果Pod或Node失败，Deployment控制器将自动替换它，以维持你所需的副本数量。
- **使用正确的工具来暴露你的应用程序。** 对内部流量使用**ClusterIP**，对所有外部、面向用户的流量使用**Ingress**控制器作为智能、经济高效的"前门"。

### 第四部分：追求极致速度（构建世界级边缘网络）

我们从零开始重建了公司的引擎。我们从一个单一、噼啪作响的服务器，发展到由Kubernetes管理的弹性、自我修复和可扩展的容器管弦乐队。我们解决了稳定性、一致性和工作流程的问题。我们建立了一座堡垒，我们相信它可以承受任何风暴。

本书的这一部分讲述了接下来发生的事情。它讲述了一场意外飓风袭击我们堡垒的时刻，以及它仍然屹立不倒的令人惊讶的原因。这是一个关于我们如何发现，在海啸中生存的秘诀不是更大的墙，而是更好的海岸线的故事。

<br/>
## 第14章：鲨鱼坦克效应：火的考验

在印度创业生态系统中，没有比"鲨鱼坦克效应"更伟大、更可怕、更令人垂涎的事件了。当一个小企业在极受欢迎的电视节目《印度鲨鱼坦克》中亮相时，他们的网站流量不仅仅是增加；它在几分钟内经历了数十万好奇观众的近乎垂直的激增。这是对任何电子商务平台的终极火的考验。

我们认为我们的新Kubernetes集群已经为此做好了准备。事实是，没有任何集中式系统能够真正准备好。

### 第一部分：不可能的高峰

那是一个安静的周四晚上。值班工程师正在处理几个轻微的警报。晚上9:45，一条消息出现在我们公司的Slack上："**伙计们！JAIN SHIKANJI正在参加鲨鱼坦克！**"

每个人立即打开我们的Grafana监控仪表板。我们都在看同一个图表："并发用户数"。这条线之前一直徘徊在几百左右，突然变成了一面垂直的墙。它迅速突破了10,000，然后是30,000，然后是50,000，最终达到了惊人的**80,000并发用户**。

Slack上出现了紧张的沉默。在我们的脑海中，我们都在计算。我们的storefront-service运行在10个Pods上。即使有最好的优化，这10个Pods也不可能处理80,000个同时在线的用户。我们都在看着"Pod数量"图表，等待我们的水平Pod自动缩放器启动并开始英勇地扩展。

但奇怪的事情发生了。用户流量达到了不可能的高峰，但我们在孟买的10个Pods的CPU使用率只是略有上升。HPA看到CPU没有达到70%的危机阈值，几乎没有添加任何新的Pods。网站仍然闪电般快速。没有警报。没有任何东西崩溃。

一位高级工程师打破了Slack的沉默："我不明白。数学计算不通。10个pods如何服务80,000个并发用户而不崩溃？HPA甚至还没有时间反应。发生了什么？"

他是对的。我们都感觉到了。堡垒在坚持，但原因不是我们想象的那样。我们幸存了下来，但感觉就像我们意外发现了一个我们不知道自己拥有的秘密超能力。

#### **技术深入：反应式自动扩展的局限性**

要理解那晚的谜团，你首先需要理解为什么我们计划的防御——HPA——会失败。

典型的自动扩展循环，如Kubernetes HPA使用的循环，是**反应式**的。它不预测流量；它对流量的影响做出反应。这个过程有几个步骤，每个步骤都需要时间：

- **指标收集：** 一个称为Metrics Server的组件从所有Pods中抓取数据（如CPU使用率）。这不是实时发生的；它以设定的间隔发生，通常每15到30秒一次。
- **检测和决策：** HPA控制器分析这些数据。它看到CPU已经飙升，并计算需要多少新的Pods才能将平均值带回目标水平。这个步骤很快，但它只能在_收集到指标之后_发生。
- **Pod启动：** 这是最慢的部分。Kubernetes必须将新的Pods调度到可用的Nodes上。然后，Node必须拉取Docker镜像（这可能需要时间）并启动容器。最后，容器内的应用程序需要启动并变得"准备好"服务流量。

从最初的流量峰值到新的Pods实际服务用户，总时间很容易达到**两到五分钟**。

**为什么HPA在"闪洪"面前失败**

"鲨鱼坦克效应"不是上升的潮汐；它是海啸。流量在不到60秒的时间内从零接近峰值。

- **类比：** 反应式自动扩展就像注意到开始下大雨，并决定在你的房子上建造一个扩展来保持一切干燥。当你打好地基时，闪洪已经席卷了你的城镇。

当HPA能够将我们的集群从10个Pods扩展到它需要的150个时，最初80,000用户的巨大浪潮已经使原来的10个Pods崩溃了十几次。

那么，如果HPA太慢而无法成为英雄，是什么拯救了我们？

我们幸存下来的原因不是因为我们在孟买的中央Kubernetes集群及时扩展。它幸存下来是因为**绝大多数的80,000用户负载甚至从未到达我们的中央集群**。

它被一个高度可扩展、大规模分布式的**边缘网络**吸收和服务，我们已经出于完全不同的原因构建了这个网络：追求极致速度。这个网络就像一个全球减震器，在海啸能够到达我们的主要城市之前，将其力量分布在几十个较小的位置。我们_如何_和_为什么_构建那个网络的故事是我们旅程的下一章。

Jain Shikanji事件对我们来说是一个分水岭时刻。这是对我们平台隐藏力量的意外发现。但那种力量根本不是偶然的。它是我们几个月前开始的一个刻意和执着的追求的直接结果。这个追求是为了解决一个与流量峰值无关的问题，而与我们最大的全球竞争对手有关。

这是一个关于我们如何向Shopify宣战，以及我们如何决定将速度作为我们最重要的单一功能的故事。

## 第15章：向Shopify宣战：将性能作为功能

到这个时候，Dukaan在印度市场已经是一个知名的名字。我们发展迅速。但为了吸引更大的卖家并扩展到国际市场，我们不断被与电子商务的全球巨头进行比较：**Shopify**。

我的联合创始人Suumit非常着迷。他会日日夜夜注册Shopify试用账号，分析他们的商店，并与同时使用两个平台的商家交谈。他在寻找一个弱点，一个他们盔甲上的裂缝。

一天晚上，他打电话给我，声音中充满了一位刚刚发现敌人秘密弱点的将军的能量。

"Subhash，我找到了，"他说，声音低沉而兴奋。"我知道我们如何获胜。他们的商店在这里很慢。真的很慢。对于印度的客户来说，典型的Shopify商店加载需要三秒钟，有时甚至四秒钟。"

他是对的。Shopify的基础设施主要位于北美。对于他们的核心市场，它很快。但对于德里或班加罗尔的客户，每个请求都必须进行一次跨越半个世界的往返旅程。

"这是我们的机会，"Suumit继续说道。"我们现在无法在功能上超越他们，他们有10年的领先优势。但我们可以更快。不仅仅是快一点。如果我们能在世界任何地方都_即时_快速，会怎么样？这是一个没有人可以忽视的功能。"

那次谈话改变了一切。我们公司的使命转变了。我们不再只是构建一个易于使用的电子商务平台。我们正在追求构建地球上最快的电子商务体验。要做到这一点，我首先必须理解我们新敌人的基本物理学：**延迟**。

#### **技术深入：互联网的物理学**

为什么托管在美国的Shopify商店在印度很慢？答案与服务器能力关系不大，更多与物理定律有关。

**概念：延迟是真正的瓶颈**

当你测量一个网站加载需要多长时间时，你实际上在测量两件事：

- **处理时间：** 服务器思考、查询数据库和构建页面所需的时间。
- **网络时间：** 你的请求到达服务器，以及服务器的响应返回给你所需的时间。

到目前为止，我们一直在优化处理时间。但网络时间受一个硬性、不可打破的物理限制的制约：**光速**。

互联网上的数据通过光纤电缆以大约光速的三分之二传播。从印度孟买到美国弗吉尼亚州数据中心（一个主要的托管中心）的距离约为13,000公里。单个数据包的往返行程为26,000公里。

让我们计算一下：

- 光纤中的光速：~200,000公里/秒
- 往返距离：26,000公里
- 最小理论时间：26,000公里 / 200,000公里/秒 = 0.13秒或**130毫秒(ms)**。

这就是**往返时间(RTT)**。130毫秒是印度的浏览器和美国的服务器之间单个对话可能达到的绝对最快速度。

但加载一个安全的网页不是一个对话；它是一系列必须按顺序进行的对话：

- **DNS查询：** "这个域名的IP地址是什么？"（1次往返）
- **TCP握手：** "你好服务器，我想开始一个连接。"（1次往返）
- **TLS握手：** "让我们建立一个安全、加密的连接。"（2次往返）
- **HTTP请求：** "好的，现在请发送给我实际的网页。"（1次往返）

仅仅为了获得HTML页面的第一个字节，印度的用户就必须等待至少5次往返。

> 5次往返 * 130毫秒/次 = 650毫秒

这意味着托管在美国的Shopify商店对于印度用户来说，在屏幕上显示任何内容之前，至少有650毫秒的延迟。这还不包括网络拥塞和下载所有图像、CSS和JavaScript文件所需的时间。3-4秒的加载时间是不可避免的。这是物理学强加的一种税收。

我们无法让光速更快。我们无法将用户移动到更接近Shopify服务器的位置。因此，我们决定做唯一可能的其他事情。

我们将把服务器移动到更接近用户的位置。在任何地方。同时。我们即将进入**边缘计算**的世界。

### 第二部分：证据和确凿证据

我们有一个理论，基于物理定律，认为我们最大的竞争对手有一个关键弱点。但理论是不够的。为了证明大规模工程努力和公司范围内的战略转变是合理的，我们需要冷酷、确凿的数据。我们需要证明它。

因此，我们走向了公开。在X（前身为Twitter）的一系列帖子中，我向全世界阐述了我们的论点。我做出了一个大胆、直接的声明：**"Shopify存在严重的性能问题。"**

**证据：印度顶级品牌**

我们不仅仅是提出主张；我们还展示了我们的工作。我们选取了四个在印度最受尊敬的顶级D2C品牌，它们都在Shopify上运营，并对它们进行了测试：**boAt、Bummer、Urbanmonkey和Hammer**。

我们使用印度测试地点的标准性能分析工具对它们的主页进行了测试。结果清晰且一致：加载时间慢、性能评分低、用户体验迟钝。

我们的公开活动开始引起关注，甚至引起了Shopify自己的性能工程总监Colin Bendell的回应。他驳回了我们的发现，将缓慢归因于一个常见的替罪羊："**现在不应该这样，最常见的问题是第三方内容**"（第三方内容）。

他暗示问题不在于Shopify的平台，而是卖家自己的应用、分析脚本和营销标签。这是一个看似合理的解释，并将责任推回给了商家。但我们知道他错了，我们决心证明这一点。

**确凿证据：加拿大vs印度测试**

为了反驳"第三方内容"理论，我们设计了一个简单但具有毁灭性的实验。我们选取了相同的商店——boAt和Bummer——并再次运行性能测试。但这次，我们为每家商店并排运行了两次测试，只改变了一个变量：

- **测试A：** 位置设置为**印度**。
- **测试B：** 位置设置为**加拿大**，Shopify的主场。

结果截然不同。正如我在推文中所展示的，Bummer在加拿大的商店不再令人失望——它很快。boAt商店也飞快。第三方内容——脚本、应用、小部件——在两次测试中完全相同。唯一改变的是用户的物理位置。

这就是我们的确凿证据。问题不在于卖家的应用。问题在于Shopify的核心基础设施。它在加拿大很快，在印度很慢。

<br/>
#### **技术深入：TTFB是确凿证据**

揭露真相的关键指标是**TTFB**，即**首字节时间**。

TTFB是对服务器响应性和用户与该服务器之间网络延迟的纯粹测量。它是从您的浏览器请求网页到它收到HTML文档的第一个字节的时间。这个测量发生在您的浏览器开始下载大型图像或执行任何繁重的第三方JavaScript之前。

我们的测试揭示了我推文中令人震惊的真相：

- boAt在印度的TTFB比在加拿大**高7倍**。
- Bummer在印度的TTFB比在加拿大**高3倍**。

正如我公开演示的，如果您在印度打开boAt网站并查看浏览器开发者工具中的"网络"选项卡，您会看到主文档请求花费了超过**1.06秒**。一秒钟的等待，只为服务器发送回一个98KB文本文件的第一个小片段！

这就是"光速税"在起作用。那一秒钟的延迟是跨越全球旅程的不可否认的证明。它证明了Shopify的核心架构集中在北美。他们没有"正确"的技术来以用户应得的性能为全球用户提供服务。

#### **概念：什么是边缘计算？**

我们发现了歌利亚（Goliath，《圣经》中的巨人）盔甲上的裂缝。问题是距离。因此，解决方案必须是消除那个距离。

你无法超越光速。但你可以作弊。你可以缩短比赛。

这就是**边缘计算**的核心理念。

边缘计算是一种架构方法，它不是在集中式云（如北美单个数据中心）中处理数据，而是将计算和数据存储尽可能靠近用户的物理位置。

不是在一个国家建立一个巨大的中央大脑，而是在世界各地几十个城市建立一个由较小的、同步的"迷你大脑"（或**边缘节点**）组成的网络。当印度的用户尝试访问您的服务时，他们不会与美国的主大脑对话。他们会与孟买或德里的本地迷你大脑对话。

Shopify的架构是集中式的。我们的将是分布式的。他们迫使用户前往服务器。

我们将把服务器带到用户身边。这就是Dukaan全球边缘网络背后的基本理念。

太好了。这是故事的关键部分。与Shopify的公开对决和我们边缘网络的成功不仅验证了我们的工作——它还吸引了全新的关注，导致了一个新的、更大的扩展挑战。

这是关于我们如何引入第一个企业巨头的故事，以及它如何迫使我们再次发展我们的数据库架构。这个新篇章将完美地契合我们对边缘网络的深入探讨。

## 第16章：巨鲸：引入企业巨头

我们的公开性能测试和"鲨鱼坦克效应"的故事开始在印度D2C社区引起轰动。我们的信息清晰且有数据支持：Dukaan比Shopify更快。这个大胆的声明像磁铁一样，不仅吸引了我们开始时的小卖家，还吸引了更大的鱼。

一天，Suumit接到了一个让我们所有人都停下来的电话。电话来自**Wow Skin Science**的领导团队，这是印度最大、最成功的D2C品牌之一。他们是一家年收入100亿卢比的巨头。他们看到了我们的声明，很感兴趣，但深表怀疑。他们的挑战很简单："证明它。"

这是一个新篇章的开始，一个将迫使我们超越一刀切的平台，构建真正企业级解决方案的篇章。

### 第一部分：挑战和"哦，糟糕"时刻

Wow Skin Science的团队很精明。他们同意进行为期一周的试用。他们不会迁移整个商店，但会针对托管在Dukaan上的登陆页面运行小型、有针对性的活动。他们想看看我们的边缘网络如何处理他们自己的真实世界流量。

在那一周里，我们整个团队都处于高度戒备状态。我们看着仪表板，随着他们的活动流量进入。我们的平台表现完美。页面瞬间加载，用户体验流畅，我们的性能声明通过他们自己的数据得到了验证。他们印象深刻。

一周后，他们回电了。

"我们确信了，"他们的技术主管说。"性能令人难以置信。我们想将我们的整个运营——我们的完整目录、数百万客户、整个100亿卢比/年的业务——迁移到Dukaan。"

我们团队Slack中的最初反应是纯粹的狂喜。这是对我们所构建的一切的最终验证。获得如此规模的客户将在一夜之间改变我们公司的轨迹。

然后，狂喜被一种安静的、集体的"哦，糟糕..."的恐惧时刻所取代。

Wow Skin Science的规模与我们曾经处理过的任何事物都不同。他们的订单量预计将比我们其他500万家商店的当前总订单量**大20倍**。他们的一次新产品发布就会产生比Jain Shikanji"鲨鱼坦克"峰值更多的流量。

我们盯着一条巨鲸，我们必须想办法把它弄进我们的船里，而不会沉没整个舰队。

#### **识别问题："嘈杂邻居"**

我们的全球边缘网络旨在处理大量的_读取_流量。但_写入_流量——新订单、客户注册、库存更新——最终都必须由我们在孟买的单一中央主PostgreSQL数据库处理。

到目前为止，这没有问题。我们数百万的小卖家创造了可预测的、分布式的负载。但Wow Skin Science不同。它们是一个单一实体，可以在很短的时间内产生大量的写入流量。

这就造成了经典的**"嘈杂邻居"问题**。

- **类比：** 我们已经建立了一个庞大、高效的公共高速公路系统（我们的平台）。现在，一支由一千辆巨型怪物卡车组成的车队（Wow Skin Science）想要使用我们的高速公路。如果他们在闪购期间同时上路，他们可能会造成巨大的交通堵塞，使所有正常的汽车（我们数百万的小卖家）完全停滞不前。

相反，我们数百万小商店中的一个病毒式爆发理论上可能导致微小的减速，这将影响Wow的关键运营。对于他们这样规模的品牌，这种共享风险是不可接受的。我们不能把巨鲸放在和其他人一样的鱼缸里。我们必须为他们建造自己的、私人的海洋。这导致了我们第一次真正实施**数据库分片**。

### 第二部分：企业堡垒

架构解决方案很明确：我们必须完全隔离他们。我们不能只给他们更强大的服务器；我们必须给他们自己的、专用的基础设施，从数据库开始。

#### **技术深入：什么是数据库分片？**

我们已经使用副本扩展了数据库的_读取_能力。但分片是一种扩展数据库**写入**能力的技术。

- **类比：** 我们当前的数据库是整个国家的单一、巨大的中央图书馆。它变得太繁忙，无法同时处理公众请求和来自一家大公司的大量专业化请求。**分片**是决定建立一个完全独立的、新的国家图书馆，专门用于"企业和大型企业"客户，而原始图书馆继续为普通公众服务。它们是两个独立的系统，拥有自己的员工和资源。

从技术上讲，分片是一种水平分区类型。您不再只有一个主数据库，而是有多个主数据库，应用程序中的路由层决定在任何给定请求中与哪个数据库通信。

#### **我们的实现：每个企业一个分片**

我们设计了一个简单但功能强大的分片策略。

- **公共分片：** 这是我们现有的、强大的主数据库集群。它将继续为我们数百万现有的和未来的中小型卖家提供服务。
- **企业分片：** 我们配置了一个全新的、完全独立的主数据库集群，配备了自己的专用、顶级裸机硬件。这个分片创建的目的只有一个：为Wow Skin Science提供服务。

这需要对我们应用程序的核心逻辑进行重大更改。我们必须在代码中构建一个"分片路由器"。在我们的应用程序执行任何数据库查询之前，它会首先执行一个简单的检查：

- 这个请求是针对哪个store_id的？
- 这个store_id是否在我们的企业客户列表中？
- 如果**是**，打开到**企业分片数据库**的连接。
- 如果**否**，打开到**公共分片数据库**的连接。

这个逻辑在我们整个代码库中实现，确保从请求第一次命中我们的应用程序的那一刻起，我们企业客户和公共卖家的数据和工作负载就会完全隔离。

#### **堡垒的好处**

这种架构完美地解决了"嘈杂邻居"问题。

- **性能隔离：** Wow Skin Science商店上的大规模闪购会冲击他们专用的企业数据库分片，但我们在公共分片上的数百万其他卖家将感觉不到任何影响。他们的怪物卡车现在有了自己的私人高速公路。相反，如果一个小卖家的产品走红，公共分片上的流量激增不会对Wow的运营造成哪怕一毫秒的延迟。
- **增强的安全性和控制力：** 对于大型企业客户来说，这种数据隔离是一个巨大的特性。他们的客户和订单数据不仅仅是逻辑上分离的；它们在物理上被分离到不同的机器上，提供了更高级别的安全性，更容易满足合规标准。
- **面向未来的可扩展模型：** 这种架构为我们提供了一个可重复的未来剧本。当下一个企业巨鲸出现时，我们不必担心公共数据库的容量。我们可以简单地为他们配置一个新的、专用的"分片3"。这一决定是我们新的"Dukaan企业"产品的基础，使我们能够服务两个截然不同的细分市场，而不会损害任何一方的体验。

## 第15章：我们的全球大脑：设计Dukaan边缘网络

我们向延迟宣战。我们的使命是构建一个电子商务平台，不仅快速，而且对地球上的每个用户都_即时_快速。我们知道理论是可靠的：我们必须将服务器移得更靠近用户。

但这不再是为图像添加简单的CDN。这是关于将我们的整个店面应用程序——计算、逻辑和数据库本身——移动到边缘。这是我们承担过的最雄心勃勃、最复杂、最大胆的工程项目。这是我们的登月计划。

### 第一部分：白板和魔法IP地址

这个项目开始于我和两位高级工程师站在一个巨大的白板前。我们试图回答一个看似简单的问题："全球电子商务平台的完美、最快可能的架构会是什么样子？"

我们勾勒出的答案是激进的。我们将在全球多个AWS区域部署我们整个店面应用程序及其数据库的独立副本：孟买服务南亚，法兰克福服务欧洲，新加坡服务东南亚，俄亥俄服务北美，等等。

当我把这个计划拿给Suumit看时，他立即看到了商业影响。

"这太神奇了，"他说，"但等等。_数据库_的副本？在每个区域？保持同步不会完全是一场噩梦吗？在世界各地运行这么多服务器...不会花费我们一大笔钱吗？"

他触及了两个核心挑战：**数据一致性**和**成本**。这个计划雄心勃勃，但充满了技术风险。我们下了一个巨大的赌注，认为性能提升将是如此巨大，以至于它们将证明复杂性是合理的。我确信它们会。

在我们考虑数据之前，我们必须解决第一个主要问题：如何将用户引导到正确的服务器？

#### **技术深入：速度的架构**

如果我们在孟买和法兰克福各有一台服务器，德国的用户浏览器如何知道要连接到法兰克福服务器？我们不能要求卖家告诉他们的德国客户访问不同的URL。我们需要一个单一的、全球的地址，可以智能地将用户路由到最近的位置。使这成为可能的魔法被称为**任播IP**。

**概念：任播IP - 全球披萨连锁店**

通常，互联网上的IP地址是**单播**的。这意味着一个IP地址指向一个特定位置的特定机器。

**任播**是不同的。通过任播，我们可以同时从世界各地的多个数据中心宣布**完全相同的IP地址**。

- **类比：** 想想像达美乐这样的全球披萨连锁店。他们可以有一个单一的、全球的电话号码：**1-800-DOMINOS**。
  - 当您在孟买从手机拨打该号码时，全球电话网络很智能。它不会将您连接到美国的呼叫中心。它查看您的位置，并自动将您的呼叫路由到最近的呼叫中心，就在孟买。
  - 当法兰克福的客户拨打_完全相同的号码_时，网络会自动将他们路由到法兰克福呼叫中心。
  - 用户不需要知道呼叫中心在哪里。他们只需拨打一个简单、易记的号码，网络就会完成找到最近的呼叫中心的繁重工作。

任播IP就是我们应用程序的全球电话号码。我们获取了自己的IP地址空间（我们拥有的一组IP地址）。然后，我们配置网络提供商从我们选择的每个AWS区域宣布这个IP。

这完全改变了游戏规则。

- 卖家只需将他们的自定义域名（mycoolstore.com）指向我们的单个任播IP地址一次。
- 当德国的客户访问mycoolstore.com时，互联网的路由协议（BGP）自动将他们的流量发送到我们的法兰克福数据中心。
- 当印度的客户访问完全相同的域名时，他们的流量会自动路由到我们的孟买数据中心。

这是无缝的、自动的，并且无限可扩展。任播是我们的全球前门。现在我们必须确保在那扇门后面，在每个区域，都有一个功能齐全、闪电般快速的店面副本准备好为用户提供服务。

### 第二部分：区域大脑

任播是我们的魔毯，毫不费力地将来自世界各地的用户送到我们最近的数据中心的门口。但用户到达一座空建筑是没有用的。我们必须确保在法兰克福、新加坡和俄亥俄的那扇门后面，有一个完整的、自给自足的、闪电般快速的店面平台副本，准备好立即为他们的请求提供服务。

这意味着在每个单一区域部署两个关键组件：计算和数据。

#### **技术深入：区域架构**

我们选择了全球九个战略性的AWS区域作为我们新的"区域大脑"的所在地。在这九个地点中的每一个，我们都建立了标准化、相同且完全独立的设置。

**实现：独立的Kubernetes集群**

在每个区域，我们部署了一个小型、自包含的Kubernetes集群。

- **类比：** 每个特许经营餐厅都有自己本地的、完美同步的只读食谱副本。法兰克福的厨师可以立即查找他们需要的任何食谱，而无需长途拨打孟买总部的电话。
- **好处（亚毫秒级延迟）：** 这是我们实现亚100毫秒页面加载目标的秘诀。当用户请求到达我们法兰克福Kubernetes集群中的Pod时，该Pod会查询其本地PostgreSQL副本——一台位于同一数据中心仅几米之遥的服务器。整个数据库事务以亚毫秒级延迟完成。我们完全消除了损害Shopify性能的跨大陆网络跳跃。

我们成功设计了区域大脑。每个区域大脑都是一个自给自足的单元，能够以惊人的速度为当地用户提供服务。

但这带来了一个全新的、巨大且令人恐惧的问题。

#### **新问题：全球数据同步**

我们现在有九个完美的只读数据库副本散布在全球各地。但是，"主食谱"——卖家进行更改的单一真相来源——仍然只存在于一个地方：我们在孟买的主数据库。

当印度的卖家更新产品价格时，该更改会写入孟买主数据库。那么，我们如何将这个单一更改可靠、按正确顺序且近实时地传播到法兰克福、俄亥俄、新加坡、巴西和悉尼的其他九个数据库副本呢？

我们之前使用的简单流复制并非为这种复杂的一对多、跨大陆数据同步而设计。简单的网络故障可能导致一个区域失去同步，显示过时数据数小时。

我们已经构建了网络的分布式大脑。现在，我们必须构建连接它们的全球中枢神经系统，确保它们以完美和谐的方式思考。

.

### 第三部分：全球中枢神经系统

我们已经构建了九个强大的独立区域大脑。但一组断开连接的大脑是无用的。我们需要一种方式来链接它们，确保在孟买做出的更改能立即被法兰克福、俄亥俄和新加坡知晓并反映出来。我们需要一个全球中枢神经系统。

我们为这项艰巨任务选择的工具是我们已经熟悉的，但我们即将在全球范围内应用它：**Apache Kafka**。

#### **技术深入：使用Kafka进行全球数据同步**

我们使用Kafka和我们的变更数据捕获工具Debezium创建了一个实时、持久且可靠的管道，用于在全球范围内同步数据。

**架构：中央新闻专线**

- **中心生产者：** 在孟买的主数据中心，我们将**Debezium**直接连接到我们的主PostgreSQL数据库。正如我们在第9章所学，Debezium监视数据库的事务日志。对于每一个变更——每个新产品、每个价格更新、每个新商店——Debezium都会立即自动生成一个结构化消息。这些消息被发布到同样位于孟买的高可用性中央Kafka集群。这个Kafka集群成为Dukaan平台上发生的**所有变更的全球真相来源**。
- **边缘消费者：** 在我们从法兰克福到圣保罗的九个区域位置中，我们部署了小型专用消费者服务。该服务唯一的工作是订阅孟买的中央Kafka集群并监听新消息。

**数据流：推送模型**

整个系统就像一个全球新闻专线服务（如路透社或美联社）。

- **类比：** 孟买的Debezium/Kafka设置是我们的总部新闻编辑室。一旦新闻事件发生（卖家更新价格），编辑室立即将文章发布到全球专线。我们的九个区域"餐厅"都是这个专线服务的订阅者。每个餐厅在后台办公室都有一台电传机。一旦文章发布，它就会几乎实时地在所有九个地点打印出来。当地经理（我们的消费者服务）阅读文章并更新本地的食谱副本（区域数据库）。

整个流程非常优雅：

- 卖家更新产品价格。更改写入孟买的主Postgres数据库。
- Debezium在数据库的事务日志中看到这一变更。
- Debezium向我们中央Kafka集群中的product_updates_global主题生成消息。
- 我们边缘位置的九个消费者服务全部订阅了此主题，在几毫秒内收到此消息。
- 每个消费者随后在自己的本地PostgreSQL只读副本上执行UPDATE命令。

在一两秒钟内，在印度做出的更改准确地反映在我们的整个全球网络中。

**好处：持久性和保证交付** 这就是为什么Kafka比简单的数据库复制更优越。Kafka的日志是**持久的**。如果与法兰克福区域的网络连接中断五分钟，法兰克福消费者会简单地落后。为它准备的消息会安全地堆积在孟买的Kafka日志中。当网络恢复时，消费者会重新连接并从上次停止的地方开始读取，快速处理所有错过的更新。这确保了每个区域最终都会变得一致，并且在传输过程中不会丢失任何数据。

#### **鲨鱼坦克神秘事件：解决**

这种全球分布式架构是在鲨鱼坦克效应期间拯救我们的秘密武器。80,000并发用户并非全部击中我们位于孟买的中央服务器。多亏了我们的任播IP，流量被自动分配到我们的全球覆盖范围。来自北美的用户击中我们的俄亥俄集群，欧洲用户击中我们的法兰克福集群，依此类推。

巨大的负载由九个独立的小型系统共享。没有单个集群被压垮。这是一种分布式防御，抵抗集中式流量激增。我们对全球性能的痴迷意外地创造了一个具有令人难以置信的可扩展性和弹性的系统。

我们向延迟宣战。我们的使命是构建一个电子商务平台，不仅要快速，而且要对地球上的每个用户都**即时**快速。本章是我们的登月计划的详细蓝图：Dukaan全球边缘网络。这是我们如何超越Web架构的传统智慧，构建出在性能、安全性和规模方面给我们带来几乎不公平优势的系统的故事。

### **玛吉配送网络问题**

要理解我们即将迈出的飞跃，你首先需要了解传统**内容分发网络（CDN）**的局限性。正如我们在第11章中讨论的，CDN是一个极好的工具。它是一个全球服务器网络，缓存您的静态文件——您的图像、CSS和JavaScript——靠近您的用户。这是重要的第一步。

但传统CDN就像玛吉配送网络。

想象一下，您位于孟买的主服务器是**玛吉工厂**。它生产您产品的美味、复杂且动态的核心——HTML文档本身，由您的应用程序和数据库构建。图像和其他静态文件是简单、不变的**香料包**。

CDN是一个由配送员组成的网络，他们只有一项工作：将一大盒香料包存放在全国各地的本地仓库中。当德里的客户想要玛吉时，当地配送员可以立即从德里仓库给他们香料包。这非常快。

但问题在于：配送员没有面条。要获得实际的面条（HTML页面），他仍然必须将请求发送回孟买的主工厂。工厂必须处理订单，烹饪新鲜的面条，然后将它们一路运到德里。

这是传统CDN的基本局限性。它可以快速传递网站的简单静态部分，但对于最重要的部分——由服务器生成的动态HTML——用户仍然必须忍受漫长、缓慢的跨大陆旅程，回到您的源服务器。这正是Shopify在印度的TTFB如此高的原因。他们的CDN可以快速传递图像，但对页面本身的初始请求必须一路传输到北美。

我们意识到，要真正快速，我们不能只从边缘交付香料包。我们必须在每个主要城市建立一个**功能齐全的玛吉工厂**。我们需要将整个烹饪过程——应用服务器和数据库——移到边缘。这是真正的边缘计算平台的核心理念。


### **边缘的三大支柱**

我们的全球边缘网络建立在三个基础支柱上。这些不仅仅是功能；它们是定义我们平台并赋予我们竞争优势的战略优势。

#### **支柱一：性能——100毫秒承诺**

我们对Shopify的战争是对延迟的战争，边缘网络是我们的终极武器。整个架构是一个组件交响曲，旨在粉碎TTFB并提供近乎即时的页面加载。

- **计算和数据共存：** 最关键的决定是在我们的九个AWS区域中的每一个都部署我们的**Kubernetes集群（计算）**和**PostgreSQL只读副本（数据）**。这是我们与传统CDN的区别所在。当用户请求到达我们的法兰克福边缘节点时，接收请求的应用容器可以查询位于同一数据中心仅几米之遥的数据库。数据的往返时间以微秒为单位，而不是跨大西洋旅行的150+毫秒。我们有效地消除了网络作为应用程序逻辑的瓶颈。
- **任播IP实现即时路由：** 我们的**任播IP**是将整个网络缝合在一起的魔法。通过从所有九个区域宣布相同的IP地址，我们让互联网自己的路由基础设施完成艰苦的工作。为互联网提供动力的边界网关协议（BGP）自动找到从用户到我们边缘节点的最短网络路径。这个过程非常快，甚至在建立连接之前就已经完成。用户被发送到最快的可能位置，而无需他们、他们的浏览器或我们的应用程序进行任何思考。我们甚至采取了额外的步骤，**拥有自己的IP空间**。这是一个关键的前瞻性决定。这意味着我们的任播IP属于Dukaan，而不是AWS。如果我们决定将某个边缘位置从AWS迁移到Google Cloud或裸机提供商，我们可以零停机时间完成。我们卖家的自定义域名仍将指向相同的IP；我们只需从新位置宣布它。我们不受单一云提供商的锁定。
- **Kafka推送模型：** 性能难题的最后一块是我们**基于Kafka的数据同步管道**。它采用**推送模型**工作。孟买卖家保存更改的那一刻，我们的中央Kafka集群就会**同时将**该更新**推**送到所有九个边缘位置。这种主动方法意味着数据几乎总是预先预热并在边缘等待，甚至在用户请求之前。我们不是等待缓存过期；我们正在积极确保边缘始终保持最新状态。

这些技术的结合使我们实现了登月计划的目标。从用户点击链接到浏览器开始渲染页面（TTFB）的延迟始终低于**50毫秒**。完整页面，加上从同一区域边缘提供的图像，将在**100毫秒**内加载完毕。当我们的竞争对手需要几秒钟才能跨大陆提供他们的网站时，我们可以在眨眼间提供我们的网站。这是一个绝对无可否认的性能胜利。

#### **支柱二：安全与稳定性——全球免疫系统**

当您构建一个集中式系统时，您创建了一个单一的巨大目标。该位置的故障，无论是由于技术故障还是恶意攻击，都会导致整个平台瘫痪。分布式边缘网络从根本上改变了这种动态，创建了一个更具弹性、自我修复的系统。

- **DDoS攻击缓解：** 分布式拒绝服务（DDoS）攻击的工作原理是用大量流量淹没目标，使其不堪重负并无法访问。在传统架构中，对dukaan.app的攻击将所有流量都导向我们位于孟买的中央服务器，迅速使它们不堪重负。
    有了我们的边缘网络，游戏规则改变了。攻击流量也通过任播路由。源自欧洲僵尸网络的攻击将完全被我们的**法兰克福边缘位置**吸收。来自北美的攻击将击中我们的**俄亥俄边缘位置**。攻击的力量自然分布在我们的全球覆盖范围内。恶意流量在边缘被防火墙和抵御，而我们在亚洲、南美和世界其他地区的用户体验零性能下降。攻击面被分散得如此之薄，以至于要瘫痪整个平台变得极其困难。
- **故障隔离和正常运行时间：** 这一原则不仅适用于恶意攻击。云计算的世界并不完美；数据中心会发生故障。在集中式模型中，如果us-east-1 AWS区域发生重大故障（过去确实发生过），任何仅托管在那里的公司都会完全瘫痪。
    我们的架构创建了一系列水密隔舱。如果我们整个孟买集群离线，我们的任播网络将自动检测到这一点。它会从该位置撤回路由公告，并在几秒钟内开始将印度用户重新路由到下一个最近的健康位置，可能是新加坡。用户体验可能会有稍高的延迟（例如，60毫秒而不是20毫秒），但网站将**保持在线**。这种自动绕过区域故障的能力为我们提供了集中式系统根本无法实现的稳定性和正常运行时间水平。

#### **支柱三：扩展性——弹性海岸线**

鲨鱼坦克事件是我们网络令人难以置信的可扩展性的最终证明。秘诀不仅仅是负载被分配，而是系统具有多重级联的弹性层。

让我们详细了解大规模流量激增期间的确切事件顺序：

- **第一层：全球分布（海岸线）：** 初始80,000用户浪潮击中我们的任播IP。互联网的路由协议作为我们的第一道防线，将这一浪潮分散到我们的九个全球"海滩"。我们的俄亥俄节点处理美国流量，法兰克福处理欧洲流量，依此类推。这立即将巨大的负载分割成更小、更易于管理的区域峰值。
- **第二层：智能重新路由（溢流渠）：** 假设美国部分的流量如此激烈，以至于我们的俄亥俄集群开始接近其容量限制。我们的智能网络监控检测到这一点。在服务器不堪重负之前，我们可以自动指示网络将一部分新传入的美国流量重新路由到**下一个最近的健康位置**，可能是我们在弗吉尼亚州甚至多伦多的集群。这是我们的溢流阀。它优雅地降低一小部分用户的性能（增加约20-30毫秒的延迟），以维持所有人100%的正常运行时间。这是一个设计用于弯曲而不是断裂的系统。
- **第三层：本地自动扩展（扩展海滩）：** 当所有这些在全球网络层面发生时，在俄亥俄集群内部，我们的**水平Pod自动缩放器（HPA）**正在努力工作。它看到本地Pod的CPU飙升，并开始其反应式扩展过程，添加新的Pod来处理增加的本地负载。这个过程较慢，需要几分钟，但它不再需要即时响应。全局重新路由已经为它争取了宝贵的时间来扩展"海滩"。一旦本地集群通过更多Pod稳定下来，全局网络就可以减轻重新路由，并将完整的美国流量负载发送回新增强的俄亥俄区域。

这种多层系统——结合主动全球分布和反应式本地扩展——是我们的杰作。它允许我们吸收可以想象到的最极端、不可预测的流量峰值，不仅能够承受它们，而且无需待命工程师收到任何警报就能做到这一点。这是我们漫长而复杂的边缘之旅的最终验证。

## 第15章：要点总结

- **要实现全球低延迟，必须将计算和数据都移至边缘。** 如果区域应用服务器必须跨大陆调用中央数据库，那么它将毫无用处。
- **任播IP是使全球路由无缝的魔法。** 它允许您使用单个IP地址自动将用户引导到最近的物理数据中心。
- **分布式架构提供令人难以置信的容错性。** 通过在多个区域部署独立集群，一个区域的故障不会影响其他区域的用户，从而显著提高整体正常运行时间。
- **使用像Kafka这样的持久事件日志作为全球数据同步骨干。** 它提供了一种可靠且可扩展的方式，使数十个分布式数据库保持近乎实时同步。
- **有时，解决一个问题（性能）会为另一个问题（可扩展性）创造优雅的解决方案。** 我们对速度的追求导致了一个可以轻松处理大规模流量峰值的系统。

很好。架构已经构建。它已经受过火焰的考验。现在，我们进入旅程的最后阶段：掌握我们创建的强大、复杂且昂贵的机器。

### 第五部分：掌握机器（优化和自动化）

构建强大的引擎是一回事。学习如何调整它以获得最大性能、高效地为它加油并自动化其维护是另一回事。在故事的最后一部分，我们从架构师和构建者转变为精通操作的大师。

我们已经创建了一个全球性、弹性且闪电般快速的平台。但它很昂贵，有时仍然是一个黑盒子。接下来的章节讲述我们对效率、可见性和完全自动化的追求。这是关于我们如何将强大的机器转变为真正世界级、具有成本效益且自动驾驶的运营的故事。

## 第16章：聚光灯：从意外的CTO到技术领导者

多年来，我的世界是一个带有绿色文本的黑色屏幕。我的战斗是在沉默中进行的，对抗崩溃的服务器和失败的数据库。我们的成功是通过正常运行时间图表和下降的延迟数字来衡量的。我们建造了一台令人难以置信的机器，但我们是在黑暗中建造的。

但随着我们平台的成长，世界开始注意到。关于Dukaan令人难以置信的性能和我们创新的边缘网络的低语开始变得更大声。有一天，聚光灯找到了我。这是一种新的火的考验的故事——与服务器无关，而与面对我作为自学成才工程师的根深蒂固的恐惧有关。

### 第一部分：邀请和冒名顶替者

这封电子邮件在一个周二下午落在我的收件箱中。主题行很简单："邀请：与Arnav Gupta一起参加Scaler播客"。

我的心跳开始加速。

对于印度创业生态系统中的任何人来说，Arnav Gupta和Scaler播客都是传奇的。Arnav是一位硬核、从第一原理出发的工程师，一位备受尊敬的领导者，他不回避称时尚为时尚。他的播客是一个三小时、毫无保留的技术深度探讨。这是一个真正的书呆子狂欢节。

而嘉宾名单...嘉宾名单是传奇性的。之前的嘉宾是：

- **Amod Malviya：** Flipkart的传奇CTO和Udaan的联合创始人。
- **Jiten：** CARS24的CTO和前Hotstar副总裁。
- **Jacob Singh：** Grofers（现为Blinkit）的前CTO和Sequoia Capital的CTO。

这些不仅仅是CTO；他们是印度技术的开国元勋。我们都渴望建造的系统的建筑师。他们是IIT毕业生，拥有深厚计算机科学理论知识的杰出人才。

而第四位嘉宾应该是我。

我对你说实话。我的第一反应不是兴奋。这是纯粹的、冰冷的、令人作呕的恐惧。**我他妈的吓坏了。**

在我的整个职业生涯中一直作为安静伙伴的冒名顶替综合症现在在我耳边尖叫。我有什么资格坐在那个椅子上？我是比哈尔邦的商科孩子。那个负担不起CA考试学习费用的人。那个从盗版PDF学习PHP并通过在凌晨3点观看东西崩溃来理解系统设计的自学成才的程序员。

我对大学教你的概念一无所知。如果Arnav让我从教科书定义解释CAP定理或在白板上计算算法的大O符号，我会僵住。我会被揭露为骗子。

这不仅仅关于我。风险巨大。我将代表Dukaan。我将代表我的整个工程团队，一群将信任放在我身上的非常有才华的人。我犯的任何错误，我说的任何愚蠢的话，都会对他们所有人产生负面影响。它会损害我们在技术社区中的声誉，并抹去我们为建立品牌所做的所有艰苦工作。

我的第一反应是礼貌地拒绝。留在我舒适的阴影中。

但后来我想到了我们的旅程。我想到了我们令人难以置信的边缘网络，我们迁移到裸机的成本节约，我们复杂的可观察性平台。我们有一个故事要讲。一个真实的、在战壕中建立世界级东西的故事。如果我太害怕讲述它，谁会呢？

这不再关于我。这是关于我们。这是关于Dukaan。这是一种责任。

我的心跳到了嗓子眼，我回复了邮件："我很荣幸。"

### 第二部分：三个小时的书呆子谈话

播客前的日子充满了焦虑。我没有尝试死记硬背计算机科学教科书；我知道那将是徒劳的。相反，我决定依靠我唯一真正的优势：故事。我不会尝试成为房间里最聪明的学者；我会成为最真实的建设者。我收集了我们的架构图、鲨鱼坦克流量高峰的Grafana仪表板，以及我们裸机迁移的成本节约图表。我不会引用理论；我会从战壕中提供证据。

我走进工作室，心跳加速。设置很专业，灯光很明亮，Arnav Gupta用坚定的握手迎接我。他正如我想象的那样：敏锐、专注且技术深厚。然而，当我们开始交谈时，我的恐惧开始消退。

他不是来测验我算法的。他是一个建设者，他想和另一个建设者交谈。问题不是抽象的；它们是真实的。我们从一开始就开始，讲述了曾经为我们整个公司提供动力的**512MB DigitalOcean水滴**的故事，以及它在凌晨3点崩溃如何成为我在扩展方面的第一个真正教训。

接下来的对话不是采访；这是两个痴迷于解决难题的技术人员之间长达三小时的深度书呆子谈话。我忘记了相机和观众。我只是在和一位同行工程师交谈，分享这本书中的故事。

我所谓的弱点——我缺乏正规培训——成为了我的超能力。我不能依靠复杂的行话，所以我必须从第一原理解释事情，使用我自己理解它们时使用的同样简单的类比。

- 当Arnav询问我们如何处理无服务器边缘函数的冷启动时，我没有给出关于λ微积分的技术讲座。我使用了**玛吉面条例子**。"第一个点击新函数的客户必须等待两分钟水烧开，"我解释道。"但之后的每个客户都能立即得到他们的玛吉。"这个概念立即明白了。这是一种简单、相关的方式来解释复杂的无服务器问题。
- 我们深入探讨了我们向**E2E Networks等提供商的裸机迁移**，以及我们实现的令人疯狂的、几乎令人难以置信的成本节约。
- 我们讨论了我们如何在第一个请求时**即时为卖家的自定义域颁发SSL证书**的魔法，这是我们团队非常自豪的边缘工程作品。
- 谈话甚至转向了团队建设的哲学和面试中**DSA（数据结构和算法）**的大辩论。我分享了我诚实、务实的观点：我可以Google排序算法，但我不能Google在凌晨3点修复生产数据库崩溃的勇气。我们重视通过实践来构建和学习，这一理念引起了深刻共鸣。

我只是在讲述我们的故事。当三个小时结束时，我感到精神疲惫，但有一种解脱感。我活了下来。我诚实地讲述了我们的故事。现在，轮到世界来评判了。

#### **后果：机场时刻**

播客一周后上线。我很紧张，但印度技术社区的反应是即时且压倒性积极的。Twitter和LinkedIn上的评论不是关于我缺乏学位；它们是关于我们解决的实际问题。工程师们对我们在构建系统时采取的实用、务实的方法感到兴奋。

最超现实的时刻出现在几周后。我在机场等待登机。一个年轻的小伙子有点犹豫地走向我。

"打扰一下，先生，"他说。"您是Subhash Choudhary吗？"

我点点头，很惊讶。

"先生，我看了您的Scaler播客，"他说，脸上露出喜色。"太棒了。您解释边缘网络的方式令人难以置信。谢谢。"

我惊呆了。这是我第一次在公共场合被认出，不是作为创始人，而是作为工程师。这个来自比哈尔邦、在第一份工作中感觉自己像个冒名顶替者的男孩，现在在机场因为他的技术见解而受到感谢。这是一个非常情感化和验证性的时刻。几周来困扰我的恐惧消失了，取而代之的是一种安静的归属感。

### 第三部分：灯塔与回响

"机场时刻"的个人认可令人难以置信，但那次三小时谈话的真正持久影响不在于我的自我；而在于我们的公司，并以我们从未预料到的方式回馈给我们。

在竞争激烈的科技创业世界中，有一种货币比风险投资更有价值：**工程人才**。在播客之前，我们只是众多试图说服聪明人加入我们使命的创业公司之一。

播客之后，一切都变了。我们点亮了一座灯塔。

#### **人才洪流**

我们的招聘页面，过去只是稳定地收到少量申请者，现在变成了洪流。我自己的LinkedIn收件箱里充斥着来自印度一些最好的产品公司的高级工程师的消息。但变化在于对话的**质量**和**意图**。

播客充当了一个三小时的深度广告，宣传我们正在解决的那种困难、有趣的问题。顶尖工程师不仅仅是在寻找工作；他们在寻找挑战。正如视频下的一条评论所说，播客是一个**"非正式的系统设计课程"**。我们不再只是一家拥有出色产品的创业公司；社区在说**"从技术角度看，这家公司看起来很棒"**。

我们的招聘过程发生了转变。播客成为我们最终的过滤器和磁铁。最终的赞美，也是如此多人联系我们的原因，被一条简单的评论概括：**"谁不想和这样的CTO一起工作。"**

#### **"街头智慧"CTO的验证**

对我个人而言，最有力的反馈是对我非常规旅程的验证。我多年来一直携带的冒名顶替综合症，作为"商科孩子"在满是计算机科学毕业生的房间里的恐惧，随着我阅读评论开始消散。

人们不关心我的学位。他们关心的是我们解决的实际问题。他们称我为**"原始、知识渊博、街头智慧的CTO"**和**"真正意义上的工程师"**。一条深深打动我的评论是：**"对这个人印象深刻！他如何从非技术背景转向技术背景，掌握所有细节！"**

另一条评论让一切都变得清晰：**"英语对我和许多人来说都是问题，我想。我很惊讶地看到这里，但他取得了这么多成就，给了我信心。"**这是一个谦卑的提醒，我的故事不仅仅是我自己的；它是无数在这个行业感到自己是局外人的人的灵感源泉。

#### **这本书的诞生**

播客很长——三个小时。我担心没有人会看。但不断出现的评论是这样的版本：**"起初我想，谁会看这么长的视频，但当我看的时候，不知不觉时间就过去了。"**我们创造了一些吸引人的东西，一些人们感到是"愉快的对话"的东西。

但有一条评论，特别是，播下了一颗种子，最终成长为你手中拿着的这本书。

一位名叫Haridarshan Choudhary的观众写道：**"...可以从整个采访中涵盖的主题创建一本系统设计书。"**

当我读到这句话时，一个灯泡亮了起来。他是对的。社区不仅仅是被娱乐；他们在学习。他们看到了从战壕中讲述的故事的价值。那条评论，以及几十条类似的评论，是我需要的最后推动力。这是一种验证，这个故事——一个普通人通过试错学习的故事——是一个值得完整讲述的故事。

在许多方面，那个播客是这本书的初稿。它证明了诚实而真实地讲述你的故事是你能做的最有力的事情——对你的公司、你的社区，以及你自己。

## 第16章：要点总结

- **公司的工程故事是你最强大的招聘工具之一。** 顶尖人才被困难问题吸引，而不仅仅是高薪。
- **公开分享你的工作。** 撰写博客文章、在meetup上发言或出现在播客上可以将你的公司确立为技术品牌，使其成为吸引最优秀工程师的磁铁。
- **真实性比证书更强大。** 能够以简单、以故事为驱动的方式解释复杂概念，并以实际经验为后盾，远比学术知识更能引起共鸣。
- **走出你的舒适区。** 最大的成长机会——无论是对你还是你的公司——往往位于你自己的恐惧和冒名顶替综合症的另一边。

## 第17章：逃离金笼：从AWS到裸机

我们做到了。我们在我们能找到的最好的基础上构建了我们的杰作：亚马逊网络服务。云给了我们用几次点击就能在世界各地部署服务器的能力，构建全球数据库网络，并扩展到令人难以置信的高度。我们生活在工程的黄金时代，由云看似无限的资源提供动力。

但我们很快发现，我们美丽的家实际上是一个**金笼**。它给了我们舒适、力量和速度，但它以惊人的价格到来，并将我们锁定在一个我们没有最终控制权的系统中。这是我们大胆逃脱的故事。

### 第一部分：8万美元的电话

电话在月底打来。是Suumit。他并不生气或惊慌。他的声音只是带着一种纯粹的、不加掩饰的震惊的平板音调。

"Subhash，"他说，"AWS账单刚刚到了。是八万美元。"

我停止了手中的工作。"八万？一个月？"

"一个月，"他确认道。"八万美国美元。"

我打开我们的AWS Cost Explorer仪表板。他是对的。我们月度支出的图表不是平缓的斜坡；而是火箭发射，反映了我们用户群的增长。我们的成功实际上让我们付出了巨大代价，我们的盈利能力正在蒸发到云端。

我们深入研究了各项费用。罪魁祸首无处不在。我们正在为以下内容付费：

- 数十个强大的EC2实例，作为我们九个全球区域的Kubernetes节点。
- 使用AWS托管Kubernetes服务（EKS）的溢价。
- 我们托管的PostgreSQL数据库（RDS）的巨额账单，包括强大的主实例和所有区域副本。
- 为同步我们全球网络的Kafka流量支付的高昂数据传输费用。

在我们堆栈的每一层，我们都在支付"便利税"。我们向AWS支付费用，以获得不必管理底层硬件的特权。在早期，这种税是值得的。它让我们能够以难以置信的速度前进。但现在，这种便利正威胁要让我们破产。我们被困在了金笼里。

#### **技术深入：基础设施的经济学**

那8万美元的账单迫使我们进行了一次长时间的、艰难的对话，关于我们基础设施的基本经济学。我们必须分析运行互联网公司的两种主要方式之间的权衡：在云端或在裸机上。

**概念：云端vs裸机的权衡**

**1\. 公共云（AWS、Google Cloud、Azure）**

- **类比：** 这就像在主要城市中心租用一套高端、全装修且有服务的公寓。
- **优点（"黄金"）：**
  - **速度与便利：** 你今天就可以搬进来。电力、管道和安全都为你处理好了。需要另一个房间？只需打个电话（API调用）。这就是为什么我们能够在几周而不是几年内构建我们的全球网络。
  - **弹性：** 你可以按需扩展或缩减。你按月支付你使用的费用。
- **缺点（"笼子"）：**
  - **大规模时极其昂贵：** 你为这种便利支付了巨额溢价。租金过高，因为它包括了房东的员工、他们的品牌、他们的营销，以及他们在建筑物原始成本之上的利润。
  - **缺乏控制权：** 你不能改变管道或拆除墙壁。你受到房东规则和他们提供的特定"电器型号"（服务器类型）的限制。你基本上是在租赁，而不是拥有。

**2\. 裸机（自托管）**

- **类比：** 这是购买一块土地并从头开始建造你自己的房子，使用像Hetzner、OVH或本地数据中心这样的提供商。
- **优点：**
  - **显著的成本节约：** 这是最引人注目的特点。你只为原材料付费：服务器本身、电力和网络连接。没有中间人的利润。成本可以比云中同等资源**便宜10到20倍**。那个8万美元的AWS账单可能变成5000美元的裸机账单。
  - **完全控制和性能：** 你拥有房子。你可以按照你想要的方式建造它。你可以选择专门的、高性能的硬件（例如，配备最快CPU和NVMe驱动器的服务器），这些通常比云提供商提供的更好、更便宜。
- **缺点：**
  - **高运营开销：** 这是巨大的权衡。你现在负责一切。你必须是建筑师、建筑工人、管道工和保安。如果硬盘在凌晨3点出现故障，没有AWS支持团队来修复它；你必须有自己的团队和流程来处理它。
  - **缺乏弹性：** 你是在购买，而不是租赁。这是资本支出。扩展意味着物理订购和架装新服务器，这可能需要几天或几周。缩减甚至更难。

在一开始，租用豪华的AWS公寓绝对是正确的决定。它给了我们所需的速度，以找到产品市场契合点并构建我们的全球网络。但现在我们的架构稳定了，我们的工作负载可预测了，租金简直太高了。

我们做出了大胆、可怕的决定：搬出去。是时候购买我们自己的土地，并在全球范围内建造我们自己的房子了。是时候将Dukaan迁移到裸机上了。

### 第二部分：大迁移

决定已经做出。我们正在离开AWS的金笼。但这带来了一个可怕的新挑战：如何在不中断一秒钟服务的情况下，将整个全球、实时生产系统从一个提供商迁移到另一个提供商？

你不能只是关闭旧系统并打开新系统。那将是计划中的停机，在我们的业务中，这是不可接受的。我们必须执行终极的高空走钢丝表演：在新的基础上重建我们的整个基础设施，并在表演仍在全面进行时无缝转移流量。

#### **我们的秘密武器：任播IP**

这次零停机迁移之所以成为可能，完全是因为我们几个月前做出的一个关键决定：**我们拥有自己的IP地址空间**。

因为我们的任播IP属于Dukaan而不是AWS，我们对它在互联网上从哪里宣告有最终控制权。我们不依赖AWS的网络。这个IP地址是我们在互联网上的永久、可移植地址。我们可以搬家，但我们的地址会保持不变。这是解锁我们在用户不知情的情况下进行迁移的关键。

#### **策略：全球范围内的绞杀者无花果**

我们决定应用我们用来拆分单体应用的相同**绞杀者无花果模式**，但这次是在全球基础设施规模上。

- **类比：** 我们不会在一天内搬出昂贵的租来的公寓。相反，我们会开始在隔壁的土地上建造我们的新房子。在一段时间内，我们将同时支付旧租金和新抵押贷款。然后，我们会逐渐开始移动我们的家具（我们的流量），一个房间一个房间地从旧公寓搬到新房子。我们会彻底测试每个房间。只有当旧公寓完全空了，我们确信新房子是完美的，我们才会最终停止支付租金。

这种渐进式、受控的迁移是安全进行的唯一方式。

#### **技术深入：迁移手册**

**步骤1：构建新基础**

首先，我们必须建造我们的新房子。我们选择了一个裸机提供商Hetzner，以其强大的服务器和实惠的价格而闻名。我们在地理上接近我们现有AWS区域的数据中心租用了物理服务器（例如，芬兰赫尔辛基的数据中心来取代我们在德国法兰克福的AWS集群）。

这就是我们谈到的"运营开销"变得非常真实的地方。我们没有得到一个漂亮的AWS仪表板。我们得到了一个裸机Linux服务器的root密码。我们负责一切：

- 安装操作系统。
- 配置服务器之间的复杂网络。
- 从头开始构建我们自己的Kubernetes集群。

我们使用像**k3s**这样的轻量级但功能强大的Kubernetes发行版来简化这个过程。但这仍然是一项巨大的任务。我们不仅要成为应用程序的专家，还要成为它运行的低级基础设施的专家。在几周内，我们精心构建并测试了我们在全球九个新数据中心的新的自托管Kubernetes集群。

**步骤2：渐进式流量转移**

这是迁移中最关键、最微妙的阶段。让我们以我们的欧洲流量为例。

- **初始状态：** 100%的欧洲流量通过我们的任播IP路由到我们在法兰克福AWS上运行的Kubernetes集群。我们在赫尔辛基的新自托管Kubernetes集群已经准备就绪，但没有承担任何流量。
- **双重宣告：** 我们配置网络开始同时从**AWS法兰克福位置和我们新的Hetzner赫尔辛基位置**宣告我们的任播IP。
- **1%测试：** 使用互联网的路由协议（BGP），我们可以控制有多少流量偏好一条路径而非另一条。我们只将**1%**的欧洲流量从AWS转移到我们在赫尔辛基的新裸机集群。其余99%继续由稳定的旧AWS基础设施提供服务。
- **监控一切：** 我们像鹰一样盯着我们的监控仪表板。我们仔细检查了那1%流量的错误率、页面加载时间和CPU使用率。新集群稳定吗？它的表现符合预期吗？
- **逐步增加：** 一旦我们确信新集群是健康的，我们就逐渐增加流量。我们会增加到10%。一天后，25%。然后是50%。每一步都经过仔细监控。如果我们看到任何问题，我们可以立即将所有流量重新转移到AWS，零停机时间。
- **最终切换：** 在几天的严格测试后，我们将**100%**的欧洲流量转移到了新的赫尔辛基集群。

法兰克福的AWS集群现在完全闲置，没有收到任何流量。它是一个空的、昂贵的公寓。

**步骤3：停用并重复**

随着流量完全迁移，我们可以安全地登录AWS控制台并终止我们法兰克福区域中的每台服务器、数据库和负载均衡器。金笼的第一个房间被拆除了。

然后，我们在接下来的两个月内，区域一个接一个地重复这个极其小心和深思熟虑的过程，直到我们的整个全球应用程序都在我们自己的自托管裸机基础设施上运行。

## 第16章：自动驾驶仪：全球网络的CI/CD

我们达到了运营的涅槃状态。我们的全球、自托管、裸机Kubernetes基础设施快速、弹性且成本低廉。我们的可观察性平台让我们前所未有地洞察复杂系统的每个角落。我们建造了一辆一级方程式赛车，我们有实时遥测数据来证明这一点。

但我们流程中还有一个部分仍然非常手动、缓慢且依赖人类。这就是将开发人员笔记本电脑上的软件新版本实际部署到我们全球九个Kubernetes集群的过程。

使我们的系统如此弹性的东西——它的分布式、多区域性质——也使它成为更新的噩梦。本章讲述构建我们现代基础设施的最后一块拼图：一个完全自动化的全球部署管道，可以作为我们整个系统的自动驾驶仪。

### 第一部分：九次部署的痛苦

我们的storefront-service新版本已经准备就绪。它有一个错误修复和一个小的性能改进。在过去，部署这个只需要一个命令。现在，这是一个高风险、长达一小时的仪式，我必须亲自执行。

我的终端窗口会打开，我会开始手动、逐个区域地推出。

- 首先，我会连接到我们的欧洲集群：kubectl config use-context frankfurt。
- 我会运行命令应用新的YAML文件：kubectl apply -f storefront-v2.3.yaml。
- 然后，我会盯着看。我会盯着法兰克福的Grafana仪表板，监控错误率、CPU使用率和延迟至少五分钟，以确保新版本稳定。
- 好的，法兰克福看起来不错。现在是美国。我会切换上下文：kubectl config use-context ohio。
- 我会运行相同的apply命令。
- 然后我会盯着俄亥俄的仪表板看五分钟。
- 我必须重复这个令人神经紧张的过程**九次**。每个全球集群一次。

这个过程效率极低，充满风险。

- **它很慢：** 一个简单的部署需要一位高级工程师一个多小时的专注时间。
- **它不一致：** 如果我在部署到第五个集群后被Slack消息分散注意力，忘记了做剩下的怎么办？我们会在世界不同地区运行不同版本的代码，这是一个后勤噩梦。
- **它有风险：** 如果我不小心输入了错误的文件名，并在一个区域部署了旧版本怎么办？整个过程都依赖于我，一个人类，不犯任何错误。

在一次常规部署中，临界点出现了。我不小心将用于storefront的配置应用到了新加坡集群的api-service上。这导致了东南亚所有用户的短暂但真实的停机。我们在十分钟内修复了它，但损害已经造成。

Suumit看到了销售图表中的下降。"新加坡发生了什么？"他问道。我解释了人为错误。"我们不能这样操作，"他说。"这个过程需要是万无一失的。它需要自动化。"

#### **识别问题：手动且容易出错的过程**

我们有一个最先进的、自我修复的生产环境。但我们**改变**那个环境的过程是我们过去脆弱的、手动的遗留物。它不可扩展，不可靠，并且创建了对任何足够勇敢运行部署的人的关键人依赖。

我们需要一个将九个集群视为单个逻辑单元的系统。我们需要一个管道，可以将开发人员批准的代码自动、一致且安全地交付到我们的整个全球舰队，而无需人类接触单个服务器。我们需要一个真正的CI/CD管道。

### 第二部分：装配线和送货车（CI/CD）

为了解决我们痛苦的部署问题，我们需要为我们的代码建立一个真正的装配线。行业术语是**CI/CD**，它代表一种理念，将彻底改变我们如何交付软件。

#### **技术深入：什么是CI/CD？**

CI/CD代表持续集成和持续部署（或交付）。听起来很复杂，但它是两个简单想法协同工作。

**CI：持续集成（质量控制站）**

持续集成是开发人员频繁将代码更改合并到中央存储库的实践。每次合并后，自动化流程会启动以构建和测试应用程序。

- **类比：** 把它想象成我们汽车工厂装配线最开始的**质量控制站**。每当一批新的生钢（开发人员的代码）到达时，机器人会自动取一个样本并对其进行一系列压力测试（单元测试、代码检查）。如果钢不纯或有缺陷，它会立即被拒绝并送回供应商，然后才能制成汽车零件。
- **目标：** CI的主要目标是及早、自动地发现错误、回归和集成问题。它确保主代码库始终健康并处于工作状态。

**CD：持续部署（自动交付车队）**

持续部署接受成功CI过程的输出并自动将其部署到生产环境。

- **类比：** 这是工厂生产线末端的**自动交付系统**。当一辆全新的汽车滚下装配线并通过最终的100点检查（CI过程）时，一队自动驾驶卡车已经在等待。它们自动装载汽车并直接交付到世界各地的正确展厅，没有人类司机碰过方向盘。
- **目标：** CD的主要目标是使发布过程成为快速、可靠、低压力的"非事件"。通过频繁部署小更改，任何单次部署导致重大停机的风险都大大降低。

<br/>
#### **我们的CI管道：GitHub Actions**

对于管道的第一部分，持续集成，我们使用了一个名为**GitHub Actions**的工具。由于我们所有的代码已经托管在GitHub上，这是一个自然的选择。它允许我们在代码旁边的简单YAML文件中定义自动化工作流。

我们配置了GitHub Actions工作流，每当开发人员打开Pull Request时，它会自动触发。这是我们新的、自动化的质量控制站。

- **代码被推送：** 开发人员完成新功能的工作并打开Pull Request。
- **自动测试运行：** PR打开的那一刻，GitHub Actions会启动一个干净的临时虚拟服务器，检出代码，并运行我们的整个自动化测试套件。如果即使单个测试失败，Pull Request也会被标记为一个大红色的"X"，并被阻止合并。这是我们的第一道防线。
- **代码检查：** 工作流还运行了"linter"，一种自动检查代码样式错误和常见错误的工具。这使我们的代码库保持干净和一致。
- **构建Docker镜像：** 如果所有测试和检查都通过，管道将执行最关键的步骤：它将使用存储库中的Dockerfile为我们的应用程序构建一个全新的Docker镜像。
- **将镜像推送到注册表：** 最后一步。新构建的、经过全面测试的Docker镜像（例如，dukaan/storefront:v2.3.1）被"标记"为唯一标识符，并推送到我们的中央容器注册表（Amazon ECR）。

我们CI管道的输出是单个、不可变、生产就绪的制品：Docker镜像。我们现在有了一个自动化且完全可靠的过程来构建我们的"集装箱"。等式的CI部分已解决。

我们已经完善了我们的工厂，用于制造高质量的汽车并将它们存放在仓库中。现在我们需要构建完全自主的全球配送卡车车队。我们需要解决持续部署问题。

### 第三部分：自动驾驶仪（使用Argo CD的GitOps）

我们使用GitHub Actions的CI管道是自动化的杰作。它接收开发人员的原始代码，并可靠地生成单个、经过测试和可信的制品：一个安全存储在我们容器注册表中的新Docker镜像。我们已经完善了制造高质量汽车的工厂。

但汽车仍然停在仓库里。我们仍然面临如何将这个新镜像（dukaan/storefront:v2.3.1）交付到我们九个全球Kubernetes集群的问题。旧的、命令式方法是编写一个脚本，运行kubectl set image...命令九次。这仍然是手动推送，仍然存在风险。

我们需要一种更优雅、更可靠、更自动化的方法。我们在一种称为**GitOps**的强大理念中找到了它。

#### **技术深入：什么是GitOps？**

GitOps是一种革命性的方式来思考云原生应用程序的持续部署。核心理念简单但深刻：**Git存储库是整个生产环境所需状态的单一真实来源**。

要对基础设施进行更改，您不需要SSH到服务器或从笔记本电脑运行kubectl命令。您在Git存储库中更改配置文件，打开Pull Request，并获得审查和合并。然后，自动化代理确保您的实时环境与Git存储库中描述的状态匹配。

- **类比：** 让我们回到我们的管弦乐队。
  - **旧方式（命令式）：** 我们会走到指挥（Kubernetes）面前，口头告诉他们："请将小提琴部分的乐谱更改为版本2.3.1。"这是一个直接的手动命令，不容易追踪或审计。
  - **GitOps方式（声明式）：** 有一个**主乐谱库**（我们的Git存储库），它是整个音乐会的单一、官方的真实来源。要进行更改，我们去图书馆，用新版本替换旧的小提琴乐谱，并获得正式批准和盖章（合并的Pull Request）。我们**从不**直接与指挥交谈。相反，一个专门的**图书管理员**（我们的GitOps代理）不断监视主图书馆。他们一看到新批准的音乐作品，就会自动将副本送到指挥那里，指挥然后让管弦乐队演奏它。

这个过程是基于拉取的、可审计的，并且非常安全。

#### **我们的工具：Argo CD**

我们为系统选择的"图书管理员"是一个名为**Argo CD**的开源GitOps工具。

Argo CD是一个在Kubernetes集群内运行的控制器。它唯一的工作是不断监视Git存储库，并将那里定义的状态与集群中实际运行的状态进行比较。如果检测到差异，它会自动采取行动同步两者，确保集群始终反映Git中的真实来源。

以下是我们如何实现新的、完全自动化的部署管道：

- **创建配置存储库：** 我们创建了一个全新的、单独的Git存储库，名为dukaan-infra-configs。这个存储库不包含应用程序代码。它只包含我们所有服务的Kubernetes YAML文件（我们的"乐谱"），按我们九个区域的文件夹组织。
- **安装Argo CD：** 我们在九个Kubernetes集群中的每一个都安装了Argo CD代理。然后，我们配置每个代理监视我们新配置存储库中的特定文件夹。法兰克福代理监视frankfurt/文件夹，俄亥俄代理监视ohio/文件夹，依此类推。

#### **新的部署工作流：合并的PR**

有了这个系统，我们的部署过程变得美丽、简单和安全。

- 开发人员想要发布storefront的v2.3.1版本。
- 我们在GitHub Actions上的CI管道运行测试，构建镜像，并将dukaan/storefront:v2.3.1推送到我们的容器注册表。这部分保持不变。
- **新步骤：** CI管道的最后一步现在是使用脚本自动打开Pull Request到我们的dukaan-infra-configs存储库。这个PR提出了一个简单的文本更改：在我们所有九个storefront-deployment.yaml文件中，它将行image: dukaan/storefront:v2.3.0更改为image: dukaan/storefront:v2.3.1。
- **人工检查点：** 现在一位高级工程师或发布经理查看这个PR。这是最后的人工关卡。他们不是运行复杂的命令；他们是在审查一个简单、可审计的文本更改。他们可以确切地看到在什么地方发生了什么变化。他们添加批准并点击**"合并"**。
- **魔力：** PR合并的那一刻，魔力开始了。所有九个集群中的Argo CD代理检测到官方库中的"乐谱"已更改。
- 每个代理将Git中的新期望状态与集群中的当前运行状态进行比较。它看到了差异：Git说要运行v2.3.1，但集群仍在运行v2.3.0。
- Argo CD宣布集群"OutOfSync"，并自动采取行动修复它。它有效地在集群内运行kubectl apply命令，触发Kubernetes安全的滚动更新到新版本。

这在我们整个全球舰队中并行、自动地发生。我们已经构建了一个真正的自动驾驶仪。长达一小时的、紧张的、手动部署仪式现在变成了平静的、五秒钟的、可审计的Git合并。

## 第17章：关键要点

- **CI/CD是现代、敏捷软件交付的基础。** **持续集成（CI）**确保您的代码始终处于健康状态，而**持续部署（CD）**确保您可以安全、快速地将其交付给用户。
- **CI管道是您的自动化质量关卡。** 它应该自动测试您的代码，检查其样式，并生成单个、不可变的制品（如Docker镜像）作为输出。
- **GitOps是持续部署的优越、声明式模型。** 通过使用Git存储库作为基础设施状态的单一真实来源，您使部署更安全、更透明、更可审计。
- **像Argo CD这样的工具充当GitOps的自动化代理。** 它们不断调和您的实时集群状态与Git存储库中定义的期望状态。
- 我们终于掌握了改变我们复杂机器的过程。部署不再是恐惧的来源，而是例行、无聊且安全的非事件。

## 第18章：大结局：现场故障转移

Dukaan的故事不再是秘密。在Scaler播客之后，我们的工程文化和我的个人旅程公之于众。但我们的下一步——公开宣布我们迁移到裸机以及令人难以置信的95%成本节约——才真正让印度科技社区沸腾。

反应是敬畏、怀疑和直接的怀疑。博客文章被撰写，Twitter线程辩论我们的理智。"不可能节省那么多。""他们的可靠性一定很糟糕。""这是一个很棒的故事，但不可能是真的。"

我们已经告诉世界我们已经构建了一个更好、更便宜、更快的引擎。现在，世界想看看引擎盖下的东西。

这是我们不只是打开引擎盖的那一天的故事；我们现场直播，用大锤砸引擎，证明它能多快修复自己。

### 第一部分：挑战——"Asli Engineering"

在热议和辩论中，收到了来自Arpit Bhayani的电子邮件。Arpit是前Google员工，也是"Asli Engineering"的创建者，这是一个深受该国最优秀工程师尊敬的科技YouTube频道。他的品牌不是关于高层次的创始人故事；它是关于弄脏手，深入代码，从第一原则探索技术。"Asli Engineering"翻译为"真正的工程"，这是对技术世界中经常肤浅、由流行语驱动的对话的直接挑战。

来自Arpit的邀请是不同的。这不是友好的聊天；这是公开的技术审计。他想谈论我们转向裸机的举措，我知道他会深入探究"如何"和"为什么"。

这次，我内心有些不同。在Scaler播客之前一直困扰我的冒名顶替综合症已经消失了。那次经历教会了我一个宝贵的教训：社区不在乎我的学位；他们关心的是我解决了哪些问题。

我的感受不是恐惧；我对我们团队所构建的东西感到强烈的自豪感，并对怀疑感到沮丧。我知道我们构建了一些特殊的东西，一些与常见的"仅云"教条相悖的东西。我也知道空谈是廉价的。我可以花一个小时_告诉_Arpit我们的系统具有弹性，但这不足以说服怀疑者。

我需要做更多。我需要_向_他们展示。

在我们计划播客时，我有一个想法——一个既绝妙又疯狂的想法。我决定在准备电话中向Arpit提出。

"Arpit，"我开始说，尽量显得随意，"关于裸机和成本的对话很棒。但任何人都可以展示成本节约图表。我想向他们展示为什么我们对这个设置如此有信心。我想向他们展示弹性。"

"你有什么想法？"他问道。

"我们进行现场故障转移，"我说。"在我们录制期间，现场直播，我会打开终端，SSH连接到我们主要的欧洲生产服务器之一...然后完全关闭它。"

电话那头沉默了一会儿。我几乎能听到他脑子里的齿轮在转动——工程师的他在计算巨大的风险，表演者的他在认可这难以置信的戏剧性。

"你是认真的？"他终于说。

"百分之一百，"我回答。"我想让人们亲眼看到会发生什么。我想让他们看到任播网络重新路由，系统自我修复，网站保持在线。没有花招，没有编辑。我们现场进行。"

Arpit的反应是纯粹的"Asli Engineering"。他听起来不担心；他听起来很兴奋。"好的，"他说，声音中明显带着笑容。"我们来做吧。Fatega to dekha jayega。"（如果坏了，我们就看看。）

舞台已经搭建好了。这不仅仅是另一个播客。这是一场没有安全网的高空走钢丝表演。对我们整个全球架构的现场、公开压力测试，以及随之而来的我们整个工程团队的声誉。我们即将冒一切风险来证明我们的观点。

### 第二部分：现场故障转移——关闭生产环境

与Arpit的播客上半场是深度探讨。我们讨论了我们的"Asli Engineering"理念。我们展示了我们的CI/CD配置。我们辩论了裸机与云的优点，用我们自己令人震惊的成本和性能数据来支持这一切。

但空谈是廉价的。

"这一切听起来令人难以置信，"Arpit说，靠向摄像头，扮演怀疑者的角色。"但这是一个复杂的系统。你怎么知道它像你说的那样有弹性？"

我笑了。这是我一直在等待的时刻。

"这是一个公平的问题，"我回答。"所以，我们不只是谈论它。让我们测试它。就在现在。现场直播。"

我分享了我的屏幕。观众不再看幻灯片了。他们在看我的桌面。

**第1步和第2步：建立基准**

"好的，首先，让我们看一个实时商店，"我说，输入buywow.in的URL，这是Wow Skin Science的官方网站。页面立即加载。

我打开浏览器的开发者工具，转到网络选项卡。我将鼠标指向主文档的响应头。"看到这个了吗？"我说，突出显示一个自定义头。"x-edge-route: bom1。这是我们的内部路由代码。这告诉我们我从印度这里的请求正在由我们在**孟买**的裸机集群提供服务。快速且本地。"

"现在，让我们看看它对世界其他地方的表现如何。"我打开了KeyCDN的全球延迟测试工具。我输入URL并点击"测试"。出现了一张世界地图，几秒钟内，结果开始填充。

- 法兰克福：**41ms**
- 伦敦：**18ms**
- 纽约：**34ms**

**第3步和第4步：证明边缘优势**

我切换到终端。黑色屏幕上的绿色文字充满了直播画面。"我现在要SSH连接到我们在**纽约**数据中心的实时、生产Kubernetes节点。"观众看着我输入ssh root@ny-prod-01，登录提示符出现了。

```
sudo shutdown -h now
```

我的SSH会话立即中断。连接丢失了。纽约的服务器正在下线。

这场现场关闭服务器的表演成为了传奇。评论区充满了难以置信的声音：

- **"他真的为了演示关闭了实际的服务器。我学到了，不要过度设计架构，只需观察数据并构建即可。"**

我们并不害怕服务器失败，因为我们的架构设计已经考虑到了失败的情况。我们接受了**"Fatega to dekha jayega"**（车到山前必有路）的哲学——这不是一种鲁莽的赌博，而是对我们能够立即自动恢复能力的自信宣言。

那次播客成为了最终的证明。我们不仅掌握了这台机器；我们还向世界展示了这种掌握能力。

## 第18章：要点总结**

- **弹性系统的终极测试是其优雅处理实时、意外故障的能力**。不要只是相信你的系统具有弹性；要测试它。
- **激进的透明度建立巨大的信任**。展示你的真实架构、真实数据，甚至在公共场合尝试破坏自己的系统，是建立可信度和吸引最优秀人才的有力方式。
- **"Asli Engineering"（真正的工程）**——一种专注于简单性、基本原则和成本效益的思维方式——与充满激情的工程师产生了深刻的共鸣。
- **展示，而不仅仅是讲述**。你能讲述的最有力的故事是现场演示。一次成功的故障转移比一千张关于高可用性的幻灯片更有价值。