# 第156期 适用于RAG的最佳开源嵌入模型 多语言自然语言处理及阿拉伯语文本的高性能开源嵌入模型

![jHAAIz](https://raw.githubusercontent.com/errolyan/tuchuang/master/uPic/jHAAIz.png)

嵌入阶段是检索增强生成（RAG）流程中的关键环节。
它处于数据提取和分块之后，决定着系统对信息的表示、搜索和检索效率。
本文将阐释嵌入的定义、工作原理，以及如何选择合适的嵌入模型，包括适用于英语、多语言及阿拉伯语场景的模型选项。

## 一、RAG工作流程中的嵌入环节
RAG工作流程通常遵循以下步骤：

1. **数据提取**：从文档、网站或数据库等多个来源收集文本（这一内容我们已在之前的文章中探讨过）
相关推荐：《适用于RAG的最佳免费文档摄入工具》
通过免费的开源工具对文档进行提取、清洗和扩充，使其真正具备适配大语言模型（LLM）的能力。
（来源：ai.gopubby.com）

2. **分块**：将文本分割为更小的、有意义且能保留上下文的单元（这一主题也已在 earlier 的文章中涉及）
相关推荐：《RAG性能的70%与分块相关》
分块是高效检索增强生成（RAG）系统的核心基础。从固定分块、递归分块到……
（来源：medium.com）

3. **嵌入**：将每个文本块转换为固定长度的数值向量。

4. **向量存储**：将嵌入向量存储在向量数据库中（如FAISS、Weaviate、Pinecone）。

5. **检索与生成**：在查询阶段，先对用户输入进行嵌入处理，找到语义相似的向量，再利用这些向量生成有依据的响应。

![YOFogn](https://raw.githubusercontent.com/errolyan/tuchuang/master/uPic/YOFogn.png)

### 基础RAG工作流程
嵌入环节确保语义相似的文本片段在向量空间中呈现出数值上的接近性。
这使得检索能够基于语义含义而非单纯的关键词匹配来实现。

## 二、嵌入与向量空间表示的概念
嵌入模型会将输入文本转换为向量——一组代表文本语义含义的浮点数数组。
该向量存在于高维空间中，空间内向量的距离远近可反映语义的相似程度。

- 语义相似的句子，其向量距离较近。
- 语义不相似的句子，其向量距离较远。

**示例**：
“AI helps companies innovate”（人工智能助力企业创新）
与
“Artificial intelligence supports business automation”（人工智能支持业务自动化）

尽管表述不同，但这两个句子所表达的核心思想相似，因此生成的向量距离较近。
这种几何表示方式使检索系统能够通过余弦相似度、点积等指标来衡量语义相似性。

## 三、嵌入模型的关键技术参数
选择嵌入模型时，有几项技术参数会直接影响其性能表现。
这些参数包括输入序列长度、输出维度、归一化特性以及批量处理能力。

正如我们在Hugging Face的MTEB（大规模文本嵌入基准测试）中所见，每个模型的这些参数都会明确列出，这便于在不同任务间对比模型性能，也有助于理解不同模型在处理语义相似性、检索任务及多语言场景时的表现。

![5ddr5R](https://raw.githubusercontent.com/errolyan/tuchuang/master/uPic/5ddr5R.png)

### MTEB（多语言版，第2版）
#### a. 输入序列长度
该参数定义了模型一次可处理的token（词元）数量。
若文本块超出此限制，则需对其进行截断或拆分为更小的子文本块。
这一参数也为我们确定最优文本块大小提供了参考：若文本块长度超过模型限制，截断操作可能导致关键上下文丢失，进而对检索质量产生负面影响。

#### b. 输出维度
输出维度即每个嵌入向量中包含的数值（特征）数量。
根据模型架构的不同，常见的维度包括384、768、1024、1536和2048。
高维嵌入能捕捉更丰富、更细微的语义关系，有助于提升检索准确性，但同时也会增加存储成本，并减缓大型数据库中的向量搜索速度。
反之，低维嵌入在大规模检索任务中效率更高、速度更快，但可能会损失部分语义深度与精度。



### 嵌入维度（即我们所说的输出维度）
#### c. 归一化与相似性指标
部分模型会输出归一化向量（向量长度=1），这简化了余弦相似度的计算过程。
但也有模型会生成非归一化向量，这类向量在建立索引或进行比较前需手动完成归一化处理。
若未对这些向量进行归一化，可能会导致相似性评分不准确——因为向量数据库可能会将向量幅值的差异误判为语义距离的差异。

#### d. 批量处理能力
支持批量推理的嵌入模型可同时处理多个文本块，这能提升大规模RAG流程的吞吐量。

## 四、最常用的开源嵌入模型
既然我们已了解向量的定义、嵌入的工作原理，以及嵌入在RAG流程中对文本语义表示的重要性，接下来就可以探索最常用的开源嵌入模型了。
这些模型在架构、维度大小和多语言支持能力上各有差异，但核心目标一致：将文本转换为有意义的数值表示，为语义搜索和知识检索提供动力。

多款开源模型因其出色的性能和易获取性而被广泛采用。
以下是下载量和基准测试评分最高的几款模型。

### 4.1 all-MiniLM-L6-v2


模型标识：sentence-transformers/all-MiniLM-L6-v2
- **维度**：384
- **性能**：处理短文本到中长文本时速度快、效率高。
- **适用场景**：适用于低延迟生产系统和高成本效益的检索任务。
- **评价**：模型体积小巧，但仍能有效捕捉语义关系。

#### 使用方法（基于Sentence-Transformers库）：
```python
pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
embeddings = model.encode(sentences)
print(embeddings)
```

### 4.2 all-mpnet-base-v2
按回车键或点击即可查看完整尺寸图片

模型标识：sentence-transformers/all-mpnet-base-v2
- **维度**：768
- **性能**：生成的嵌入质量高，具备强大的上下文理解能力。
- **适用场景**：通用语义搜索和聚类任务。
- **评价**：在准确性和速度之间实现了良好平衡。

#### 使用方法（基于Sentence-Transformers库）：
```python
pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
embeddings = model.encode(sentences)
print(embeddings)
```

## 五、多语言嵌入模型
在实际应用中，数据很少只以单一语言存在——例如简历、社交媒体帖子和用户评论可能会使用多种语言。
跨国企业、跨境平台及多语言信息系统需要检索和理解不同语言形式的内容。
多语言嵌入模型的核心价值正在于此：它们构建了一个共享语义空间，使不同语言的词语、短语和文档能基于语义含义（而非词汇本身）实现对齐。

以下模型是多语言场景下性能出众的开源解决方案，在全球数据集上展现出了优异的性能、可扩展性和稳健性。

### 5.1 Jina Embeddings v3
按回车键或点击即可查看完整尺寸图片

模型标识：jinaai/jina-embeddings-v3
- 支持100多种语言和大上下文窗口。
- 对短文档和长文档均能保持稳定性能。
- **维度**：可配置（512-1024）
- **性能**：支持长序列（最多8000个token）和多语言处理。
- **适用场景**：大规模语义搜索、RAG系统或混合检索系统。
- **评价**：在可扩展性和质量之间实现了出色平衡。

#### 使用方法（基于Sentence-Transformers库）：
```python
!pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("jinaai/jina-embeddings-v3", trust_remote_code=True)

task = "retrieval.query"
embeddings = model.encode(
    ["What is the weather like in Berlin today?"],  # 示例查询：“柏林今天天气如何？”
    task=task,
    prompt_name=task,
)
```

### 5.2 multilingual E5（e5-base-v2）
该模型在不同语言上均能保持均衡的准确性和效率。
适用于多语言语义检索和问答系统。

按回车键或点击即可查看完整尺寸图片

模型标识：intfloat/multilingual-e5-base

#### 使用方法（基于Sentence-Transformers库）：
```python
pip install sentence_transformers~=2.2.2

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('intfloat/multilingual-e5-base')
input_texts = [
    'query: how much protein should a female eat',  # 查询：“女性应摄入多少蛋白质”
    'query: 南瓜的家常做法',  # 查询：“南瓜的家常做法”
    "passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.",  # 段落：“根据美国疾病控制与预防中心（CDC）的通用指南，19-70岁女性的蛋白质平均需求量为每天46克。但从图表中可以看出，若处于孕期或为马拉松赛事训练，蛋白质需求量则需增加。可查看下方图表了解每日应摄入的蛋白质具体量。”
    "passage: 1.清炒南瓜丝 原料:嫩南瓜半个 调料:葱、盐、白糖、鸡精 做法: 1、南瓜用刀薄薄的削去表面一层皮,用勺子刮去瓤 2、擦成细丝(没有擦菜板就用刀慢慢切成细丝) 3、锅烧热放油,入葱花煸出香味 4、入南瓜丝快速翻炒一分钟左右,放盐、一点白糖和鸡精调味出锅 2.香葱炒南瓜 原料:南瓜1只 调料:香葱、蒜末、橄榄油、盐 做法: 1、将南瓜去皮,切成片 2、油锅8成热后,将蒜末放入爆香 3、爆香后,将南瓜片放入,翻炒 4、在翻炒的同时,可以不时地往锅里加水,但不要太多 5、放入盐,炒匀 6、南瓜差不多软和绵了之后,就可以关火 7、撒入香葱,即可出锅"  # 段落：“1.清炒南瓜丝 原料：嫩南瓜半个 调料：葱、盐、白糖、鸡精 做法：1、用刀薄薄削去南瓜表面一层皮，用勺子刮去瓜瓤 2、将南瓜擦成细丝（若无擦菜板，可用刀慢慢切成细丝） 3、热锅倒油，放入葱花煸出香味 4、放入南瓜丝快速翻炒约1分钟，加入盐、少许白糖和鸡精调味后出锅 2.香葱炒南瓜 原料：南瓜1个 调料：香葱、蒜末、橄榄油、盐 做法：1、南瓜去皮后切成片 2、油锅烧至八成热，放入蒜末爆香 3、爆香后放入南瓜片翻炒 4、翻炒过程中可不时向锅中加水，但水量不宜过多 5、加入盐炒匀 6、待南瓜变得软绵后关火 7、撒上香葱即可出锅”
]
embeddings = model.encode(input_texts, normalize_embeddings=True)
```

## 🕌 阿拉伯语嵌入模型：应对复杂语言的挑战
阿拉伯语具有独特的复杂性。其从右至左的书写体系、丰富的形态变化以及方言多样性，使其成为自然语言处理（NLP）领域难度极高的语言之一。
通用多语言模型往往难以充分捕捉阿拉伯语的语义深度。
因此，要实现精准的阿拉伯语检索与生成，专门的阿拉伯语嵌入模型至关重要。

以下是我测试过的一款开源阿拉伯语嵌入模型：

### GATE-AraBert-v1
GATE-AraBERT-v1是一款开创性的阿拉伯语嵌入模型，基于大规模现代标准阿拉伯语（MSA）语料库训练而成。该模型能有效捕捉阿拉伯语的句法、形态和上下文信息，可处理阿拉伯语特有的语言特征，包括从右至左的书写方式、词根体系以及元音符号。

该模型在文档检索、问答等各类语义任务中均表现出色，是构建通用阿拉伯语RAG系统的理想选择。

按回车键或点击即可查看完整尺寸图片

模型标识：Omartificial-Intelligence-Space/GATE-AraBert-v1

#### 直接使用方法（基于Sentence-Transformers库）：
```python
pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer

# 从🤗 Hugging Face Hub下载模型
model = SentenceTransformer("Omartificial-Intelligence-Space/GATE-AraBert-v1")
# 执行推理
sentences = [
    'الكلب البني مستلقي على جانبه على سجادة بيج، مع جسم أخضر في المقدمة.',  # 示例句子1：“棕色的狗侧躺在米色地毯上，前方有一个绿色物体。”
    'لقد مات الكلب',  # 示例句子2：“这只狗已经去世了。”
    'شخص طويل القامة',  # 示例句子3：“一个高个子的人。”
]
embeddings = model.encode(sentences)
print(embeddings.shape)  # 输出向量形状
# 输出结果：[3, 768]（表示3个句子，每个句子的嵌入向量维度为768）

# 计算嵌入向量间的相似性评分
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)  # 输出相似性矩阵形状
# 输出结果：[3, 3]（表示3个句子两两之间的相似性评分矩阵）
```

此外，GATE-AraBERT的嵌入向量还可与动态维度技术（如俄罗斯套娃（Matryoshka）方法）结合使用，根据计算资源或存储限制灵活调整向量大小。这一特性使该模型在阿拉伯语生产级NLP应用中既保证了准确性，又具备了可扩展性。

## 七、如何选择合适的模型
选择嵌入模型需综合考虑文本块大小、内存资源和具体应用场景：

1. **文本块大小与输入长度**：处理长文本块时，需选择token限制更长的模型，以避免截断导致上下文丢失。
2. **内存与维度**：高维嵌入（1024-2048）能捕捉更丰富的语义，但需更多内存和存储空间；低维嵌入（384-768）则速度更快、资源消耗更低。
3. **语言支持**：英语数据集适合使用MiniLM或MPNet模型；多语言任务可选择Jina v3、EmbeddingGemma或E5模型；阿拉伯语场景则需使用GATE-AraBERT或AraBERT Matryoshka等专用模型。
4. **可扩展性**：批量处理能力和动态维度嵌入（如Matryoshka方法）有助于高效处理大规模数据集。

最优模型需在准确性、效率和系统限制之间找到平衡，以适配RAG流程的具体需求。

选择嵌入模型时，需根据任务需求合理权衡性能、内存占用和速度，同时需注意：嵌入质量的高低直接取决于所选模型的优劣。

使用优质嵌入模型的主要优势包括：
- 提升查询与文档间的语义相似性匹配度。
- 提高检索任务的召回率和精确率。
- 使生成器输出的响应更符合上下文逻辑。

简而言之，RAG系统的智能水平取决于其嵌入向量的质量。

## 九、总结
嵌入环节将非结构化文本转换为结构化、具有语义意义的向量。
这些向量是RAG系统实现智能检索的基础。

对于以英语为主的数据集，MiniLM和MPNet等模型仍是可靠选择。
对于多语言或阿拉伯语数据，Jina Embeddings或E5模型能提供必要的跨语言理解和上下文捕捉能力。

嵌入阶段是文本转化为数据、语义转化为可量化指标的关键环节。
一个RAG系统的性能强弱，完全取决于其依赖的嵌入向量质量。

按回车键或点击即可查看完整尺寸图片

如果本文对你有帮助，不妨分享给朋友？
共同学习，共同进步。