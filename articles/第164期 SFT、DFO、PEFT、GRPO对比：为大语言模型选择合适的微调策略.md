 

# 第164期 SFT、DFO、PEFT、GRPO对比：为大语言模型选择合适的微调策略

GPT、LLaMA、Mistral等大语言模型（LLM）彻底改变了我们与数据和自动化工具的交互方式。但要让这些模型在建筑、建筑信息模型（BIM）、金融、医疗等专业领域高效发挥作用，往往需要对其进行适配，而微调技术正是实现这一目标的关键手段。

目前广泛讨论的四种微调方法如下：

- • 监督微调（SFT，Supervised Fine-Tuning）
- • 直接偏好优化（DFO，Direct Preference Optimization）
- • 参数高效微调（PEFT，Parameter-Efficient Fine-Tuning）
- • 组相对策略优化（GRPO，Group Relative Policy Optimization）

每种方法都有其独特优势、成本特点和适用场景，下文将对它们逐一拆解并进行对比。

## 1️⃣ 监督微调（SFT）

监督微调是传统的微调方式，通过输入-输出对标注数据集更新整个大语言模型的权重参数。

### 工作原理

1. 1. 以预训练基础模型为起点；
2. 2. 使用特定任务数据集进行训练（例如“BIM图纸修正→修正后的图纸名称”这类任务）；
3. 3. 更新网络中的所有参数。

### ✅ 优势

- • 最适合专业知识迁移；
- • 模型能完全适配目标领域；
- • 在大规模数据集上表现出色。

### ❌ 劣势

- • 成本高昂（对GPU资源需求大）；
- • 需要数万个样本数据；
- • 存在“灾难性遗忘”风险（即模型丢失基础模型原本具备的能力）。

![null](https://fastly.jsdelivr.net/gh/bucketio/img3@main/2025/10/24/1761319286667-880521fc-a5f2-4ff2-a2ea-5bb3439373a2.png)

以下是DeepLearning.ai提供的部分SFT实现示例，若需实践操作，可在其官网报名相关课程。

## 2️⃣ 直接偏好优化（DFO）

直接偏好优化是一种无需强化学习的大语言模型人类偏好对齐方法，无需像基于人类反馈的强化学习（RLHF）那样依赖独立的奖励模型，可直接优化模型输出质量。

换句话说，直接偏好优化（DFO）属于对比学习方法，会同时利用正样本和负样本，通过最小化对比损失来惩罚负向响应、鼓励正向响应。DFO的损失函数本质上是交叉熵损失在正负响应奖励差异上的应用，其中奖励通过重新参数化的奖励模型估算得出。

### 工作原理

1. 1. 收集响应对：一个是更优响应，一个是较劣响应；
2. 2. 无需训练奖励模型，直接优化模型参数以提高更优响应的出现概率。

### ✅ 优势

- • 比RLHF更简洁，无需额外的奖励模型；
- • 适合与主观偏好（如礼貌语气、安全性）对齐；
- • 训练过程更稳定。

### ❌ 劣势

- • 需要带标注的偏好数据集；
- • 在事实知识提升或领域适配方面，效果远不如SFT。

![null](https://fastly.jsdelivr.net/gh/bucketio/img4@main/2025/10/24/1761319306896-d31f3b58-bd83-4b52-bd85-924b6f3e084b.png)

以下是DeepLearning.ai提供的部分DFO实现示例，若需实践操作，可在其官网报名相关课程。

## 3️⃣ 参数高效微调（PEFT）

参数高效微调通过添加小型可训练适配器（如LoRA、前缀微调、P-Tuning v2等）实现微调，同时冻结基础模型的大部分参数。

### 工作原理

1. 1. 基础大语言模型参数保持固定；
2. 2. 注入并训练轻量级参数（通常占模型总权重的1%以下）；
3. 3. 可针对不同领域切换多个适配器（具体适配器类型可参考我的上一篇文章）。

（相关延伸阅读：《大语言模型中各类适配器介绍》，链接：bobrupakroy.medium.com，文章探讨了如何通过适配器提升大语言模型效率）

![null](https://fastly.jsdelivr.net/gh/bucketio/img3@main/2025/10/24/1761319336041-86798989-092b-4d71-94dd-64180bfecdf2.png)

编码器-解码器工作流程 | 图片来源：StackExchange  

（相关延伸阅读：《深入理解Transformer架构：通过……彻底改变自然语言处理》，链接：bobrupakroy.medium.com，文章深入解析了Transformer架构）

参数高效微调会尝试调整适配器层的权重，根据用于微调的数据集校准模型输出。诸如LoRA、适配器、前缀微调等PEFT方法，均旨在微调模型参数的一小部分，或在冻结原始模型权重的前提下添加新的可训练参数。

以LoRA这类PEFT方法为例，我们可以对以下组件进行修改：

![null](https://fastly.jsdelivr.net/gh/bucketio/img18@main/2025/10/24/1761319347032-f5e1196f-0ab2-4bdd-a2fe-05522a560e4f.png)

通过以下代码可查看模型层结构：

```python
  from torchsummary import summary
summary(model, input_size=(seq_len, hidden_dim))
# 或

from transformers import GPT2Model
# 加载GPT-2模型
model = GPT2Model.from_pretrained("gpt2")
# 打印完整模型架构
print(model)
```

模型架构输出如下：

```
  GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (c_attn): Conv1D(2304, 768)
        (c_proj): Conv1D(768, 768)
        ...
      )
      ...
    )
    ...
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
```

# 关键部分：

# c_attn：映射为Q、K、V（768×3=2304）

# c_proj：最终注意力投影（即文中提及的P层）

# 以上组件位于h（Transformer层）内的每个Block中

```
### ✅ 优势
- 成本仅为SFT的1/10到1/100；
- 适用于小型数据集（仅需数千个样本）；
- 可基于一个基础模型搭配多个领域适配器；
- 部署与维护便捷。

### ❌ 劣势
- 针对高度复杂任务时，准确率略低于完整的SFT；
- 仍会继承基础模型的局限性。

（相关延伸阅读：《使用LoRA进行参数高效微调（PEFT）的详细实践指南》，链接：bobrupakroy.medium.com，文章深入讲解了PEFT的实操流程）


## 4️⃣ 近端策略优化（PPO）
近端策略优化（PPO）是一种用于基于人类反馈的强化学习（RLHF）的强化学习算法，通过奖励模型训练策略模型，优化模型以生成更优响应。

### 工作原理
1. 以基础模型（如GPT）为起点；
2. 针对某个提示生成候选输出；
3. 由基于人类反馈训练的奖励模型对这些输出打分；
4. PPO优化大语言模型，在最大化预期奖励的同时，确保模型输出与原始模型保持接近（避免“偏移”）。

### ✅ 优势
- 经典的RLHF方法（早期ChatGPT训练即采用该方法）；
- 能提升模型的有用性、安全性与偏好对齐度；
- 适用于持续反馈循环场景。

### ❌ 劣势
- 流程复杂、成本高且步骤多：
  需要先训练SFT基础模型→再训练奖励模型→最后进行PPO优化；
- 对奖励模型质量敏感；
- 不适用于领域知识训练。


![](https://fastly.jsdelivr.net/gh/bucketio/img16@main/2025/10/24/1761319372654-84e0ce0e-3b29-46b4-88a9-64b0d01491f4.png)


近端策略优化（PPO）  

如图所示，流程中包含两个模型：奖励模型（基础模型）与参考模型，通过KL散度进行对比，进而更新经过PEFT优化的模型。

（相关延伸阅读：《结合强化学习（PPO）与PEFT微调大语言模型：实现RLHF自动化》，链接：bobrupakroy.medium.com，文章详细讲解了PPO的实现过程）


## 5️⃣ 组相对策略优化（GRPO）
组相对策略优化（GRPO）是一种强化学习方法，通过对比模型的多组响应、优化模型以向“优响应相对于劣响应的排序”靠拢，从而提升大语言模型的对齐效果。

可以将其理解为“无需独立奖励模型的RLHF”，它利用多个答案间的相对偏好而非单一分数来优化模型。

### 工作原理
1. 模型针对某个提示生成多个输出；
2. 通过评分函数或人工标注对这些输出进行排序；
3. GRPO调整模型参数，使模型更倾向于生成排序更优的输出。

### ✅ 优势
- 比RLHF中使用的PPO更高效；
- 在安全性、推理能力与多轮对话连贯性方面表现出色；
- 当可获取相对偏好数据而非绝对奖励时，该方法十分实用。

### ❌ 劣势
- 比SFT或PEFT更复杂；
- 需通过大规模推理生成候选响应；
- 核心目标是对齐而非领域学习，因此不主要用于知识适配。

按回车键或点击可查看完整尺寸图片  

GRPO | 图片来源：https://miro.medium.com/  

如图所示，与PPO不同，该流程无需额外的大语言模型用于输出对比，而是通过“奖励服务器”（即基于Python脚本实现的业务逻辑）对输出进行排序。


## 🆚 对比表格


![](https://fastly.jsdelivr.net/gh/bucketio/img6@main/2025/10/24/1761319391786-a908acc3-a78a-46f8-ac82-3e3a88ca1f2e.png)



## 💡 总结思考
2025年，对于企业而言，“参数高效微调（PEFT）+检索增强生成（RAG）”仍是将大语言模型适配到特定领域（如BIM或建筑行业）的最实用方案。

若需实现模型与人类偏好的对齐及安全行为，则可在上述方案基础上，叠加DFO、GRPO或PPO等强化学习技术。

感谢阅读！若你喜欢这篇短文，我的Medium专栏（链接：https://medium.com/@bobrupakroy）中还有大量关于高级分析、数据科学与机器学习的文章可供阅读。

我还活跃于Facebook、Instagram、Udemy、Blogger、Issuu、Slideshare、Scribd等平台，也可在Quora（链接：https://www.quora.com/profile/Rupak-Bob-Roy）上找到我。

如有任何需求，欢迎告知。期待与你交流！

也欢迎查看文中链接，希望能对你有所帮助。
```

 