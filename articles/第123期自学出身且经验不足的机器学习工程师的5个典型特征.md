# 第123期:自学出身且经验不足的机器学习工程师的5个典型特征


在初级工程师竞争激烈的就业市场中，只有具备最强技能的人才能成功入职。本文将梳理出自学出身、经验不足的机器学习工程师的五大典型特征，助你避开这些“陷阱”，在简历呈现、面试表现乃至入职初期的工作中都展现出最佳状态。

我的观点源于两方面经验：一是在亚马逊团队中指导众多成员的经历，二是多年担任职业教练的实践；同时，我自身也属于“半自学”群体——由于背景并非传统科班出身，我需要通过大量自主学习提升能力，才达到如今的职业高度。

接下来，我们直接进入正题。

## 注意事项
在展开第一个特征之前，有个重要提醒：“自学”这个词的含义其实有些复杂。事实上，无论你是否接受过正规培训，职业生涯中所学的绝大部分知识都来自实际工作。

因此，我真正想探讨的，是那些“未接受过良好指导、且无大型项目经验”的工程师所呈现的特征。即便拥有大学学位，也可能在这些方面存在不足。

尤其是第一个特征，这类问题更为常见。

### 特征一：只关注模型本身
经验不足的机器学习工程师最明显的“红色警报”之一，就是他们的关注点仅停留在模型本身。

不妨换个更宏观的视角：作为机器学习工程师，核心职责是解决问题，而仅存在于Jupyter笔记本中的模型几乎无法实现这一目标。我曾见过一些极具天赋的人花费数周时间构建模型，其离线性能指标十分出色，但最终却发现现有系统无法兼容该模型的输入格式，或是模型推理时间长达5秒——而业务需求却要求响应时间控制在1秒以内。

从项目启动之初，就应当考虑整体系统设计：模型将如何部署？运行在服务器还是移动设备上？数据通过何种方式接入——批处理、实时API还是流处理？模型失效或输出异常时该如何应对？如何监控模型在生产环境中的性能？

这些问题从一开始就必须纳入考量，因为它们将极大影响“哪种模型适合当前任务”这一关键决策。

而有时，系统设计中最关键的考量点是“是否真的需要机器学习”——这就引出了第二个特征。

### 特征二：开篇就选用过于复杂的模型
第二个典型特征是“直接选用自己能想到的最复杂模型”。这是许多自学工程师容易陷入的陷阱，因为他们希望通过这种方式展示自己对前沿技术的掌握程度。

但从复杂模型起步，会引发诸多问题：
- 缺乏可用于对比的基准指标
- 投资回报无法明确衡量
- 迭代启动周期过长
- 模型难以维护
- 无法向相关方清晰解释模型逻辑
- 出现问题时调试难度极大

更关键的是，该问题或许根本无需机器学习介入。我职业生涯中最引以为傲的时刻之一，就是说服产品经理放弃构建实时个性化推荐模型，转而使用一段精心编写的SQL查询——这段查询以1%的复杂度和开发时间，满足了80%的业务需求。

因此，始终应从简单方案入手，先搭建可用的基准版本。无论是线性回归、逻辑回归，还是基础启发式算法皆可。之后再逐步迭代，且只有在能通过明确的性能提升和业务价值证明“需要增加复杂度”时，才进行相应调整。

### 特征三：不良的软件工程实践
第三个体现经验不足的特征是“软件工程实践存在缺陷”。这一点至关重要，因为机器学习本质上仍属于软件工程领域，而这正是学术项目与工业界工作之间差距最大的地方。

首先谈谈测试。机器学习流水线与其他软件产品一样，都需要测试环节。关键在于尽早建立测试机制，并在代码变更时持续执行测试——数据预处理、特征工程、模型训练逻辑，尤其是预测流水线，都需要对应的测试覆盖。

测试与部署检查需通过CI/CD（持续集成/持续部署）流水线执行，哪怕是GitHub Actions这类简单工具也可。但如果有人表示“合并代码前手动运行测试”或“部署到生产环境前手动检查模型”，那情况就非常糟糕了。

模块化设计也很关键。还记得我们之前提到的“不要只关注模型本身”吗？使用单一的“大而全”笔记本是不可取的。代码编写应遵循“便于维护、扩展和整体系统迭代”的原则。

说到迭代，经验不足的工程师常提交“大规模代码变更”。这种做法极不推荐：一方面，大规模变更几乎无法进行全面评审（而初学者尤其需要细致评审）；另一方面，在变更易于调整的阶段，你会错失获得有效反馈的机会。此外，这种行为还会让团队感到困扰，引发不满。

但即便拥有完美的工程实践，仍有一个关键步骤常被工程师忽略或过度投入——这就引出了第四个特征。

### 特征四：跳过探索性数据分析（EDA）或过度进行EDA
第四个特征体现在两个极端：要么完全跳过探索性数据分析（EDA），要么在这一步投入过多精力，走向另一个极端。

职业生涯初期，你知道需要做EDA，于是绘制大量图表，但在复盘时却流于形式——因为你并未关注真正有价值的信息。

我们不需要“为了画图而画图”。优质的EDA应能回答具体问题：是否需要对特征进行标准化或缩放处理？空值存在于哪些位置，该如何处理？上游数据是否存在明显错误，需如何解决？有哪些潜在的数据泄露风险需要警惕？

最重要的是，“哪种模型适合当前问题”？许多模型有特定假设需要验证，例如线性模型要求变量间存在线性关系，神经网络则需要大量数据支撑——这类关键信息都需通过EDA确认。

举个具体例子：假设你正在处理欺诈检测问题，初步EDA显示99.9%的交易为合法交易。这一发现立即表明“准确率”是无用的评估指标，且需要采用专门技术处理严重的类别不平衡问题。

理解异常值也至关重要：它们是数据中的错误，还是需要重点捕捉的关键案例？在上述欺诈检测案例中，异常值往往就是实际的欺诈交易。

此外，EDA还能为特征工程提供思路。优质的EDA应让你明确“如何转换特征以提升预测能力”。

核心原则是“带有明确目的”。每一张图表、每一个统计摘要，都应能为“如何开展建模工作”提供决策依据。

但即便EDA做得完美、模型构建出色，仍有一个领域常让经验不足的工程师“自食恶果”——这就是第五个特征。

### 特征五：指标选择不当或解读错误
最后一个特征是“对指标理解不足”，其危害性可能最大，因为它会影响你对所有工作的评估判断。

最基础的错误是“在错误的数据集上评估指标”。我曾见过有人在静态测试集（而非合适的留存集）上报告多个模型的性能，更有甚者，直接使用训练集评估性能——这种做法显然不可取。

其次是“选择完全不适用的指标”，最典型的例子就是过度依赖“准确率”。现实世界中，类别平衡的数据集极为罕见。例如在邮件垃圾检测任务中，若95%的邮件为正常邮件，那么一个“全部预测为正常邮件”的模型就能达到95%的准确率，但实际上毫无实用价值。

为何说“99%的准确率应引发警惕而非兴奋”？因为在多数实际问题中，99%的准确率通常意味着两种情况：要么存在严重的类别不平衡且未得到妥善处理，要么存在数据泄露——未来信息被混入训练数据中。

我曾评审过一个客户流失预测模型，其宣称准确率达98%，听起来十分出色。但深入调查后发现，模型中意外包含了“客户流失后才可得的特征”——这是典型的数据泄露问题。修正后，模型实际性能约为72%，虽仍属良好，但更贴合实际情况。

另一个重要问题是“未将模型指标与实际业务指标关联”。或许你的模型精确率（precision）达到0.85，但这对营收有何影响？F1分数提升5%能如何转化为客户满意度提升或成本节约？若无法建立这种关联，就意味着你并未解决实际业务问题——而这正是你的核心职责。

“仅从全局层面检查指标”也是一大陷阱。你需要分析模型在不同数据子集上的性能：例如，模型可能在25-35岁用户群体中表现优异，但在65岁以上用户中效果糟糕；或是在城市地区性能良好，在农村地区却不尽如人意。这些洞察对理解模型局限性、推动优化至关重要。

最后，还需从“定性角度”理解模型错误。不要只关注数字——要深入分析模型失效的具体案例。你应能解释“模型为何对特定样本分类错误”，并利用这些洞见改进特征工程或模型架构。

我曾参与过一个性能异常糟糕的模型项目。通过系统性的定性错误分析，发现问题根源在于评估数据标签标注不当——并非模型出错，而是标签有误。这一问题源于人工标注团队的培训不统一，仅靠指标本身根本无法发现。

## 结语
这五大特征——仅关注模型、开篇用复杂模型、软件工程实践不良、EDA操作不当、指标理解偏差——是“无生产环境机器学习项目经验”的最明显信号。

但好消息是，只要心态正确、多加实践，这些问题都能彻底解决。重点在于“端到端解决问题”，而非仅构建“酷炫的模型”；从简单方案起步，根据实际业务价值逐步迭代；将机器学习视为软件工程的一个分支，遵循相应规范；开展有明确目的的数据分析，为建模决策提供支撑；同时深入理解指标背后的意义——既要知晓模型性能，也要明确其业务影响。