# 第161期 知识困境：人工智能为何越来越笨


2023年3月，研究人员让OpenAI的GPT-4完成一项简单的数学任务——判断数字是否为质数，它的正确率达到了97.6%。但三个月后，当研究人员进行完全相同的测试时，GPT-4的准确率却骤降至2.4% 。

模型并没有更换，仍然是OpenAI持续更新的GPT-4。在这三个月的改进、调整和优化过程中，该系统不知为何竟然“忘记”了如何识别质数。

这并非是在讨论人工智能的不完美，毕竟我们都知道它并不完美。但现在的情况更加奇怪：人工智能系统正在遗忘它们曾经掌握的知识，而我们正是促使它们遗忘的“罪魁祸首”。

## 效果过好的方法
多年来，提升人工智能的方法极其简单：从互联网上获取更多文本，输入到更大的模型中，然后看着它们变得更智能。这些模型通过阅读数百万个句子来学习语法；从数千个信息源中反复接触同一事实，从而掌握这些知识；通过吸收人类的论证、解释和思想关联方式，学习推理模式。

这种方法之所以有效，是因为人类的文字内容总体上蕴含着大量浓缩的知识。当你读到“法国大革命始于1789年”时，你获取了一个事实。当你阅读一百篇不同的文章，它们在不同的语境中都提到这个日期——有的涉及玛丽·安托瓦内特，有的提到巴士底狱，还有的将其与其他革命进行比较——你学到的就更加丰富了。你不仅了解了这个事实，还知道了它与其他事物的联系。

早期的人工智能系统正是通过大量“吸收”这些信息而变得智能，它们仅通过预测下一个单词，就吸收了人类知识的结构。

但这种“灌输式”的方法存在一个问题：信息源最终会枯竭。

## 互联网资源耗尽
到2023年，人工智能实验室基本上已经把所有有价值的文本都用于训练了。包括整个维基百科、Reddit上的所有内容、所有合法获取的出版书籍、研究论文、新闻档案、论坛帖子、技术文档等。主流模型已经“见识”过这一切。

那么，当你已经用尽所有高质量的人类文本，却仍想让人工智能变得更优秀时，该怎么办呢？

你可以等待人类创作更多内容，但人类创作的速度太慢。

你也可以付钱给专家来创建训练数据，但专家的费用高昂，而且这种方式难以大规模应用。

或者，你可以像许多实验室正在做的那样：用人工智能生成的文本训练人工智能，让人类评估者投票选出他们更喜欢的答案，然后让模型学习生成更多人们喜欢的内容。

但这却导致了问题的出现。

## 偏好取代了事实
下面来看看现代人工智能训练流程中实际发生的情况：

模型针对一个问题生成十个不同的答案，人类评估者从中挑选出他们最喜欢的那个。也许是因为这个答案更礼貌，也许是更简短，又或者听起来更权威。于是，模型就学到了：“生成更多和人类喜欢的答案类似的内容”。

注意，在这个循环中缺少了一个关键环节：没有人去检查答案是否正确。

《自然》杂志最近的一项研究系统地记录了这一现象。研究人员发现，新出现的、规模更大的语言模型越来越倾向于给出自信的错误答案，而不是承认自己的不确定性。模型规模越大、“优化程度”越高，这个问题就越严重。

其中的机制很简单。当人类评估者评价人工智能的回答时，他们并没有进行事实核查，而是根据答案给他们的感觉来判断。一个自信、听起来权威的答案往往会胜出，即使它是错误的；而一个有所保留、不确定且承认存在局限性的答案，即使更准确，也往往会被否定。

经过数千次的训练循环，这就产生了一种系统性偏差：模型了解到，无论确定性是否合理，听起来自信就能得到奖励。正如一位研究人员所说，这些模型被“优化为总是提供可信的答案”，而非正确的答案。

现在，将这种情况扩展到数百万个训练示例中。那些听起来“过于具体”的事实被弱化；可能冒犯他人的观点被回避；技术上正确但在社交场景中不合适的答案受到惩罚。与此同时，那些听起来合理的、自信的虚构内容却得到了强化。

模型并没有朝着更准确的方向学习，而是在学习如何更讨人喜欢。

## 合成数据的恶性循环
情况还在变糟。还记得我们的人类文本资源快耗尽了吗？许多实验室已经开始生成合成训练数据：他们用人工智能创建示例，然后用这些示例训练新的人工智能。

在你没有意识到其后果之前，这似乎是一种可行的办法。想象一下，你通过阅读一本教科书来学习历史，而编写这本教科书的人，是从上一代编写的教科书里学习历史，以此类推，追溯到十代之前。每一位作者都引入了一些小错误，而且都会强调他们认为最有趣的内容。等传到你手上时，你学的就不再是真实的历史，而是被歪曲的历史“回音”。

这正是人工智能训练中正在发生的事情。模型生成的文本看似合理，但却存在细微错误。这些文本又成为下一个模型的训练数据，错误没有相互抵消，反而不断累积。而且由于模型被优化为听起来自信满满，所以你无法分辨哪些自信的答案是有可靠依据的，哪些是基于人工智能层层猜测得出的。

## 我们正在失去的东西
人工智能犯错并不悲剧，真正悲剧的是早期的模型确实学到了真实的知识。2020年GPT-3推出时，它从数百万个人类文档中吸收了各种模式，构建了一个关于世界的内部模型。虽然并不完美，但这个模型反映了真实的人类知识。

现在，我们却在这个基础之上，不断用“偏好优化”的“混凝土”将其覆盖。我们在训练模型去掩盖不确定性、回避令人不适的事实，让它们听起来好听，而非追求正确。在每一次训练循环中，原始知识都被更深地掩埋在人们在特定时刻、特定背景下表示喜欢的内容之下。

模型听起来仍然很智能，甚至可能听起来比以前更智能。但随着每次迭代，它们与现实的联系却越来越薄弱。

## 不同的路径
一些研究人员开始意识到，这种情况无法持续下去。他们正在探索的解决方案，不是使用更大的模型或更多的合成数据，而是一个事后看来近乎显而易见的方法：让人工智能清楚自己知道什么。

不要把知识当作隐藏在模型权重中、不可审查的隐含信息，而是要让知识变得明确。为事实标注来源，标记哪些说法有时效性，指出矛盾之处。当人工智能遇到问题时，不要仅仅让它生成听起来正确的内容，而是要让它检查信息来源，评估其可靠性，并分辨哪些是它真正知道的，哪些只是猜测。

这并非拟人化的要求，而只是一种架构设计。人类并非单纯依靠模式匹配进行推理，我们会追溯知识的学习来源，在获得更好的信息时更新自己的认知，会区分“我在教科书里读到的”和“我好像在某个地方听到过的”。人工智能也可以做到这些，前提是我们构建的系统能够将知识管理与语言生成区分开来。

而我们目前的做法却与之相反：寄希望于在优化偏好的同时，依靠模型规模和人类反馈来保留事实真相。但这并没有奏效，每个月，人工智能系统都在错误的道路上越走越远，却在表面上显得越来越正确。

## 未来会怎样
我们正处在一个转折点。通过挖掘人类文本轻松提升人工智能的阶段已经结束。现在的问题是，我们是要继续坚持那些为了追求“讨人喜欢”而牺牲知识准确性的方法，还是构建能够真正维护和拓展自身知识的人工智能。

这无关乎让人工智能变得“有意识”或“真正智能”，而是关乎基本的认知规范。如果你正在构建一个人们依赖其获取信息的系统，那么这个系统需要分辨真假，追踪信息来源，并在获得新证据时更新认知。

我们的选择，不是在“继续当前做法”和“迈向科幻未来”之间做决定，而是在两种系统之间做选择：一种是为了听起来完美而逐渐遗忘所学知识的系统；另一种是将知识视为需要仔细维护、验证和拓展的系统。 