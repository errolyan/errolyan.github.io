# 第126期 向量数据库高级教程（检索以及相似度计算）



向量数据库存储数值化的“语义”（即嵌入向量），并支持通过相似度而非精确文本进行搜索。

在如今这个AI驱动的时代，语义搜索、向量嵌入、检索增强生成（RAG）等术语正逐渐成为日常技术词汇的一部分。我在X（原Twitter）上关注了大部分AI领域的技术人士，几乎每隔一天就能看到与向量数据库（Vector DB）相关的内容。

向量数据库是一款功能强大的后台支撑引擎，能让AI更智能、响应更迅速、对上下文的感知更敏锐。

本文将为你详细介绍向量数据库的定义、工作原理、重要性，以及它与传统数据库的区别。内容涵盖理论知识、工具对比、实际应用场景，还包含一个基于Sentence-Transformers和Qdrant（本地部署）的Python实战教程。

![](https://fastly.jsdelivr.net/gh/bucketio/img15@main/2025/10/05/1759633245082-95d4810d-4696-4820-8cee-31904c38bd0d.png)

若你觉得本文对你有帮助，欢迎点赞支持，这能让更多人看到这篇文章。为了撰写本文，我投入了大量时间和精力，希望能帮助所有想要学习向量数据库的读者。你也可以点击下方链接阅读我的其他博客：

《适合你的AI作品集的10个LLM与RAG项目（2025-2026）》  
检索增强生成（RAG）就像是为你的AI升级了内存，还配备了谷歌搜索栏。它不再根据训练时“认为”学到的内容编造答案，而是能获取实时、相关的信息——本质上，它不再“幻觉输出”，而是开始引用来源了。
techwithram.medium.com


## 首先从向量说起
在AI领域，向量是数据的数值化表示形式。无论是句子、图像，甚至是音频片段，都能被转化为一长串数字（维度通常在100到1000以上），这些数字捕捉了数据的语义或特征信息。

举个例子：

“巴黎是法国的首都。”  
→ [0.13, 0.92, -0.34, …, 0.45]
这类数值化表示被称为嵌入向量（Embeddings），正是因为有了它们，大型语言模型（LLMs）才能更好地理解“巴黎”和“伦敦”之间的语义关联，而不仅仅是通过字符串对比来判断。

但传统数据库（如MySQL、MongoDB）并非为存储和搜索数百万个高维向量而设计。

这正是向量数据库的用武之地。


## 什么是向量数据库？
向量数据库是一款专门设计的系统，旨在高效存储、建立索引和搜索向量嵌入，尤其适用于大规模数据场景。

与传统数据库通过行或列进行匹配不同，向量数据库基于语义相似度返回结果。

具体来说：

- 传统数据库：查找“apple”（苹果）时，只会匹配与“apple”完全相同的文本
- 向量数据库：查找“apple”时，会匹配与“apple”语义相似的内容，如“fruit”（水果）、“granny smith”（澳洲青苹）、“orchard”（果园）

嵌入向量通常由特定模型生成，例如Transformer系列模型（如BERT家族）、用于图像与文本联合处理的CLIP模型，或是用于生成句子级嵌入向量的Sentence-Transformers模型。


## 向量数据库的实际工作原理（索引、ANN、距离度量）
### 核心问题：近邻搜索
给定一个查询向量，需要从大量向量集合中找到与之最相似的向量。暴力搜索（Brute Force）会将查询向量与每个向量逐一对比，时间复杂度为O(n)——当向量数量达到数百万时，这种方式会非常缓慢。向量数据库通过近似近邻（Approximate Nearest Neighbor, ANN）技术和优化的索引结构解决了这一问题。

### ANN算法（简要理论）
近似近邻（ANN）是近邻搜索的一种类型，也是向量数据库中常用的技术，它能在允许一定近似度的前提下，找到与查询点最接近的数据点。

与精确近邻搜索不同，ANN更注重速度和效率，通过接受微小的近似误差来换取显著的速度提升。这种方法在高维空间中尤为有效，而高维空间正是现代AI应用的典型场景，在这类场景中精确匹配的计算成本极高。

例如，在图像识别系统中，ANN可以分析图像并将其转化为向量，再与数据库中的图像向量进行对比；在音乐流媒体服务、医学影像领域，ANN也有着广泛应用。

### 关键步骤：
- **HNSW（层次化可导航小世界）**：构建分层的邻近图，从粗粒度到细粒度逐步导航，快速定位目标。许多现代向量数据库将HNSW作为默认索引方式，因其在速度和准确率之间取得了出色的平衡。
- **IVF（倒排文件）与PQ（乘积量化）**：常用于FAISS（Facebook AI相似性搜索库）和部分向量数据库，可实现向量压缩并划分搜索空间。
- **ScaNN、Annoy、LSH**：其他可选的ANN算法，选择哪种算法需根据数据规模、准确率要求和内存限制来决定。


![](https://fastly.jsdelivr.net/gh/bucketio/img6@main/2025/10/05/1759633273653-aa43deb3-4665-4e42-b18f-e4f8da588f30.png)



### 相似度度量
向量既可以表示为数字列表，也可以表示为方向和大小的组合。简单来说，你可以将向量想象成在空间中指向特定方向的线段。

![](https://fastly.jsdelivr.net/gh/bucketio/img9@main/2025/10/05/1759633296482-eb896820-67a7-4568-a329-4c64a30a8272.png)

### 如何计算向量相似度？
计算两个向量之间相似度（或距离）的方法有多种，这些方法被称为相似度度量（Metrics）。其中最常用的包括：

- **点积（Dot Product）**：将两个向量对应元素相乘后求和。点积值越大，说明向量的相似度越高。
- **余弦相似度（Cosine Similarity）**：通过两个向量的点积除以它们模长（范数）的乘积计算得出。余弦相似度为1表示两个向量完全同向（相似度极高），为0表示无相似度，为-1则表示两个向量完全反向（相似度极低）。
- **欧几里得距离（Euclidean Distance）**：假设两个向量是向量空间中的箭头，欧几里得距离就是连接这两个箭头顶端的直线长度。欧几里得距离越小，向量的相似度越高。
- **曼哈顿距离（Manhattan Distance）**：也称为出租车距离，计算方式是在向量空间中沿着网格路径累加两个向量各维度的距离。曼哈顿距离越小，向量的相似度越高。

除此之外，还有许多其他的相似度度量方法。若你想深入了解，可参考相关专业资料。


![](https://fastly.jsdelivr.net/gh/bucketio/img15@main/2025/10/05/1759633313912-aa79ca8a-8d1a-4f90-b296-7b6e2e14a773.png)


## 专用向量数据库与基于扩展的向量数据库
### 专用向量数据库
示例：Qdrant、Pinecone、Weaviate、Milvus、Chroma。这类数据库具有以下特点：

- 开箱即支持数据导入、索引构建、筛选和ANN搜索的优化
- 提供元数据筛选、混合搜索（向量+标量筛选）、单条记录多向量、扩展选项等功能
- 最适用于生产环境中的RAG系统、推荐引擎和高查询量的工作负载

### 基于传统数据库扩展的向量数据库
示例：PostgreSQL + pgvector插件、Redis向量搜索、带向量模块的Elasticsearch。

- 若你已在使用PostgreSQL或Redis，希望在不引入新系统的情况下添加向量搜索功能，这类方案会很合适
- 与专用向量数据库相比，可能在ANN性能或高级功能上存在一定妥协，但对于较简单的使用场景，或对ACID事务一致性有严格要求的场景，仍是明智之选

《到2026年，每位资深IT专业人员都必须掌握的顶级AI工具与框架》  
一份涵盖LLM、向量数据库、智能体等领域的实用指南
techwithram.medium.com


## 主流向量数据库系统——经过调研的简要对比
- **Pinecone**：全托管的无服务器向量数据库，支持自动扩展和企业级集成，非常适合生产环境中的RAG系统。官网：Pinecone
- **Qdrant**：开源向量数据库，基于Rust语言开发，性能出色且支持灵活筛选；通过Docker可轻松实现本地快速启动，适合构建生产级系统并保持对系统的控制权。
- **Weaviate**：开源向量数据库，提供GraphQL前端，支持内置机器学习模块和知识图谱风格的功能。
- **pgvector（PostgreSQL插件）**：为PostgreSQL添加向量列和相似度计算函数，支持ACID事务，且兼容SQL语法，适合已在使用PostgreSQL的团队。
- **Milvus、Chroma与FAISS**：Milvus适用于大规模分布式GPU工作负载；Chroma适合本地LLM工作流；FAISS本身并非完整的数据库，而是一款用于实现ANN搜索的基础库，常被集成到其他系统中。


## 实际架构与应用场景
- **检索增强生成（RAG）**：将文档转化为嵌入向量并存储在向量数据库中；查询时，检索Top-K个最相关的文档，将其作为上下文传递给LLM。这是构建长上下文聊天机器人的常用模式（教程中常见Pinecone与OpenAI的组合）。
- **推荐引擎**：针对“喜欢这个的用户也喜欢……”这类场景，计算用户和物品的嵌入向量，再通过近邻搜索实现推荐。
- **多模态搜索**：利用CLIP模型生成图像和文本的嵌入向量，支持跨模态搜索（如通过文本搜索图像，或通过图像搜索文本）。
- **混合搜索**：将向量检索与关键词/SQL筛选结合，例如“仅检索2024年发布的Top-10相似文章”。


## 实战教程——使用Sentence-Transformers和Qdrant构建本地语义搜索
本教程简洁实用，可直接运行。我们将完成以下步骤：

1. 安装依赖库
2. 通过Docker在本地运行Qdrant
3. 使用Sentence-Transformers（all-MiniLM-L6-v2模型）生成嵌入向量
4. 将文档索引到Qdrant中
5. 查询相似文档

为何选择这套技术栈？Qdrant的本地快速启动体验出色，且提供完善的Python客户端；Sentence-Transformers能轻松生成高质量的语义搜索嵌入向量，两者都有详尽的官方文档。

建议：本教程为本地学习演示版本。若用于生产环境，需考虑使用安全的托管实例、更复杂的模型，以及通过异步上传实现批量处理。

### A. 环境搭建（命令）
在终端中运行以下命令：

```bash
# 创建虚拟环境（可选）
python -m venv venv
source venv/bin/activate
# 安装Python库
pip install qdrant-client sentence-transformers numpy uvicorn fastapi
```

通过Docker在本地启动Qdrant（推荐方式）：
```bash
# 启动Qdrant容器（需先安装Docker）
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant
```

### B. 示例数据集
创建一个小型样本语料库（Python列表）——实际应用中，你可以使用文章、产品描述或PDF文档等数据。

```python
documents = [
    {"id": "1", "text": "如何制作完美的煎饼：食谱与技巧。"},
    {"id": "2", "text": "PostgreSQL性能调优的最佳实践。"},
    {"id": "3", "text": "机器学习与神经网络入门指南。"},
    {"id": "4", "text": "2025年美食爱好者的10大旅行目的地。"},
    {"id": "5", "text": "理解向量数据库与语义搜索。"}
]
```

### C. 编码与上传至Qdrant（完整Python脚本）
```python
# 文件名：qdrant_semantic_search.py
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams
from sentence_transformers import SentenceTransformer
import numpy as np

# 1 连接本地Qdrant
client = QdrantClient(url="http://localhost:6333")

# 2 创建集合（若不存在）
collection_name = "demo_docs"
if collection_name not in [c.name for c in client.get_collections().collections]:
    client.recreate_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=384, distance=Distance.COSINE)  # 使用all-MiniLM-L6-v2模型，对应384维向量
    )

# 3 加载嵌入向量模型
model = SentenceTransformer("all-MiniLM-L6-v2")  # 模型轻量、速度快、效果好
texts = [d["text"] for d in documents]

# 4 生成嵌入向量
embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)

# 5 批量上传
points = []
for doc, emb in zip(documents, embeddings):
    points.append({
        "id": int(doc["id"]),
        "vector": emb.tolist(),
        "payload": {"text": doc["text"]}
    })
client.upsert(collection_name=collection_name, points=points)
print("已向Qdrant上传", len(points), "个文档。")
```

本教程使用all-MiniLM-L6-v2模型（384维向量），该模型轻量且速度快，适合演示场景；Sentence-Transformers提供了多种模型，你可根据准确率和延迟需求选择合适的模型。

创建Qdrant集合时，我们选择了余弦距离（Distance.COSINE），这是句子嵌入向量常用的相似度度量方式。

《混合递归（MoR）：谷歌DeepMind在AI领域的下一个重大突破》  
2017年，谷歌大脑（Google Brain）发表了一篇论文，彻底改变了人工智能的发展轨迹——《注意力就是一切》（Attention Is All You Need）……
techwithram.medium.com

### D. 查询：相似度搜索
将以下代码添加到qdrant_semantic_search.py中，或作为独立代码片段运行：

```python
query = "如何加快我的Postgres数据库速度？"
q_emb = model.encode([query], convert_to_numpy=True)[0]
search_result = client.search(
    collection_name=collection_name,
    query_vector=q_emb.tolist(),
    limit=3,
    with_payload=True
)
for res in search_result:
    print(f"ID: {res.id} 得分: {res.score:.4f}")
    print("文本内容:", res.payload.get("text"))
    print("---")
```

预期结果：关于Postgres的查询应返回与PostgreSQL性能调优相关的文档，也可能返回向量数据库相关文档（取决于嵌入向量的相似度）。你可以调整返回结果数量（limit）和得分阈值来优化查询效果。

### E. 可选步骤：通过重排序提升相关性
向量数据库能快速返回初步的候选结果，你可以使用专用的交叉编码器（Cross-Encoder）或LLM对Top-K结果进行重排序，以获得更优的结果排序。这是生产环境中RAG流水线的常见做法（向量检索→重排序→LLM整合）。许多教程（包括Qdrant的官方教程）都介绍过这种流水线模式。


## 性能优化技巧、指标与监控（实用理论）
- **索引参数调优**：HNSW有ef_construction、m、ef（搜索参数）等参数，这些参数会影响吞吐量和召回率，需结合实际查询场景进行调优。
- **批量插入**：通过批量上传向量，避免单个向量上传的额外开销。
- **内存与CPU**：ANN索引对内存要求较高；部分数据库支持基于磁盘的IVF+PQ方案，可在降低内存占用的同时，以增加延迟为代价。
- **关键监控指标**：查询每秒（QPS）、99分位延迟（P99 latency）、召回率@k（recall@k）、索引构建时间、内存使用率。
- **可观测性**：记录查询延迟，并将Top-K召回率与小型基准数据集对比，实现自动回归检查。


## 如何选择合适的向量数据库——实用 checklist
- 是否需要托管式无服务器扩展？（推荐：Pinecone）
- 是否需要开源控制权和便捷的本地开发体验？（推荐：Qdrant / Weaviate / Chroma）
- 已在使用Postgres且需要ACID事务支持？（推荐：pgvector）
- 是否需要混合搜索（关键词+语义）功能？（推荐：Elasticsearch、Weaviate等）
- 预算与基础设施考量：内存型系统（如Redis）速度极快但成本较高；FAISS+自建部署成本较低，但需要专业技术支持。


## 结语
向量数据库是现代AI应用的“记忆模块”，能让系统记住并理解数据的语义。选择合适的工具，衡量使用效果，不断迭代优化。当你的应用能精准推荐用户所需内容时，不妨在心里默默为自己的技术选择点赞。

希望本文能帮助你解答关于向量数据库的疑问。感谢阅读，欢迎点赞、分享，也可以收藏本文以便日后查阅。